[{"id":"arxiv_2510.02315v1","arxiv_id":"2510.02315v1","title":"Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject\n  Fidelity","abstract":"Text-to-image (T2I) models excel on single-entity prompts but struggle with\nmulti-subject descriptions, often showing attribute leakage, identity\nentanglement, and subject omissions. We introduce the first theoretical\nframework with a principled, optimizable objective for steering sampling\ndynamics toward multi-subject fidelity. Viewing flow matching (FM) through\nstochastic optimal control (SOC), we formulate subject disentanglement as\ncontrol over a trained FM sampler. This yields two architecture-agnostic\nalgorithms: (i) a training-free test-time controller that perturbs the base\nvelocity with a single-pass update, and (ii) Adjoint Matching, a lightweight\nfine-tuning rule that regresses a control network to a backward adjoint signal\nwhile preserving base-model capabilities. The same formulation unifies prior\nattention heuristics, extends to diffusion models via a flow-diffusion\ncorrespondence, and provides the first fine-tuning route explicitly designed\nfor multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and\nStable Diffusion XL, both algorithms consistently improve multi-subject\nalignment while maintaining base-model style. Test-time control runs\nefficiently on commodity GPUs, and fine-tuned controllers trained on limited\nprompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal\nControl for Unentangled Subjects), which achieves state-of-the-art\nmulti-subject fidelity across models.","authors":["Eric Tillmann Bill","Enis Simsar","Thomas Hofmann"],"published":"2025-10-02T17:59:58Z","updated":"2025-10-02T17:59:58Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02315v1","pdf_url":"http://arxiv.org/pdf/2510.02315v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Ever tried to get a text-to-image model to render two distinct people and ended up with a mash-up?  This paper offers a principled fix: cast flow matching as stochastic optimal control to steer samplers for multi-subject fidelity.  Helps T2I users, artists, and dataset creators.","challenges":"ðŸŽ¯ Key problems addressed: - Attribute leakage: attributes mix between subjects (e.g., clothes, accessories). - Identity entanglement: subject identities merge or swap. - Subject omission: one or more requested subjects are missing or ignored.","innovations":"âœ¨ Core contributions: - Reinterpret flow matching (FM) via stochastic optimal control (SOC) to derive an optimizable objective for disentangling subjects. - Test-time controller: training-free, single-pass perturbation of base velocity. - Adjoint Matching: lightweight fine-tuning that regresses a control network to a backward adjoint signal while preserving base-model behavior. - Unifies prior attention heuristics and extends to diffusion models via a flowâ†”diffusion correspondence. - FOCUS: Flow Optimal Control for Unentangled Subjects â€” SOTA multi-subject fidelity across models. Novelty: first principled SOC-based objective and both test-time and fine-tuning routes specifically designed for multi-subject fidelity.","experiments":"ðŸ“Š Quantitative result: Not specified in the paper. Main experimental claims: both algorithms consistently improve multi-subject alignment on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL while maintaining base-model style. Test-time control runs efficiently on commodity GPUs; fine-tuned controllers trained on limited prompts generalize to unseen prompts.","insights":"ðŸ¤” Future directions & applications: - Research: extend control framework to many (>2) subjects and integrate explicit text-conditioned alignment losses or learned compositional prompt representations. - Applications: commercial T2I pipelines, character-accurate asset generation for games/film, and synthetic dataset creation for multi-entity vision tasks. Could this optimal-control view enable interactive, real-time disentanglement controls in creative tools?","keywords":["flow matching","stochastic optimal control","multi-subject fidelity","test-time control","Adjoint Matching","flow-diffusion correspondence","Stable Diffusion","FOCUS"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Ever tried to get a text-to-image model to render two distinct people and ended up with a mash-up?  This paper offers a principled fix: cast flow matching as stochastic optimal control to steer samplers for multi-subject fidelity.  Helps T2I users, artists, and dataset creators.\n\n**Challenges:** ðŸŽ¯ Key problems addressed: - Attribute leakage: attributes mix between subjects (e.g., clothes, accessories). - Identi...","analyzed_at":"2025-10-05T08:05:37.121Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02314v1","arxiv_id":"2510.02314v1","title":"StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided\n  Illusions","abstract":"3D scene representation methods like Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have significantly advanced novel view synthesis. As\nthese methods become prevalent, addressing their vulnerabilities becomes\ncritical. We analyze 3DGS robustness against image-level poisoning attacks and\npropose a novel density-guided poisoning method. Our method strategically\ninjects Gaussian points into low-density regions identified via Kernel Density\nEstimation (KDE), embedding viewpoint-dependent illusory objects clearly\nvisible from poisoned views while minimally affecting innocent views.\nAdditionally, we introduce an adaptive noise strategy to disrupt multi-view\nconsistency, further enhancing attack effectiveness. We propose a KDE-based\nevaluation protocol to assess attack difficulty systematically, enabling\nobjective benchmarking for future research. Extensive experiments demonstrate\nour method's superior performance compared to state-of-the-art techniques.\nProject page: https://hentci.github.io/stealthattack/","authors":["Bo-Hsu Ke","You-Zhe Xie","Yu-Lun Liu","Wei-Chen Chiu"],"published":"2025-10-02T17:59:57Z","updated":"2025-10-02T17:59:57Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02314v1","pdf_url":"http://arxiv.org/pdf/2510.02314v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Want to stealthily poison a 3D scene without breaking most views?  StealthAttack crafts viewpoint-dependent illusory objects in 3D Gaussian Splatting by injecting Gaussians into low-density regions (KDE-guided), keeping innocent views intact.  Important for 3D robustness research and security.","challenges":"ðŸŽ¯ Problems addressed: - 3D scene representations (NeRF, 3DGS) are vulnerable to image-level poisoning. - Hard to create attacks that are visible only in poisoned views while remaining stealthy in others. - Lack of systematic protocols to measure attack difficulty.","innovations":"âœ¨ Key innovations: - KDE-guided density analysis to find low-density regions for inserting malicious Gaussian points. - Viewpoint-dependent illusory objects: poison affects targeted views strongly but minimally perturbs innocent views. - Adaptive noise strategy to break multi-view consistency and strengthen attack effectiveness. - KDE-based evaluation protocol for systematic benchmarking.","experiments":"ðŸ“Š Quantitative result: Not specified in the paper. Main experimental claim: Extensive experiments demonstrate the method achieves superior performance compared to state-of-the-art poisoning techniques, validating density-guided and adaptive-noise strategies.","insights":"ðŸ¤” What's next? - Research defenses: design density-aware regularizers or detectors that spot suspicious Gaussian insertions or viewpoint-conditional artifacts. - Broader study: test poisoning and defenses across other 3D representations and real-world capture pipelines to assess practical risk. Could density-aware defenses close the gap?","keywords":["3D Gaussian Splatting","poisoning attack","kernel density estimation","viewpoint-dependent attack","multi-view consistency","adversarial robustness","3D scene representation","KDE-based evaluation"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want to stealthily poison a 3D scene without breaking most views?  StealthAttack crafts viewpoint-dependent illusory objects in 3D Gaussian Splatting by injecting Gaussians into low-density regions (KDE-guided), keeping innocent views intact.  Important for 3D robustness research and security.\n\n**Challenges:** ðŸŽ¯ Problems addressed: - 3D scene representations (NeRF, 3DGS) are vulnerable to image-level poisoning....","analyzed_at":"2025-10-05T08:05:30.866Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02313v1","arxiv_id":"2510.02313v1","title":"Clink! Chop! Thud! -- Learning Object Sounds from Real-World\n  Interactions","abstract":"Can a model distinguish between the sound of a spoon hitting a hardwood floor\nversus a carpeted one? Everyday object interactions produce sounds unique to\nthe objects involved. We introduce the sounding object detection task to\nevaluate a model's ability to link these sounds to the objects directly\ninvolved. Inspired by human perception, our multimodal object-aware framework\nlearns from in-the-wild egocentric videos. To encourage an object-centric\napproach, we first develop an automatic pipeline to compute segmentation masks\nof the objects involved to guide the model's focus during training towards the\nmost informative regions of the interaction. A slot attention visual encoder is\nused to further enforce an object prior. We demonstrate state of the art\nperformance on our new task along with existing multimodal action understanding\ntasks.","authors":["Mengyu Yang","Yiming Chen","Haozheng Pei","Siddhant Agarwal","Arun Balajee Vasudevan","James Hays"],"published":"2025-10-02T17:59:52Z","updated":"2025-10-02T17:59:52Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02313v1","pdf_url":"http://arxiv.org/pdf/2510.02313v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Can a model tell a spoon clink on hardwood vs. carpet?  This paper introduces \"sounding object detection\": a multimodal, object-aware framework that learns from in-the-wild egocentric videos to link sounds to the actual objects involved.  Why it matters: improves object-level audio-visual understanding.","challenges":"ðŸŽ¯ Key problems tackled: - Models struggle to link specific sounds to the exact objects producing them. - Lack of object-centric supervision in audio-visual learning. - Need for methods that learn from in-the-wild egocentric interactions.","innovations":"âœ¨ Core innovations: - Automatic pipeline to compute segmentation masks of interacting objects to guide training. - Slot-attention visual encoder to enforce an object prior. - Multimodal object-aware framework that trains on egocentric videos. Novelty: object-centric audio-visual learning guided by automatically extracted masks + slot attention.","experiments":"ðŸ“Š Results & proof: - Demonstrated state-of-the-art performance on the new sounding object detection task and on existing multimodal action understanding benchmarks. - Specific numeric improvements: Not specified in the paper.","insights":"ðŸ¤” What's next? - Research directions: explore object-level audio-visual pretraining and self-supervised object-sound representations; adapt for real-time robotic perception during manipulation. - Applications: improved robot touch/audio feedback, richer AR/VR sound grounding. Could object-level sound grounding enable safer manipulation?","keywords":["sounding object detection","audio-visual learning","egocentric video","slot attention","segmentation masks","object-centric","multimodal","sound recognition"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Can a model tell a spoon clink on hardwood vs. carpet?  This paper introduces \"sounding object detection\": a multimodal, object-aware framework that learns from in-the-wild egocentric videos to link sounds to the actual objects involved.  Why it matters: improves object-level audio-visual understanding.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Models struggle to link specific sounds to the exact objects produ...","analyzed_at":"2025-10-05T08:05:40.546Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02312v1","arxiv_id":"2510.02312v1","title":"KaVa: Latent Reasoning via Compressed KV-Cache Distillation","abstract":"Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.","authors":["Anna Kuzina","Maciej Pioro","Paul N. Whatmough","Babak Ehteshami Bejnordi"],"published":"2025-10-02T17:59:51Z","updated":"2025-10-02T17:59:51Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02312v1","pdf_url":"http://arxiv.org/pdf/2510.02312v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Want CoT-level reasoning without the huge compute and verbose traces? KaVa distills a teacher's compressed KV-cache into a latent-reasoning student, keeping CoT accuracy while enabling efficient, deployable latent inference for complex reasoning.","challenges":"ðŸŽ¯ Challenges: - Chain-of-thought (CoT) yields verbose, costly traces and memory overhead. - Latent reasoning is efficient but lacks supervision for complex, natural-language traces. - Compressed KV-caches contain useful but unaligned, abstract info.","innovations":"âœ¨ Innovations: - Distill knowledge from a teacher's compressed KV-cache into a latent student via self-distillation. - Align stepwise KV trajectories using continuous latent tokens. - Novel use of compressed KV-cache as a supervisory signal for latent reasoning.","experiments":"ðŸ“Š Experiments: Not specified in the paper. Qualitative findings: KaVa consistently outperforms strong latent baselines, shows much smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency.","insights":"ðŸ¤” Insights & next steps: - Explore applying KV-cache distillation to multimodal or retrieval-augmented models. - Investigate on-device/edge deployment for cost-sensitive apps and better latent interpretability. Could this make CoT-quality reasoning practical in low-cost settings?","keywords":["KaVa","latent reasoning","KV-cache","compressed KV-cache","distillation","self-distillation","chain-of-thought","LLMs"],"category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want CoT-level reasoning without the huge compute and verbose traces? KaVa distills a teacher's compressed KV-cache into a latent-reasoning student, keeping CoT accuracy while enabling efficient, deployable latent inference for complex reasoning.\n\n**Challenges:** ðŸŽ¯ Challenges: - Chain-of-thought (CoT) yields verbose, costly traces and memory overhead. - Latent reasoning is efficient but lacks supervision for co...","analyzed_at":"2025-10-05T08:05:59.515Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02311v1","arxiv_id":"2510.02311v1","title":"Inferring Dynamic Physical Properties from Video Foundation Models","abstract":"We study the task of predicting dynamic physical properties from videos. More\nspecifically, we consider physical properties that require temporal information\nto be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,\nand dynamic friction of an object sliding on a surface. To this end, we make\nthe following contributions: (i) We collect a new video dataset for each\nphysical property, consisting of synthetic training and testing splits, as well\nas a real split for real world evaluation. (ii) We explore three ways to infer\nthe physical property from videos: (a) an oracle method where we supply the\nvisual cues that intrinsically reflect the property using classical computer\nvision techniques; (b) a simple read out mechanism using a visual prompt and\ntrainable prompt vector for cross-attention on pre-trained video generative and\nself-supervised models; and (c) prompt strategies for Multi-modal Large\nLanguage Models (MLLMs). (iii) We show that video foundation models trained in\na generative or self-supervised manner achieve a similar performance, though\nbehind that of the oracle, and MLLMs are currently inferior to the other\nmodels, though their performance can be improved through suitable prompting.","authors":["Guanqi Zhan","Xianzheng Ma","Weidi Xie","Andrew Zisserman"],"published":"2025-10-02T17:59:50Z","updated":"2025-10-02T17:59:50Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02311v1","pdf_url":"http://arxiv.org/pdf/2510.02311v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Want to infer how bouncy, runny, or slippery something is just from video? This paper tackles predicting dynamic physical properties (elasticity, viscosity, dynamic friction) from video using video foundation models and prompting. Important for visual physical reasoning and perceptual systems.","challenges":"ðŸŽ¯ Key problems tackled: - Inferring properties that require temporal dynamics (not single-frame cues). - Lack of tailored datasets for dynamic physical properties from video. - Adapting large pre-trained video/MLLMs to physical-property estimation.","innovations":"âœ¨ Core innovations: - Collected new video datasets (synthetic train/test + real split) for elasticity, viscosity, friction. - Oracle method: supply classical CV-derived visual cues as upper-bound. - Readout via visual prompt + trainable prompt vector for cross-attention on pre-trained video generative/self-supervised models. - Prompt strategies explored for Multi-modal LLMs (MLLMs). Novelty: framing dynamic physical property inference as a prompting/readout task on video foundation models and comparing oracle, foundation models, and MLLMs.","experiments":"ðŸ“Š Single most compelling quantitative result: Not specified in the paper. Main demonstrated breakthrough: Video foundation models trained generatively or via self-supervision attain similar performance (but below the oracle); MLLMs perform worse out-of-the-box but can be improved with suitable prompting.","insights":"ðŸ¤” What's next? - Future research: adapt foundation models via physics-aware fine-tuning or few-shot domain transfer to close gap to oracle; combine learned models with simulators for better generalization. - Applications: improved robotic perception/manipulation and material inspection from video. Could prompting + physics priors enable reliable real-world physical sensing from passive video?","keywords":["dynamic physical properties","elasticity","viscosity","dynamic friction","video foundation models","visual prompt","trainable prompt vector","self-supervised learning","video generative models","MLLMs","dataset"],"category":"computer_vision","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want to infer how bouncy, runny, or slippery something is just from video? This paper tackles predicting dynamic physical properties (elasticity, viscosity, dynamic friction) from video using video foundation models and prompting. Important for visual physical reasoning and perceptual systems.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Inferring properties that require temporal dynamics (not single-frame cues)....","analyzed_at":"2025-10-05T08:06:03.823Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02308v1","arxiv_id":"2510.02308v1","title":"Robust Tangent Space Estimation via Laplacian Eigenvector Gradient\n  Orthogonalization","abstract":"Estimating the tangent spaces of a data manifold is a fundamental problem in\ndata analysis. The standard approach, Local Principal Component Analysis\n(LPCA), struggles in high-noise settings due to a critical trade-off in\nchoosing the neighborhood size. Selecting an optimal size requires prior\nknowledge of the geometric and noise characteristics of the data that are often\nunavailable. In this paper, we propose a spectral method, Laplacian Eigenvector\nGradient Orthogonalization (LEGO), that utilizes the global structure of the\ndata to guide local tangent space estimation. Instead of relying solely on\nlocal neighborhoods, LEGO estimates the tangent space at each data point by\northogonalizing the gradients of low-frequency eigenvectors of the graph\nLaplacian. We provide two theoretical justifications of our method. First, a\ndifferential geometric analysis on a tubular neighborhood of a manifold shows\nthat gradients of the low-frequency Laplacian eigenfunctions of the tube align\nclosely with the manifold's tangent bundle, while an eigenfunction with high\ngradient in directions orthogonal to the manifold lie deeper in the spectrum.\nSecond, a random matrix theoretic analysis also demonstrates that low-frequency\neigenvectors are robust to sub-Gaussian noise. Through comprehensive\nexperiments, we demonstrate that LEGO yields tangent space estimates that are\nsignificantly more robust to noise than those from LPCA, resulting in marked\nimprovements in downstream tasks such as manifold learning, boundary detection,\nand local intrinsic dimension estimation.","authors":["Dhruv Kohli","Sawyer J. Robertson","Gal Mishne","Alexander Cloninger"],"published":"2025-10-02T17:59:45Z","updated":"2025-10-02T17:59:45Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02308v1","pdf_url":"http://arxiv.org/pdf/2510.02308v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Struggling to estimate tangent spaces from noisy manifold data? LEGO (Laplacian Eigenvector Gradient Orthogonalization) is a spectral method that uses global graph Laplacian eigenvector gradients to produce robust local tangent estimatesâ€”helpful for manifold learning, boundary detection, and noisy data analysis.","challenges":"ðŸŽ¯ Key problems tackled: - Local PCA (LPCA) fails in high-noise settings. - Critical neighborhood-size trade-off requires unknown geometric/noise priors. - Purely local methods ignore global manifold structure, reducing robustness.","innovations":"âœ¨ Core ideas: - LEGO: estimate tangent at each point by orthogonalizing gradients of low-frequency graph Laplacian eigenvectors. - The novelty: use global low-frequency spectral gradients to guide local tangent estimation rather than only local neighborhoods. - Two theoretical supports: differential-geometric analysis on a tubular neighborhood and random-matrix-theoretic robustness to sub-Gaussian noise.","experiments":"ðŸ“Š Most compelling quantitative result: Not specified in the paper. Qualitative experimental claim: LEGO yields tangent-space estimates significantly more robust to noise than LPCA, producing marked improvements in manifold learning, boundary detection, and local intrinsic-dimension estimation.","insights":"ðŸ¤” Next steps & applications: - Explore integration of LEGO with manifold denoising and spectral embeddings for large-scale/high-dim data. - Potential applications: robust geometric estimation in robotics, medical imaging, and single-cell analysis. Could global spectral guidance become the standard for noisy geometric inference?","keywords":["manifold learning","tangent space estimation","graph Laplacian","Laplacian eigenvectors","spectral methods","random matrix theory","LPCA","noise robustness"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Struggling to estimate tangent spaces from noisy manifold data? LEGO (Laplacian Eigenvector Gradient Orthogonalization) is a spectral method that uses global graph Laplacian eigenvector gradients to produce robust local tangent estimatesâ€”helpful for manifold learning, boundary detection, and noisy data analysis.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Local PCA (LPCA) fails in high-noise settings. - Critical...","analyzed_at":"2025-10-05T08:06:02.639Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02307v1","arxiv_id":"2510.02307v1","title":"NoiseShift: Resolution-Aware Noise Recalibration for Better\n  Low-Resolution Image Generation","abstract":"Text-to-image diffusion models trained on a fixed set of resolutions often\nfail to generalize, even when asked to generate images at lower resolutions\nthan those seen during training. High-resolution text-to-image generators are\ncurrently unable to easily offer an out-of-the-box budget-efficient alternative\nto their users who might not need high-resolution images. We identify a key\ntechnical insight in diffusion models that when addressed can help tackle this\nlimitation: Noise schedulers have unequal perceptual effects across\nresolutions. The same level of noise removes disproportionately more signal\nfrom lower-resolution images than from high-resolution images, leading to a\ntrain-test mismatch. We propose NoiseShift, a training-free method that\nrecalibrates the noise level of the denoiser conditioned on resolution size.\nNoiseShift requires no changes to model architecture or sampling schedule and\nis compatible with existing models. When applied to Stable Diffusion 3, Stable\nDiffusion 3.5, and Flux-Dev, quality at low resolutions is significantly\nimproved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and\nFlux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by\n10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results\ndemonstrate the effectiveness of NoiseShift in mitigating resolution-dependent\nartifacts and enhancing the quality of low-resolution image generation.","authors":["Ruozhen He","Moayed Haji-Ali","Ziyan Yang","Vicente Ordonez"],"published":"2025-10-02T17:59:43Z","updated":"2025-10-02T17:59:43Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02307v1","pdf_url":"http://arxiv.org/pdf/2510.02307v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Want high-quality images but with less compute? NoiseShift fixes a surprising gap: diffusion noise affects low-res images far more than high-res ones. The paper introduces a training-free, resolution-aware noise recalibration that boosts low-res text-to-image quality â€” great for users needing budget-friendly outputs.","challenges":"ðŸŽ¯ Key problems tackled: - Text-to-image diffusion models trained at fixed resolutions fail to generalize to lower resolutions. - Noise schedulers remove disproportionately more signal from low-res images, causing trainâ€“test mismatch. - High-res generators lack an easy, out-of-the-box budget-efficient low-res option.","innovations":"âœ¨ Core ideas & what's novel: - NoiseShift: a training-free method that recalibrates the denoiser's noise level conditioned on output resolution. - No changes required to model architecture or sampling schedule; compatible with existing models. - Novel insight: account for unequal perceptual effects of noise across resolutions and correct them at inference.","experiments":"ðŸ“Š Standout result: On LAION-COCO, NoiseShift improves Stable Diffusion 3.5 by 15.89% FID (average). This demonstrates NoiseShift can substantially reduce resolution-dependent artifacts and materially improve low-resolution generation quality for state-of-the-art models.","insights":"ðŸ¤” What's next (potential directions & applications): - Explore adaptive, resolution-aware schedulers integrated into training or samplers for further gains. - Apply NoiseShift ideas to mobile/edge deployments for cheaper previews or bandwidth-efficient image generation (e.g., low-res thumbnails that still look good). Could resolution-aware noise control enable new trade-offs between cost and fidelity?","keywords":["NoiseShift","diffusion models","resolution-aware","low-resolution generation","Stable Diffusion 3","FID","noise scheduling"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want high-quality images but with less compute? NoiseShift fixes a surprising gap: diffusion noise affects low-res images far more than high-res ones. The paper introduces a training-free, resolution-aware noise recalibration that boosts low-res text-to-image quality â€” great for users needing budget-friendly outputs.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Text-to-image diffusion models trained at fixed reso...","analyzed_at":"2025-10-05T08:06:22.705Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02306v1","arxiv_id":"2510.02306v1","title":"Drawing Conclusions from Draws: Rethinking Preference Semantics in\n  Arena-Style LLM Evaluation","abstract":"In arena-style evaluation of large language models (LLMs), two LLMs respond\nto a user query, and the user chooses the winning response or deems the\n\"battle\" a draw, resulting in an adjustment to the ratings of both models. The\nprevailing approach for modeling these rating dynamics is to view battles as\ntwo-player game matches, as in chess, and apply the Elo rating system and its\nderivatives. In this paper, we critically examine this paradigm. Specifically,\nwe question whether a draw genuinely means that the two models are equal and\nhence whether their ratings should be equalized. Instead, we conjecture that\ndraws are more indicative of query difficulty: if the query is too easy, then\nboth models are more likely to succeed equally. On three real-world arena\ndatasets, we show that ignoring rating updates for draws yields a 1-3% relative\nincrease in battle outcome prediction accuracy (which includes draws) for all\nfour rating systems studied. Further analyses suggest that draws occur more for\nqueries rated as very easy and those as highly objective, with risk ratios of\n1.37 and 1.35, respectively. We recommend future rating systems to reconsider\nexisting draw semantics and to account for query properties in rating updates.","authors":["Raphael Tang","Crystina Zhang","Wenyan Li","Carmen Lai","Pontus Stenetorp","Yao Lu"],"published":"2025-10-02T17:59:41Z","updated":"2025-10-02T17:59:41Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02306v1","pdf_url":"http://arxiv.org/pdf/2510.02306v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Ever treated a \"draw\" as two models being equal? What if it's the question, not the models?   This paper rethinks arena-style LLM evaluation: draws may signal query difficulty/objectivity, not model parity. It shows ignoring draw-based rating updates boosts prediction accuracy (1â€“3%).","challenges":"ðŸŽ¯ Key problems tackled: - Existing arena evaluations use chess-like Elo semantics that equate draws with equal skill. - Draws conflate model comparison with query difficulty/objectivity, corrupting rating dynamics. - Current ratings may mispredict battle outcomes when draws are common.","innovations":"âœ¨ Core contributions: - Critically reframed draw semantics: draws may reflect query difficulty, not model equality. - Simple alternative: ignore rating updates when battles are draws, applied to four rating systems. - Empirical analysis linking draws to easy and highly objective queries (risk ratios reported).","experiments":"ðŸ“Š Main experimental takeaway: On three real-world arena datasets, ignoring rating updates for draws yields a 1â€“3% relative increase in battle outcome prediction accuracy across all four rating systems studied â€” showing draws can harm rating fidelity.","insights":"ðŸ¤” What's next? - Research: develop rating updates that account for query difficulty/objectivity (e.g., weight updates by query hardness). - Applications: fairer LLM leaderboards, better dataset curation, and improved human-in-the-loop evaluation. Could draw-aware ratings change how we compare models?","keywords":["arena-style evaluation","draw semantics","Elo","LLM evaluation","rating systems","query difficulty","objectivity","battle outcome prediction"],"category":"natural_language_processing","relevance_score":8,"technical_depth":"intermediate","summary":"**Introduction:** ðŸš€ Ever treated a \"draw\" as two models being equal? What if it's the question, not the models?   This paper rethinks arena-style LLM evaluation: draws may signal query difficulty/objectivity, not model parity. It shows ignoring draw-based rating updates boosts prediction accuracy (1â€“3%).\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Existing arena evaluations use chess-like Elo semantics that equate draws with equal ...","analyzed_at":"2025-10-05T08:06:21.484Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02305v1","arxiv_id":"2510.02305v1","title":"Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is\n  Geometry Adaptive","abstract":"Diffusion models have achieved state-of-the-art performance, demonstrating\nremarkable generalisation capabilities across diverse domains. However, the\nmechanisms underpinning these strong capabilities remain only partially\nunderstood. A leading conjecture, based on the manifold hypothesis, attributes\nthis success to their ability to adapt to low-dimensional geometric structure\nwithin the data. This work provides evidence for this conjecture, focusing on\nhow such phenomena could result from the formulation of the learning problem\nthrough score matching. We inspect the role of implicit regularisation by\ninvestigating the effect of smoothing minimisers of the empirical score\nmatching objective. Our theoretical and empirical results confirm that\nsmoothing the score function -- or equivalently, smoothing in the log-density\ndomain -- produces smoothing tangential to the data manifold. In addition, we\nshow that the manifold along which the diffusion model generalises can be\ncontrolled by choosing an appropriate smoothing.","authors":["Tyler Farghly","Peter Potaptchik","Samuel Howard","George Deligiannidis","Jakiw Pidstrigach"],"published":"2025-10-02T17:59:39Z","updated":"2025-10-02T17:59:39Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02305v1","pdf_url":"http://arxiv.org/pdf/2510.02305v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Why do diffusion models generalize so well? This paper shows a key reason: smoothing the score (equivalently smoothing in the logâ€‘density domain) is geometryâ€‘adaptive â€” it smooths tangentially to the data manifold. A major step toward explaining diffusion models' strong generalisation.","challenges":"ðŸŽ¯ Key problems tackled: - Mechanisms behind diffusion models' strong generalisation are unclear. - It's not understood how score matching yields geometryâ€‘aware behaviour. - Little control over the manifold along which diffusion models generalise.","innovations":"âœ¨ Core innovations: - Study implicit regularisation by smoothing minimisers of the empirical scoreâ€‘matching objective. - Prove smoothing the score â‡” smoothing in the logâ€‘density domain. - Theoretical and empirical evidence that logâ€‘domain smoothing acts tangentially to the data manifold. - Show the manifold of generalisation can be influenced by the choice of smoothing. Novelty: reframing regularisation as geometryâ€‘adaptive logâ€‘domain smoothing that aligns with manifold structure.","experiments":"ðŸ“Š Experiments & proof: Main experimental claim: empirical results confirm that smoothing the score (logâ€‘density smoothing) produces tangential smoothing to the data manifold and that the choice of smoothing controls the manifold along which the model generalises. Quantitative numbers / benchmarks: Not specified in the paper.","insights":"ðŸ¤” What's next? - Research: design noise schedules or smoothing operators that adapt locally to manifold dimension; study impacts on sample quality and robustness. - Applications: manifoldâ€‘aware data augmentation, controllable generative models, improved conditional sampling. Could targeted logâ€‘domain smoothing enable safer, more controllable generative systems?","keywords":["diffusion_models","manifold_hypothesis","score_matching","log_density_smoothing","geometry_adaptive","implicit_regularisation","generative_models"],"category":"generative_models","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Why do diffusion models generalize so well? This paper shows a key reason: smoothing the score (equivalently smoothing in the logâ€‘density domain) is geometryâ€‘adaptive â€” it smooths tangentially to the data manifold. A major step toward explaining diffusion models' strong generalisation.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Mechanisms behind diffusion models' strong generalisation are unclear. - It's not un...","analyzed_at":"2025-10-05T08:06:28.441Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02302v1","arxiv_id":"2510.02302v1","title":"Knowledge Distillation Detection for Open-weights Models","abstract":"We propose the task of knowledge distillation detection, which aims to\ndetermine whether a student model has been distilled from a given teacher,\nunder a practical setting where only the student's weights and the teacher's\nAPI are available. This problem is motivated by growing concerns about model\nprovenance and unauthorized replication through distillation. To address this\ntask, we introduce a model-agnostic framework that combines data-free input\nsynthesis and statistical score computation for detecting distillation. Our\napproach is applicable to both classification and generative models.\nExperiments on diverse architectures for image classification and text-to-image\ngeneration show that our method improves detection accuracy over the strongest\nbaselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image\ngeneration. The code is available at\nhttps://github.com/shqii1j/distillation_detection.","authors":["Qin Shi","Amber Yijia Zheng","Qifan Song","Raymond A. Yeh"],"published":"2025-10-02T17:59:14Z","updated":"2025-10-02T17:59:14Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02302v1","pdf_url":"http://arxiv.org/pdf/2510.02302v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Ever wondered if a model was secretly cloned via distillation?  This paper introduces knowledge distillation detection: a practical way to tell if a student model was distilled from a given teacher when you only have the student's weights and the teacher's API.  Why it matters: helps prove model provenance and curb unauthorized replication. ðŸ”","challenges":"ðŸŽ¯ Key problems tackled: - Detecting distillation when only the student's weights and a teacher API are available (no internals/data). - Real-world threat of unauthorized model replication via distillation. - Need to detect across both classification and generative models.","innovations":"âœ¨ Novel solution highlights: - Model-agnostic framework combining data-free input synthesis + statistical score computation. - Applies to both classification and generative (text-to-image) models. - Novel twist: practical, no-access setting (open student weights + teacher API) for provenance detection.","experiments":"ðŸ“Š Results & breakthrough: - Large gains vs strongest baselines: +59.6% (CIFAR-10), +71.2% (ImageNet), +20.0% (text-to-image gen). This demonstrates a strong, generalizable ability to detect distilled students across tasks and architectures.","insights":"ðŸ¤” What's next? - Explore robustness to adaptive attackers who try to hide distillation (e.g., fine-tuning or obfuscation). - Extend detection to large-scale models (e.g., LLMs) and automated provenance auditing in model marketplaces. Could this become a standard for model provenance verification?","keywords":["knowledge distillation detection","data-free input synthesis","statistical score","model provenance","open-weights models","distillation","classification","text-to-image generation"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Ever wondered if a model was secretly cloned via distillation?  This paper introduces knowledge distillation detection: a practical way to tell if a student model was distilled from a given teacher when you only have the student's weights and the teacher's API.  Why it matters: helps prove model provenance and curb unauthorized replication. ðŸ”\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Detecting distillation wh...","analyzed_at":"2025-10-05T08:06:46.960Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02300v1","arxiv_id":"2510.02300v1","title":"Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models","abstract":"We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.","authors":["Runqian Wang","Yilun Du"],"published":"2025-10-02T17:59:06Z","updated":"2025-10-02T17:59:06Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02300v1","pdf_url":"http://arxiv.org/pdf/2510.02300v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ What if generative models could ditch time-conditional chains and just optimize to a data landscape? Equilibrium Matching (EqM) learns an equilibrium gradient of an implicit energy and samples by optimization â€” faster, adaptive inference that boosts image generation (and more).","challenges":"ðŸŽ¯ Problems tackled: - Diffusion/flow models rely on non-equilibrium, time-conditional dynamics that complicate training & sampling. - Fixed, sequential samplers limit adaptive compute and optimizer choices at inference. - Existing bridges between flows and EBMs are loose and inefficient.","innovations":"âœ¨ Key ideas: - Equilibrium Matching (EqM): learn the equilibrium gradient of an implicit energy landscape. - Replace time-conditional velocities with a unified equilibrium landscape. - Optimization-based sampling: gradient descent on learned energy with adjustable step sizes, adaptive optimizers, and adaptive compute. - Theoretical justification for learning/sampling the data manifold.","experiments":"ðŸ“Š Main result: EqM achieves FID 1.90 on ImageNet 256Ã—256, outperforming prior diffusion/flow methods empirically. This demonstrates that optimization-driven sampling on an implicit equilibrium energy can reach SOTA generation quality.","insights":"ðŸ¤” Where to go next: - Explore learned optimizers or learned samplers on the EqM landscape for faster convergence. - Apply EqM to conditional generation, other modalities (audio/3D), or as a robust OOD detection backbone. Could optimization-first generative modeling reshape model-efficiency tradeoffs?","keywords":["Equilibrium Matching","EqM","implicit energy-based models","energy gradient","diffusion","flow models","optimization-based sampling","ImageNet","FID 1.90","denoising","OOD detection","image composition","data manifold"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ What if generative models could ditch time-conditional chains and just optimize to a data landscape? Equilibrium Matching (EqM) learns an equilibrium gradient of an implicit energy and samples by optimization â€” faster, adaptive inference that boosts image generation (and more).\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Diffusion/flow models rely on non-equilibrium, time-conditional dynamics that complicate trainin...","analyzed_at":"2025-10-05T08:06:44.025Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02298v1","arxiv_id":"2510.02298v1","title":"ARMADA: Autonomous Online Failure Detection and Human Shared Control\n  Empower Scalable Real-world Deployment and Adaptation","abstract":"Imitation learning has shown promise in learning from large-scale real-world\ndatasets. However, pretrained policies usually perform poorly without\nsufficient in-domain data. Besides, human-collected demonstrations entail\nsubstantial labour and tend to encompass mixed-quality data and redundant\ninformation. As a workaround, human-in-the-loop systems gather domain-specific\ndata for policy post-training, and exploit closed-loop policy feedback to offer\ninformative guidance, but usually require full-time human surveillance during\npolicy rollout. In this work, we devise ARMADA, a multi-robot deployment and\nadaptation system with human-in-the-loop shared control, featuring an\nautonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA\nenables paralleled policy rollout and requests human intervention only when\nnecessary, significantly reducing reliance on human supervision. Hence, ARMADA\nenables efficient acquisition of in-domain data, and leads to more scalable\ndeployment and faster adaptation to new scenarios. We evaluate the performance\nof ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on\naverage, surpassing prior state-of-the-art failure detection approaches by over\n20%. Besides, ARMADA manifests more than 4$\\times$ increase in success rate and\ngreater than 2$\\times$ reduction in human intervention rate over multiple\nrounds of policy rollout and post-training, compared to previous\nhuman-in-the-loop learning methods.","authors":["Wenye Yu","Jun Lv","Zixi Ying","Yang Jin","Chuan Wen","Cewu Lu"],"published":"2025-10-02T17:59:02Z","updated":"2025-10-02T17:59:02Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02298v1","pdf_url":"http://arxiv.org/pdf/2510.02298v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Struggling to scale robot policies in the real world? ARMADA is a multi-robot, human-in-the-loop deployment + adaptation system that uses FLOAT, an autonomous online failure detector. It parallelizes rollouts and calls humans only when neededâ€”cutting supervision and speeding adaptation.","challenges":"ðŸŽ¯ Key problems addressed: - Pretrained policies fail without sufficient in-domain data. - Human demonstrations are costly, mixed-quality, and redundant. - Human-in-the-loop rollouts usually demand full-time supervision, limiting scalability.","innovations":"âœ¨ Core contributions: - ARMADA: multi-robot deployment & post-training pipeline with shared human control. - FLOAT: autonomous online failure detection that triggers human intervention only as needed. - Novelty: FLOAT enables parallel policy rollouts with sparse, targeted human inputâ€”scalable real-world adaptation.","experiments":"ðŸ“Š Main results: FLOAT achieves nearly 95% average accuracy, outperforming prior failure-detection approaches by >20%. ARMADA delivers >4Ã— increase in task success rate and >2Ã— reduction in human intervention rate across multiple rollout/post-training roundsâ€”showing scalable, efficient adaptation.","insights":"ðŸ¤” Where to go next: - Research directions: extend FLOAT to heterogeneous fleets and integrate active learning to minimize queries; explore autonomous recovery actions when failures are detected. - Applications: warehouse robots, field robotics, assistive robots in care settings. Could sparse, confidence-driven intervention redefine fleet-scale deployment?","keywords":["ARMADA","FLOAT","failure detection","human-in-the-loop","shared control","imitation learning","online adaptation","multi-robot deployment","real-world datasets","policy post-training"],"category":"robotics","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Struggling to scale robot policies in the real world? ARMADA is a multi-robot, human-in-the-loop deployment + adaptation system that uses FLOAT, an autonomous online failure detector. It parallelizes rollouts and calls humans only when neededâ€”cutting supervision and speeding adaptation.\n\n**Challenges:** ðŸŽ¯ Key problems addressed: - Pretrained policies fail without sufficient in-domain data. - Human demonstration...","analyzed_at":"2025-10-05T08:06:54.067Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02297v1","arxiv_id":"2510.02297v1","title":"Interactive Training: Feedback-Driven Neural Network Optimization","abstract":"Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.","authors":["Wentao Zhang","Yang Young Lu","Yuntian Deng"],"published":"2025-10-02T17:59:00Z","updated":"2025-10-02T17:59:00Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02297v1","pdf_url":"http://arxiv.org/pdf/2510.02297v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Ever wanted to tweak a neural net mid-training when things go wrong? Interactive Training introduces a real-time, feedback-driven framework that lets humans or AI agents intervene during training via a control server. Important for stabilizing and customizing training on the fly.","challenges":"ðŸŽ¯ Problems addressed: - Fixed training recipes can't react to instabilities or emerging issues. - High sensitivity to initial hyperparameters makes runs brittle. - Lack of mechanisms for real-time human/agent intervention during training.","innovations":"âœ¨ Core contributions: - Open-source Interactive Training framework with a control server to mediate communication. - Real-time intervention: dynamically adjust optimizer hyperparameters, training data, and checkpoints. - Supports human experts and automated AI agents to monitor and act during live training. Novelty: enables feedback-driven, live control of training dynamics rather than static preconfigured runs.","experiments":"ðŸ“Š Not specified in the paper.  Qualitative evidence (reported): case studies show superior training stability, reduced sensitivity to initial hyperparameters, and improved adaptability to evolving user needsâ€”demonstrating practical benefits of real-time intervention.","insights":"ðŸ¤” Future directions & applications: - Research: develop autonomous controller agents (RL or rule-based) that learn intervention policies from logs. - Research: integrate with distributed or large-scale pretraining workflows for foundation models. - Applications: MLOps/debugging tools, real-time model tuning in production. Could feedback-driven agents become standard monitors that autonomously stabilize training?","keywords":["Interactive Training","control server","human-in-the-loop","AI agents","training stability","hyperparameter tuning","model checkpoints","open-source","feedback-driven"],"category":"machine_learning","relevance_score":8,"technical_depth":"intermediate","summary":"**Introduction:** ðŸš€ Ever wanted to tweak a neural net mid-training when things go wrong? Interactive Training introduces a real-time, feedback-driven framework that lets humans or AI agents intervene during training via a control server. Important for stabilizing and customizing training on the fly.\n\n**Challenges:** ðŸŽ¯ Problems addressed: - Fixed training recipes can't react to instabilities or emerging issues. - High sensitivity t...","analyzed_at":"2025-10-05T08:07:11.892Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02296v1","arxiv_id":"2510.02296v1","title":"Continual Personalization for Diffusion Models","abstract":"Updating diffusion models in an incremental setting would be practical in\nreal-world applications yet computationally challenging. We present a novel\nlearning strategy of Concept Neuron Selection (CNS), a simple yet effective\napproach to perform personalization in a continual learning scheme. CNS\nuniquely identifies neurons in diffusion models that are closely related to the\ntarget concepts. In order to mitigate catastrophic forgetting problems while\npreserving zero-shot text-to-image generation ability, CNS finetunes concept\nneurons in an incremental manner and jointly preserves knowledge learned of\nprevious concepts. Evaluation of real-world datasets demonstrates that CNS\nachieves state-of-the-art performance with minimal parameter adjustments,\noutperforming previous methods in both single and multi-concept personalization\nworks. CNS also achieves fusion-free operation, reducing memory storage and\nprocessing time for continual personalization.","authors":["Yu-Chien Liao","Jr-Jen Chen","Chi-Pin Huang","Ci-Siang Lin","Meng-Lin Wu","Yu-Chiang Frank Wang"],"published":"2025-10-02T17:58:56Z","updated":"2025-10-02T17:58:56Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02296v1","pdf_url":"http://arxiv.org/pdf/2510.02296v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Want image models that learn new concepts without retraining everything? Concept Neuron Selection (CNS) personalizes diffusion models incrementally by identifying and finetuning only concept-related neurons. Practical for user-specific generators and continual updates.","challenges":"ðŸŽ¯ Key problems tackled: - Incrementally updating diffusion models is computationally challenging. - Catastrophic forgetting when adding new concepts. - Need to preserve zero-shot text-to-image capabilities while personalizing. - Reduce memory/storage and processing time for continual personalization.","innovations":"âœ¨ Core innovations: - Concept Neuron Selection (CNS): identifies neurons in diffusion models tied to target concepts. - Incremental finetuning of selected concept neurons to add new concepts. - Joint preservation of previously learned concepts to mitigate forgetting. - Fusion-free operation (no costly model merging), minimizing extra storage and runtime. Novel twist: neuron-level selection inside diffusion models for continual personalization with minimal parameter changes.","experiments":"ðŸ“Š Most compelling quantitative result: Not specified in the paper. Main experimental breakthrough: CNS is reported to achieve state-of-the-art performance in continual personalization, outperforming prior methods on both single- and multi-concept personalization while using minimal parameter adjustments and enabling fusion-free, lower-memory operation.","insights":"ðŸ¤” Potential next steps & applications: - Explore automated neuron selection policies or learnable selectors for broader model families (e.g., text encoders, other generative backbones). - Investigate on-device continual personalization and privacy-preserving updates using CNS-style sparse updates. Applications: personalized avatars, adaptive content creation, and efficient multi-user model deployments. Could neuron-level personalization become the standard for lightweight, continual model adaptation?","keywords":["Continual learning","Diffusion models","Personalization","Concept neuron","Catastrophic forgetting","Zero-shot","Model efficiency","Fusion-free"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want image models that learn new concepts without retraining everything? Concept Neuron Selection (CNS) personalizes diffusion models incrementally by identifying and finetuning only concept-related neurons. Practical for user-specific generators and continual updates.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Incrementally updating diffusion models is computationally challenging. - Catastrophic forgetting whe...","analyzed_at":"2025-10-05T08:07:20.274Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.02295v1","arxiv_id":"2510.02295v1","title":"VideoNSA: Native Sparse Attention Scales Video Understanding","abstract":"Video understanding in multimodal language models remains limited by context\nlength: models often miss key transition frames and struggle to maintain\ncoherence across long time scales. To address this, we adapt Native Sparse\nAttention (NSA) to video-language models. Our method, VideoNSA, adapts\nQwen2.5-VL through end-to-end training on a 216K video instruction dataset. We\nemploy a hardware-aware hybrid approach to attention, preserving dense\nattention for text, while employing NSA for video. Compared to\ntoken-compression and training-free sparse baselines, VideoNSA achieves\nimproved performance on long-video understanding, temporal reasoning, and\nspatial benchmarks. Further ablation analysis reveals four key findings: (1)\nreliable scaling to 128K tokens; (2) an optimal global-local attention\nallocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)\nthe learnable combined sparse attention help induce dynamic attention sinks.","authors":["Enxin Song","Wenhao Chai","Shusheng Yang","Ethan Armand","Xiaojun Shan","Haiyang Xu","Jianwen Xie","Zhuowen Tu"],"published":"2025-10-02T17:58:54Z","updated":"2025-10-02T17:58:54Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.02295v1","pdf_url":"http://arxiv.org/pdf/2510.02295v1.pdf","scraped_at":"2025-10-05T08:04:09.771Z","analysis":{"introduction":"ðŸš€ Ever tried to make sense of a 2-hour video with an LLM and hit a context wall? VideoNSA adapts Native Sparse Attention to video-language models, letting multimodal LLMs keep coherent, long-range video context â€” great for long-video understanding and reasoning.","challenges":"ðŸŽ¯ Problems addressed: - Short context windows miss key transition frames and long-range dependencies. - Dense attention is costly and hardware-limited for long video sequences. - Existing token-compression or training-free sparsity fail on temporal/spatial benchmarks.","innovations":"âœ¨ Core ideas: - Adapted Native Sparse Attention (NSA) for video-language models. - Hardware-aware hybrid attention: dense for text, NSA for video tokens. - End-to-end training of Qwen2.5-VL on a 216K video instruction dataset. - Learnable combined sparse attention inducing dynamic attention sinks.","experiments":"ðŸ“Š Key demonstrated breakthroughs: - Scales reliably to 128K tokens and improves long-video understanding, temporal reasoning, and spatial benchmarks vs token-compression and training-free sparse baselines. - Numeric gains: Not specified in the paper.","insights":"ðŸ¤” What's next: - Research: Combine NSA with retrieval or memory modules for even longer-term multimodal context; explore adaptive sparsity schedules per task. - Applications: long-form video summarization, sports/behavior analytics, surveillance. Could this enable truly persistent multimodal memory?","keywords":["VideoNSA","Native Sparse Attention","NSA","Qwen2.5-VL","long-context","video-language","sparse-attention","128K tokens","temporal reasoning","hybrid attention"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Ever tried to make sense of a 2-hour video with an LLM and hit a context wall? VideoNSA adapts Native Sparse Attention to video-language models, letting multimodal LLMs keep coherent, long-range video context â€” great for long-video understanding and reasoning.\n\n**Challenges:** ðŸŽ¯ Problems addressed: - Short context windows miss key transition frames and long-range dependencies. - Dense attention is costly and ha...","analyzed_at":"2025-10-05T08:07:18.744Z","model":"openai/gpt-5-mini"}},{"id":"hf_longcodezip__compress_long_context_for_code_language_models_1759651484780","title":"LongCodeZip: Compress Long Context for Code Language Models","abstract":"How to compress long code context? ðŸ“š\\nCheck out our LongCodeZip! Paper just got accepted to ASE 2025. ðŸ”¥\\nCode: https://github.com/YerbaPage/LongCodeZipPaper: https://huggingface.co/papers/2510.00446\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T02:07:13.310Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;645b0c3ec35da9c7afd95421&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg&quot;,&quot;fullname&quot;:&quot;Yuling&quot;,&quot;name&quot;:&quot;YerbaPage&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:255}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.6779364943504333},&quot;editors&quot;:[&quot;YerbaPage&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68df436560e76f4d99b9f50b&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;657f461be50ca9a699f8754d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a2153251b473de201ffd4df72c1b67fc.svg&quot;,&quot;fullname&quot;:&quot;Atharva Srivastava&quot;,&quot;name&quot;:&quot;computerponr&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:6},&quot;createdAt&quot;:&quot;2025-10-03T03:30:45.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;Congratulation boys!!\\n&quot;,&quot;html&quot;:&quot;Congratulation boys!!\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T03:30:45.115Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;657f461be50ca9a699f8754d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a2153251b473de201ffd4df72c1b67fc.svg&quot;,&quot;fullname&quot;:&quot;Atharva Srivastava&quot;,&quot;name&quot;:&quot;computerponr&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:6}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.5558434128761292},&quot;editors&quot;:[&quot;computerponr&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/a2153251b473de201ffd4df72c1b67fc.svg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ðŸ‘&quot;,&quot;users&quot;:[&quot;YerbaPage&quot;],&quot;count&quot;:1},{&quot;reaction&quot;:&quot;â¤ï¸&quot;,&quot;users&quot;:[&quot;YerbaPage&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68dfd776198a4f889fdcaca6&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;65bb837dbfb878f46c77de4c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/65bb837dbfb878f46c77de4c/PKyQ_-wTNH1Hyv5HxhWdX.jpeg&quot;,&quot;fullname&quot;:&quot;Prithiv Sakthi&quot;,&quot;name&quot;:&quot;prithivMLmods&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3324},&quot;createdAt&quot;:&quot;2025-10-03T14:02:30.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;Ranks function-level chunks conditional perplexions relative to the instruction, proportionating the ATB.\\nThis is awesome work, guys! Congratulations! ðŸ”¥ðŸ‘\\n\\n![3Pf5RWob0LBpyT0yMoM2-](https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/-95dGqfouU1goxqLXaD1R.png)\\n&quot;,&quot;html&quot;:&quot;Ranks function-level chunks conditional perplexions relative to the instruction, proportionating the ATB.This is awesome work, guys! Congratulations! ðŸ”¥ðŸ‘\\n\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T14:02:30.502Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;65bb837dbfb878f46c77de4c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/65bb837dbfb878f46c77de4c/PKyQ_-wTNH1Hyv5HxhWdX.jpeg&quot;,&quot;fullname&quot;:&quot;Prithiv Sakthi&quot;,&quot;name&quot;:&quot;prithivMLmods&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3324}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.6244713664054871},&quot;editors&quot;:[&quot;prithivMLmods&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/65bb837dbfb878f46c77de4c/PKyQ_-wTNH1Hyv5HxhWdX.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ðŸ¤—&quot;,&quot;users&quot;:[&quot;YerbaPage&quot;],&quot;count&quot;:1},{&quot;reaction&quot;:&quot;â¤ï¸&quot;,&quot;users&quot;:[&quot;YerbaPage&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68dfe47bcb032d78f0530f88&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66d1892971ba7a722e0c0d78&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pxLH2RFr3TNMP0NVSREMU.jpeg&quot;,&quot;fullname&quot;:&quot;Arthur EDMOND&quot;,&quot;name&quot;:&quot;Shumatsurontek&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:2},&quot;createdAt&quot;:&quot;2025-10-03T14:58:03.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;This is promising will try !&quot;,&quot;html&quot;:&quot;This is promising will try !\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T14:58:03.194Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66d1892971ba7a722e0c0d78&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pxLH2RFr3TNMP0NVSREMU.jpeg&quot;,&quot;fullname&quot;:&quot;Arthur EDMOND&quot;,&quot;name&quot;:&quot;Shumatsurontek&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:2}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9590368866920471},&quot;editors&quot;:[&quot;Shumatsurontek&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pxLH2RFr3TNMP0NVSREMU.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ðŸš€&quot;,&quot;users&quot;:[&quot;YerbaPage&quot;],&quot;count&quot;:1},{&quot;reaction&quot;:&quot;ðŸ‘&quot;,&quot;users&quot;:[&quot;YerbaPage&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68e07a88462eb02f83344a79&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267},&quot;createdAt&quot;:&quot;2025-10-04T01:38:16.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;This is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper. \\n\\nThe following papers were recommended by the Semantic Scholar API \\n\\n* [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://huggingface.co/papers/2509.13723) (2025)\\n* [SCOPE: A Generative Approach for LLM Prompt Compression](https://huggingface.co/papers/2508.15813) (2025)\\n* [Impact-driven Context Filtering For Cross-file Code Completion](https://huggingface.co/papers/2508.05970) (2025)\\n* [UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression](https://huggingface.co/papers/2509.15763) (2025)\\n* [PagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference](https://huggingface.co/papers/2509.04377) (2025)\\n* [SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion](https://huggingface.co/papers/2508.15495) (2025)\\n* [SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://huggingface.co/papers/2508.06447) (2025)\\n\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n\\n If you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\\n\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`&quot;,&quot;html&quot;:&quot;This is an automated message from the Librarian Bot. I found the following papers similar to this paper. \\nThe following papers were recommended by the Semantic Scholar API \\n\\nDSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning (2025)\\nSCOPE: A Generative Approach for LLM Prompt Compression (2025)\\nImpact-driven Context Filtering For Cross-file Code Completion (2025)\\nUniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression (2025)\\nPagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference (2025)\\nSynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion (2025)\\nSlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning (2025)\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n If you want recommendations for any Paper on Hugging Face checkout this Space\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: \\n\\n@librarian-bot\\n\\t recommend\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T01:38:16.670Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.6809963583946228},&quot;editors&quot;:[&quot;librarian-bot&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2510.00446&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68ddef306024653e8a3ed0e9&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;645b0c3ec35da9c7afd95421&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuling&quot;,&quot;user&quot;:&quot;YerbaPage&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yuling Shi&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-02T13:55:10.013Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ddef306024653e8a3ed0ea&quot;,&quot;name&quot;:&quot;Yichun Qian&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ddef306024653e8a3ed0eb&quot;,&quot;name&quot;:&quot;Hongyu Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ddef306024653e8a3ed0ec&quot;,&quot;name&quot;:&quot;Beijun Shen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ddef306024653e8a3ed0ed&quot;,&quot;name&quot;:&quot;Xiaodong Gu&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-10-01T02:54:57.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-03T00:37:13.283Z&quot;,&quot;title&quot;:&quot;LongCodeZip: Compress Long Context for Code Language Models&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;645b0c3ec35da9c7afd95421&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuling&quot;,&quot;user&quot;:&quot;YerbaPage&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Code generation under long contexts is becoming increasingly critical as\\nLarge Language Models (LLMs) are required to reason over extensive information\\nin the codebase. While recent advances enable code LLMs to process long inputs,\\nhigh API costs and generation latency remain substantial bottlenecks. Existing\\ncontext pruning techniques, such as LLMLingua, achieve promising results for\\ngeneral text but overlook code-specific structures and dependencies, leading to\\nsuboptimal performance in programming tasks. In this paper, we propose\\nLongCodeZip, a novel plug-and-play code compression framework designed\\nspecifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)\\ncoarse-grained compression, which identifies and ranks function-level chunks\\nusing conditional perplexity with respect to the instruction, retaining only\\nthe most relevant functions; and (2) fine-grained compression, which segments\\nretained functions into blocks based on perplexity and selects an optimal\\nsubset under an adaptive token budget to maximize relevance. Evaluations across\\nmultiple tasks, including code completion, summarization, and question\\nanswering, show that LongCodeZip consistently outperforms baseline methods,\\nachieving up to a 5.6x compression ratio without degrading task performance. By\\neffectively reducing context size while preserving essential information,\\nLongCodeZip enables LLMs to better scale to real-world, large-scale code\\nscenarios, advancing the efficiency and capability of code intelligence\\napplications.&quot;,&quot;upvotes&quot;:81,&quot;discussionId&quot;:&quot;68ddef316024653e8a3ed0ee&quot;,&quot;githubRepo&quot;:&quot;https://github.com/YerbaPage/LongCodeZip&quot;,&quot;ai_summary&quot;:&quot;LongCodeZip is a code compression framework for LLMs that uses dual-stage compression to reduce context size without degrading performance, improving efficiency in code intelligence applications.&quot;,&quot;ai_keywords&quot;:[&quot;Large Language Models&quot;,&quot;code LLMs&quot;,&quot;context pruning&quot;,&quot;LLMLingua&quot;,&quot;conditional perplexity&quot;,&quot;function-level chunks&quot;,&quot;fine-grained compression&quot;,&quot;token budget&quot;,&quot;code completion&quot;,&quot;code summarization&quot;,&quot;question answering&quot;,&quot;code intelligence applications&quot;],&quot;githubStars&quot;:61,&quot;organization&quot;:{&quot;_id&quot;:&quot;6724d0b84c0a2bf36e39a226&quot;,&quot;name&quot;:&quot;Stanford-University&quot;,&quot;fullname&quot;:&quot;Stanford University&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/6724cfba409a4f96ce1d773b/BjPcf9AfmEU3WeSVb33s8.png&quot;}},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;645b0c3ec35da9c7afd95421&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuling&quot;,&quot;user&quot;:&quot;YerbaPage&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;678781fb6bc613abc37f5ad4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/678781fb6bc613abc37f5ad4/7Se4ieXuSG3Vac_m0REYs.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;YANG&quot;,&quot;user&quot;:&quot;YangLCC&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b1a165de36d6ef979f636f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b1a165de36d6ef979f636f/Vk6rmQFZ3FSzwOAk3SqBr.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;fanwanx&quot;,&quot;user&quot;:&quot;FANTKwan&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b1a0d17c5844c6bcf091db&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bf933b81eb35b529741c1704ad8ef73b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;benqi&quot;,&quot;user&quot;:&quot;Aidabenk&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b1a02c46b13200819aec08&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b1a02c46b13200819aec08/83apjllqYocftrXgeeSp9.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;fenchenw&quot;,&quot;user&quot;:&quot;Fanwenyoo&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b19f81615a3737b5772b3b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2ca4c53715e82a7e17c4b631eaf34042.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Fenzhif&quot;,&quot;user&quot;:&quot;FANCERTA&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b192229becd2d044116314&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b192229becd2d044116314/IXDR0p9_-PcgVxJOnQZY-.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;S.Dong&quot;,&quot;user&quot;:&quot;Shiny-JI&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b1937446b132008197d856&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b1937446b132008197d856/t9N1XCp7e2--P-xSa8pbk.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;KSBMyu05&quot;,&quot;user&quot;:&quot;KangSubMi&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b194a5ecf00cb7a519d4f1&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b194a5ecf00cb7a519d4f1/hLNuSoM1kSTp0GElLpF2P.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Sally&quot;,&quot;user&quot;:&quot;CArriy&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b1963c73b4976b632ab4c4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b1963c73b4976b632ab4c4/0mFN3OvlUO1NH9hKvOfZR.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Aki&quot;,&quot;user&quot;:&quot;Caleboo&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b198a3b25d54b45106b857&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b198a3b25d54b45106b857/y8pqqLDaOSIBM2fKHrePn.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;SongHye&quot;,&quot;user&quot;:&quot;ADidennn&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b19a7d2a4cd18639e740fb&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b19a7d2a4cd18639e740fb/mJTfZWFJ6sPc7zNf2PmEM.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Elvisy&quot;,&quot;user&quot;:&quot;JEonColin&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:3,&quot;organization&quot;:{&quot;_id&quot;:&quot;6724d0b84c0a2bf36e39a226&quot;,&quot;name&quot;:&quot;Stanford-University&quot;,&quot;fullname&quot;:&quot;Stanford University&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/6724cfba409a4f96ce1d773b/BjPcf9AfmEU3WeSVb33s8.png&quot;}}\"> Papers arxiv:2510.00446","authors":[],"published":"2025-10-05","updated":"2025-10-05T08:04:44.784Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.00446","pdf_url":"","scraped_at":"2025-10-05T08:04:44.784Z","abstract_quality":6,"analysis":{"introduction":"ðŸš€ Struggling with huge code contexts that spike API costs and slow down LLMs? LongCodeZip is a plugâ€‘andâ€‘play framework that compresses long code context for code LLMs so they scale to real-world repos while preserving performance â€” ideal for dev tools & code search.","challenges":"ðŸŽ¯ Problems tackled: - High API costs & generation latency when LLMs process long code contexts. - Generic pruning (e.g., LLMLingua) ignores code structure and dependencies â†’ suboptimal for programming tasks. - Difficulty scaling LLM reasoning across large, multi-file codebases.","innovations":"âœ¨ Key innovations: - Coarse-grained stage: rank function-level chunks by conditional perplexity w.r.t. the instruction and retain the most relevant functions. - Fine-grained stage: segment retained functions into perplexity-based blocks and select an optimal subset under an adaptive token budget. - Novelty: code-structure-aware, dual-stage perplexity compression tailored for code LLMs.","experiments":"ðŸ“Š Results: Evaluated on code completion, summarization, and QA â€” LongCodeZip consistently outperforms baselines and achieves up to 5.6Ã— compression without degrading task performance. Proof: substantial context reduction while preserving accuracy.","insights":"ðŸ¤” What's next? - Research idea: replace or augment perplexity ranking with learned relevance models fine-tuned on code signals. - Apply dynamic/online compression during editing or CI to reduce latency and cost in practice. Broader impact: cheaper, scalable code LLM APIs and more practical repo-level assistants. What apps become possible at scale?","keywords":["Large Language Models","code LLMs","context pruning","conditional perplexity","function-level chunks","fine-grained compression","adaptive token budget","code completion","code summarization","question answering"],"category":"machine_learning","relevance_score":9,"technical_depth":"intermediate","summary":"**Introduction:** ðŸš€ Struggling with huge code contexts that spike API costs and slow down LLMs? LongCodeZip is a plugâ€‘andâ€‘play framework that compresses long code context for code LLMs so they scale to real-world repos while preserving performance â€” ideal for dev tools & code search.\n\n**Challenges:** ðŸŽ¯ Problems tackled: - High API costs & generation latency when LLMs process long code contexts. - Generic pruning (e.g., LLMLingua) ...","analyzed_at":"2025-10-05T08:07:49.126Z","model":"openai/gpt-5-mini"}},{"id":"hf_self_forcing____towards_minute_scale_high_quality_video_generation_1759651487938","title":"Self-Forcing++: Towards Minute-Scale High-Quality Video Generation","abstract":"Please checkout our project page at: https://self-forcing-plus-plus.github.io/, thank you!\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T01:51:50.299Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;65862671e878be571bf9fc52&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg&quot;,&quot;fullname&quot;:&quot;cuijiaxing&quot;,&quot;name&quot;:&quot;cuijiaxing&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.5630297660827637},&quot;editors&quot;:[&quot;cuijiaxing&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68df460b64e55d4d54d1c98b&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66207f34bd2dca4c783c6357&quot;,&quot;avatarUrl&quot;:&quot;/avatars/edd20b005c9a8595162a553a68bd614d.svg&quot;,&quot;fullname&quot;:&quot;iamyourgrandpapa&quot;,&quot;name&quot;:&quot;iamyoudaddy&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false},&quot;createdAt&quot;:&quot;2025-10-03T03:42:03.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;Great work ðŸ‘ðŸ‘ðŸ‘&quot;,&quot;html&quot;:&quot;Great work ðŸ‘ðŸ‘ðŸ‘\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T03:42:03.160Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66207f34bd2dca4c783c6357&quot;,&quot;avatarUrl&quot;:&quot;/avatars/edd20b005c9a8595162a553a68bd614d.svg&quot;,&quot;fullname&quot;:&quot;iamyourgrandpapa&quot;,&quot;name&quot;:&quot;iamyoudaddy&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.589540421962738},&quot;editors&quot;:[&quot;iamyoudaddy&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/edd20b005c9a8595162a553a68bd614d.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68e079cb8c37cbb469f5f6b6&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267},&quot;createdAt&quot;:&quot;2025-10-04T01:35:07.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;This is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper. \\n\\nThe following papers were recommended by the Semantic Scholar API \\n\\n* [Rolling Forcing: Autoregressive Long Video Diffusion in Real Time](https://huggingface.co/papers/2509.25161) (2025)\\n* [LongLive: Real-time Interactive Long Video Generation](https://huggingface.co/papers/2509.22622) (2025)\\n* [InfVSR: Breaking Length Limits of Generic Video Super-Resolution](https://huggingface.co/papers/2510.00948) (2025)\\n* [WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception](https://huggingface.co/papers/2508.15720) (2025)\\n* [Autoregressive Video Generation beyond Next Frames Prediction](https://huggingface.co/papers/2509.24081) (2025)\\n* [LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE](https://huggingface.co/papers/2509.21790) (2025)\\n* [LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation](https://huggingface.co/papers/2508.03694) (2025)\\n\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n\\n If you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\\n\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`&quot;,&quot;html&quot;:&quot;This is an automated message from the Librarian Bot. I found the following papers similar to this paper. \\nThe following papers were recommended by the Semantic Scholar API \\n\\nRolling Forcing: Autoregressive Long Video Diffusion in Real Time (2025)\\nLongLive: Real-time Interactive Long Video Generation (2025)\\nInfVSR: Breaking Length Limits of Generic Video Super-Resolution (2025)\\nWorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception (2025)\\nAutoregressive Video Generation beyond Next Frames Prediction (2025)\\nLongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE (2025)\\nLongVie: Multimodal-Guided Controllable Ultra-Long Video Generation (2025)\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n If you want recommendations for any Paper on Hugging Face checkout this Space\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: \\n\\n@librarian-bot\\n\\t recommend\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T01:35:07.190Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.6892101168632507},&quot;editors&quot;:[&quot;librarian-bot&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2510.02283&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68df2b55df49fb0df1e03be2&quot;,&quot;name&quot;:&quot;Justin Cui&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df2b55df49fb0df1e03be3&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6381c5d63680a7cf34e08ca9&quot;,&quot;avatarUrl&quot;:&quot;/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;wujie10558@gmail.com&quot;,&quot;user&quot;:&quot;wujie10&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jie Wu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:32:59.026Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df2b55df49fb0df1e03be4&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;637f0eb22438d7485b8ef5d7&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ming Li&quot;,&quot;user&quot;:&quot;limingcv&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ming Li&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:33:02.772Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df2b55df49fb0df1e03be5&quot;,&quot;name&quot;:&quot;Tao Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df2b55df49fb0df1e03be6&quot;,&quot;name&quot;:&quot;Xiaojie Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df2b55df49fb0df1e03be7&quot;,&quot;name&quot;:&quot;Rui Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df2b55df49fb0df1e03be8&quot;,&quot;name&quot;:&quot;Andrew Bai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df2b55df49fb0df1e03be9&quot;,&quot;name&quot;:&quot;Yuanhao Ban&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df2b55df49fb0df1e03bea&quot;,&quot;name&quot;:&quot;Cho-Jui Hsieh&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/Fb3ffDmXrslDcQSJH51MW.mp4&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/65862671e878be571bf9fc52/I0qCOZqF-XJo2Xg3i__RJ.mp4&quot;],&quot;publishedAt&quot;:&quot;2025-10-02T17:55:42.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-03T00:21:50.292Z&quot;,&quot;title&quot;:&quot;Self-Forcing++: Towards Minute-Scale High-Quality Video Generation&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;65862671e878be571bf9fc52&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;cuijiaxing&quot;,&quot;user&quot;:&quot;cuijiaxing&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Diffusion models have revolutionized image and video generation, achieving\\nunprecedented visual quality. However, their reliance on transformer\\narchitectures incurs prohibitively high computational costs, particularly when\\nextending generation to long videos. Recent work has explored autoregressive\\nformulations for long video generation, typically by distilling from\\nshort-horizon bidirectional teachers. Nevertheless, given that teacher models\\ncannot synthesize long videos, the extrapolation of student models beyond their\\ntraining horizon often leads to pronounced quality degradation, arising from\\nthe compounding of errors within the continuous latent space. In this paper, we\\npropose a simple yet effective approach to mitigate quality degradation in\\nlong-horizon video generation without requiring supervision from long-video\\nteachers or retraining on long video datasets. Our approach centers on\\nexploiting the rich knowledge of teacher models to provide guidance for the\\nstudent model through sampled segments drawn from self-generated long videos.\\nOur method maintains temporal consistency while scaling video length by up to\\n20x beyond teacher's capability, avoiding common issues such as over-exposure\\nand error-accumulation without recomputing overlapping frames like previous\\nmethods. When scaling up the computation, our method shows the capability of\\ngenerating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the\\nmaximum span supported by our base model's position embedding and more than 50x\\nlonger than that of our baseline model. Experiments on standard benchmarks and\\nour proposed improved benchmark demonstrate that our approach substantially\\noutperforms baseline methods in both fidelity and consistency. Our long-horizon\\nvideos demo can be found at https://self-forcing-plus-plus.github.io/&quot;,&quot;upvotes&quot;:67,&quot;discussionId&quot;:&quot;68df2b56df49fb0df1e03beb&quot;,&quot;projectPage&quot;:&quot;https://self-forcing-plus-plus.github.io/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/justincui03/Self-Forcing-Plus-Plus&quot;,&quot;ai_summary&quot;:&quot;A method is proposed to enhance long-horizon video generation by using sampled segments from self-generated long videos to guide student models, maintaining quality and consistency without additional supervision or retraining.&quot;,&quot;ai_keywords&quot;:[&quot;diffusion models&quot;,&quot;transformer architectures&quot;,&quot;autoregressive formulations&quot;,&quot;bidirectional teachers&quot;,&quot;latent space&quot;,&quot;quality degradation&quot;,&quot;temporal consistency&quot;,&quot;position embedding&quot;],&quot;githubStars&quot;:44,&quot;organization&quot;:{&quot;_id&quot;:&quot;67d1140985ea0644e2f14b99&quot;,&quot;name&quot;:&quot;ByteDance-Seed&quot;,&quot;fullname&quot;:&quot;ByteDance Seed&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png&quot;}},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;65862671e878be571bf9fc52&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b2a1b939f3112b476e7641e0c5fd2dc7.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;cuijiaxing&quot;,&quot;user&quot;:&quot;cuijiaxing&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;637f0eb22438d7485b8ef5d7&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/637f0eb22438d7485b8ef5d7/70h7dekqj7LuBobOXckmJ.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ming Li&quot;,&quot;user&quot;:&quot;limingcv&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6227a1133988a4f0e9309089&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e10570cfda8c2d6dbecd2a51eedf8799.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Andrew Bai&quot;,&quot;user&quot;:&quot;andrewbai&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64d9d1e80f992cf1cea308bb&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64d9d1e80f992cf1cea308bb/zosqW7iHDSxOA8SJtB7V-.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;zhao xu&quot;,&quot;user&quot;:&quot;zhaoxu98&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;654137252274785dbbbf8e08&quot;,&quot;avatarUrl&quot;:&quot;/avatars/208fcafcb895a7bd7eaf8d436c6bcd7e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuanhao Ban&quot;,&quot;user&quot;:&quot;banyh2000&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64f0f0fb193de80eda38c1ea&quot;,&quot;avatarUrl&quot;:&quot;/avatars/23afd41a1d1a0555150dc5692f5a5cdd.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tong Xie&quot;,&quot;user&quot;:&quot;txie&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66207f34bd2dca4c783c6357&quot;,&quot;avatarUrl&quot;:&quot;/avatars/edd20b005c9a8595162a553a68bd614d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;iamyourgrandpapa&quot;,&quot;user&quot;:&quot;iamyoudaddy&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62ea79dd01ed9b0e8f61ccd3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chengsong Huang&quot;,&quot;user&quot;:&quot;ChengsongHuang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67abb26debe64eaa3a624bd7&quot;,&quot;avatarUrl&quot;:&quot;/avatars/39259308f5aeda7f80a5489335554913.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zonglin Lyu&quot;,&quot;user&quot;:&quot;ucfzl&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67fdc24ad2c9d1369d390f01&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/51JiReMgHiFHZiJ9OVdaf.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jingran Zhang&quot;,&quot;user&quot;:&quot;zhangjingran&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67feacb1a8fe680ca972f811&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bca39bf3610d0f62037fe0aaf4890e02.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Cyna&quot;,&quot;user&quot;:&quot;zjr03&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:1,&quot;organization&quot;:{&quot;_id&quot;:&quot;67d1140985ea0644e2f14b99&quot;,&quot;name&quot;:&quot;ByteDance-Seed&quot;,&quot;fullname&quot;:&quot;ByteDance Seed&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png&quot;}}\"> Papers arxiv:2510.02283","authors":[],"published":"2025-10-05","updated":"2025-10-05T08:04:47.938Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.02283","pdf_url":"","scraped_at":"2025-10-05T08:04:47.938Z","abstract_quality":6,"analysis":{"introduction":"ðŸš€ Want minute-scale, high-quality video from diffusion models without retraining on long videos? Self-Forcing++ guides autoregressive student models using sampled segments from self-generated long videos to scale length up to minute-scale â€” a step-change for long-horizon video gen. Benefits: better long-video synthesis for researchers, creators, simulators.","challenges":"ðŸŽ¯ Problems tackled: - High compute cost of transformer-based diffusion for long videos. - Teacher models canâ€™t synthesize long videos â†’ students degrade when extrapolating. - Compounding errors and over-exposure in continuous latent space.","innovations":"âœ¨ Key ideas: - Use sampled segments from self-generated long videos as guidance for students. - Train autoregressive student distilled from short-horizon bidirectional teachers without long-video supervision. - Maintain temporal consistency and avoid recomputing overlapping frames. Novel twist: leveraging a teacherâ€™s own self-generated long segments to guide extrapolation beyond the teacherâ€™s horizon.","experiments":"ðŸ“Š Main quantitative result: Generated videos up to 4 minutes 15 seconds â€” ~99.9% of the base model's position-embedding span, >50Ã— longer than the baseline and up to 20Ã— beyond the teacherâ€™s capability. Experiments show substantial fidelity and consistency gains over baselines (details in paper).","insights":"ðŸ¤” Whatâ€™s next? - Research direction: Combine self-forcing guidance with retrieval or multimodal conditioning to improve long-term semantic coherence. - Application: long-form content, simulation, and virtual-world generation. Could this enable coherent, editable minute-scale synthetic scenes?","keywords":["video generation","diffusion models","autoregressive","long-horizon","self-generated guidance","temporal consistency","position embedding"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want minute-scale, high-quality video from diffusion models without retraining on long videos? Self-Forcing++ guides autoregressive student models using sampled segments from self-generated long videos to scale length up to minute-scale â€” a step-change for long-horizon video gen. Benefits: better long-video synthesis for researchers, creators, simulators.","analyzed_at":"2025-10-05T08:07:49.080Z","model":"openai/gpt-5-mini"}},{"id":"hf_exgrpo__learning_to_reason_from_experience_1759651491425","title":"ExGRPO: Learning to Reason from Experience","abstract":"We present a systematic study of what makes reasoning experiences valuable in RLVR and propose a framework that leverages these insights to exploit high-value experiences for efficient RLVR.\\nModel Collection: https://huggingface.co/collections/rzzhan/exgrpo-68d8e302efdfe325187d5c96\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T16:37:02.337Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;62495cb96ee7ee6b646db130&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;,&quot;fullname&quot;:&quot;Runzhe Zhan&quot;,&quot;name&quot;:&quot;rzzhan&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3}},&quot;numEdits&quot;:1,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.8738799691200256},&quot;editors&quot;:[&quot;rzzhan&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68dfdd5bcb032d78f051fabb&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63f3502a520c14618925825a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e986a2a6625e7be6890616a417f908d2.svg&quot;,&quot;fullname&quot;:&quot;Yafu Li&quot;,&quot;name&quot;:&quot;yaful&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:4},&quot;createdAt&quot;:&quot;2025-10-03T14:27:39.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;Github: https://github.com/ElliottYan/LUFFY/tree/main/ExGRPO&quot;,&quot;html&quot;:&quot;Github: https://github.com/ElliottYan/LUFFY/tree/main/ExGRPO\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T14:27:39.338Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63f3502a520c14618925825a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e986a2a6625e7be6890616a417f908d2.svg&quot;,&quot;fullname&quot;:&quot;Yafu Li&quot;,&quot;name&quot;:&quot;yaful&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:4}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.6876582503318787},&quot;editors&quot;:[&quot;yaful&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/e986a2a6625e7be6890616a417f908d2.svg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ðŸš€&quot;,&quot;users&quot;:[&quot;rzzhan&quot;,&quot;yoonkg&quot;],&quot;count&quot;:2}],&quot;isReport&quot;:false},&quot;replies&quot;:[{&quot;id&quot;:&quot;68dffb0bbad50a573804827a&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;62495cb96ee7ee6b646db130&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;,&quot;fullname&quot;:&quot;Runzhe Zhan&quot;,&quot;name&quot;:&quot;rzzhan&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3},&quot;createdAt&quot;:&quot;2025-10-03T16:34:19.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:true,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;Added to the metadata.&quot;,&quot;html&quot;:&quot;Added to the metadata.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T16:35:01.404Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;62495cb96ee7ee6b646db130&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;,&quot;fullname&quot;:&quot;Runzhe Zhan&quot;,&quot;name&quot;:&quot;rzzhan&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3}},&quot;numEdits&quot;:1,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.860673725605011},&quot;editors&quot;:[&quot;rzzhan&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false,&quot;parentCommentId&quot;:&quot;68dfdd5bcb032d78f051fabb&quot;}}]},{&quot;id&quot;:&quot;68e079efc0a1176a4dee8f5d&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267},&quot;createdAt&quot;:&quot;2025-10-04T01:35:43.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;This is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper. \\n\\nThe following papers were recommended by the Semantic Scholar API \\n\\n* [Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse](https://huggingface.co/papers/2509.25808) (2025)\\n* [ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism](https://huggingface.co/papers/2508.11356) (2025)\\n* [More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration](https://huggingface.co/papers/2510.02227) (2025)\\n* [MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources](https://huggingface.co/papers/2509.21268) (2025)\\n* [Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR](https://huggingface.co/papers/2508.14029) (2025)\\n* [CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning](https://huggingface.co/papers/2509.25004) (2025)\\n* [Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle](https://huggingface.co/papers/2508.05612) (2025)\\n\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n\\n If you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\\n\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`&quot;,&quot;html&quot;:&quot;This is an automated message from the Librarian Bot. I found the following papers similar to this paper. \\nThe following papers were recommended by the Semantic Scholar API \\n\\nImproving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse (2025)\\nETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism (2025)\\nMore Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration (2025)\\nMMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources (2025)\\nBeyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR (2025)\\nCLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning (2025)\\nShuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle (2025)\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n If you want recommendations for any Paper on Hugging Face checkout this Space\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: \\n\\n@librarian-bot\\n\\t recommend\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T01:35:43.760Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7339387536048889},&quot;editors&quot;:[&quot;librarian-bot&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2510.02245&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd2&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;62495cb96ee7ee6b646db130&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Runzhe Zhan&quot;,&quot;user&quot;:&quot;rzzhan&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Runzhe Zhan&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:29:46.232Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd3&quot;,&quot;name&quot;:&quot;Yafu Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd4&quot;,&quot;name&quot;:&quot;Zhi Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd5&quot;,&quot;name&quot;:&quot;Xiaoye Qu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd6&quot;,&quot;name&quot;:&quot;Dongrui Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd7&quot;,&quot;name&quot;:&quot;Jing Shao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd8&quot;,&quot;name&quot;:&quot;Derek F. Wong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd9&quot;,&quot;name&quot;:&quot;Yu Cheng&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-10-02T17:31:30.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-03T01:31:39.660Z&quot;,&quot;title&quot;:&quot;ExGRPO: Learning to Reason from Experience&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;62495cb96ee7ee6b646db130&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Runzhe Zhan&quot;,&quot;user&quot;:&quot;rzzhan&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\\nfor improving the reasoning ability of large language models. However, standard\\non-policy training discards rollout experiences after a single update, leading\\nto computational inefficiency and instability. While prior work on RL has\\nhighlighted the benefits of reusing past experience, the role of experience\\ncharacteristics in shaping learning dynamics of large reasoning models remains\\nunderexplored. In this paper, we are the first to investigate what makes a\\nreasoning experience valuable and identify rollout correctness and entropy as\\neffective indicators of experience value. Based on these insights, we propose\\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\\norganizes and prioritizes valuable experiences, and employs a mixed-policy\\nobjective to balance exploration with experience exploitation. Experiments on\\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\\nimproves reasoning performance on mathematical/general benchmarks, with an\\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\\nstabilizes training on both stronger and weaker models where on-policy methods\\nfail. These results highlight principled experience management as a key\\ningredient for efficient and scalable RLVR.&quot;,&quot;upvotes&quot;:59,&quot;discussionId&quot;:&quot;68df3b52df49fb0df1e03cda&quot;,&quot;githubRepo&quot;:&quot;https://github.com/ElliottYan/LUFFY/tree/main/ExGRPO&quot;,&quot;ai_summary&quot;:&quot;ExGRPO, a framework that prioritizes valuable reasoning experiences, improves and stabilizes reinforcement learning from verifiable rewards for large language models.&quot;,&quot;ai_keywords&quot;:[&quot;reinforcement learning from verifiable rewards&quot;,&quot;RLVR&quot;,&quot;on-policy training&quot;,&quot;rollout experiences&quot;,&quot;experience characteristics&quot;,&quot;rollout correctness&quot;,&quot;entropy&quot;,&quot;ExGRPO&quot;,&quot;Experiential Group Relative Policy Optimization&quot;,&quot;mixed-policy objective&quot;,&quot;exploration&quot;,&quot;experience exploitation&quot;,&quot;reasoning performance&quot;,&quot;mathematical benchmarks&quot;,&quot;general benchmarks&quot;],&quot;githubStars&quot;:328},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;62495cb96ee7ee6b646db130&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Runzhe Zhan&quot;,&quot;user&quot;:&quot;rzzhan&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;682fe5e1c8ddcbd4645ec29d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/843a5c6ba36230c02cce4a8bf9d47319.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Mehmet YÄ±lmaz&quot;,&quot;user&quot;:&quot;mY1lmaz&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;682fe2641af275ff5c4a06e0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f939d34170c74fc873a2abb6e97680ff.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Emily Carter&quot;,&quot;user&quot;:&quot;EmilyC11&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68075d12dadb28dddc14a509&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7a9d86c6fb56ece2ed5bf36350142612.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Finch&quot;,&quot;user&quot;:&quot;KKKepler&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68076f20757bde45c40abdd8&quot;,&quot;avatarUrl&quot;:&quot;/avatars/5eca1d2a379774310e7f7874a5be0013.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;rao&quot;,&quot;user&quot;:&quot;bingqing1&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63f3502a520c14618925825a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e986a2a6625e7be6890616a417f908d2.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yafu Li&quot;,&quot;user&quot;:&quot;yaful&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;680b35fcdf6ff26584543fee&quot;,&quot;avatarUrl&quot;:&quot;/avatars/baf73994882d15e47583fca812ca5f50.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Michael Brooks&quot;,&quot;user&quot;:&quot;MichaelBrooks&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6830504c47fb9b0c7253f0dc&quot;,&quot;avatarUrl&quot;:&quot;/avatars/48a7033fa83c8a2cdcd9c90dea7d5199.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ada Nwosu&quot;,&quot;user&quot;:&quot;AdaNwosu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6808ac6516fed0aa450a26c4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/895088b3f28ec57ccf825ad4275df117.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Dan Schuurmans&quot;,&quot;user&quot;:&quot;Schuurmans&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68075e1a0a34ff7fff3d9c86&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f1a59c5e7548fee66112b0b78a40841e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Clair&quot;,&quot;user&quot;:&quot;DanClair77&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;680771da2df2acd9cea8a282&quot;,&quot;avatarUrl&quot;:&quot;/avatars/cffe5727d84290d9f0589cba111e4776.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;zhu&quot;,&quot;user&quot;:&quot;haoran7&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64264095ba51f8a2136946a0&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhaochen Su&quot;,&quot;user&quot;:&quot;Warrieryes&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:2}\"> Papers arxiv:2510.02245","authors":[],"published":"2025-10-05","updated":"2025-10-05T08:04:51.425Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.02245","pdf_url":"","scraped_at":"2025-10-05T08:04:51.425Z","abstract_quality":5,"analysis":{"introduction":"ðŸš€ Want LLMs that learn reasoning faster and more stably? ExGRPO identifies and exploits \"high-value\" reasoning experiences to make Reinforcement Learning from Verifiable Rewards (RLVR) far more efficient. Big win for researchers training 1.5Bâ€“8B reasoning models. ðŸ”¥","challenges":"ðŸŽ¯ Problems tackled: - On-policy RLVR discards rollouts after one update â†’ computational inefficiency & instability. - Which past experiences are worth reusing is unclear for reasoning models. - Prior replay techniques arenâ€™t tailored to reasoning-specific signals.","innovations":"âœ¨ Key ideas: - Identifies experience value via rollout correctness & entropy. - Organizes/prioritizes experiences (experience groups). - Mixed-policy objective to balance exploration vs. exploiting high-value experiences. Novelty: principled experience management for RLVR.","experiments":"ðŸ“Š Main result: ExGRPO improves reasoning on math/general benchmarks across 1.5Bâ€“8B backbones, giving an average gain of +3.5 / +7.6 points over on-policy RLVR, and stabilizes training where on-policy methods fail. Concrete proof of efficient, scalable RLVR.","insights":"ðŸ¤” What's next? - Apply experience-aware replay to multimodal or retrieval-augmented LLMs to see if gains extend beyond text reasoning. - Combine ExGRPO with curriculum or self-play to synthesize harder problems for continuous improvement. Could this make automated problem-solving systems more reliable?","keywords":["reinforcement learning from verifiable rewards","RLVR","on-policy training","rollout experiences","experience characteristics","rollout correctness","entropy","ExGRPO","Experiential Group Relative Policy Optimization","mixed-policy objective","exploration","experience exploitation","reasoning performance","mathematical benchmarks","general benchmarks"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want LLMs that learn reasoning faster and more stably? ExGRPO identifies and exploits \"high-value\" reasoning experiences to make Reinforcement Learning from Verifiable Rewards (RLVR) far more efficient. Big win for researchers training 1.5Bâ€“8B reasoning models. ðŸ”¥\n\n**Challenges:** ðŸŽ¯ Problems tackled: - On-policy RLVR discards rollouts after one update â†’ computational inefficiency & instability. - Which past exp...","analyzed_at":"2025-10-05T08:07:43.426Z","model":"openai/gpt-5-mini"}},{"id":"hf_stockbench__can_llm_agents_trade_stocks_profitably_in_real_world_markets__1759651499761","title":"StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?","abstract":"Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T02:08:58.125Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;name&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:118}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9109583497047424},&quot;editors&quot;:[&quot;taesiri&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ðŸ¤—&quot;,&quot;users&quot;:[&quot;TranSirius&quot;,&quot;RicardoL1u&quot;,&quot;JK-TK&quot;,&quot;Sharif19&quot;],&quot;count&quot;:4}],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68e079adcb032d78f068078d&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267},&quot;createdAt&quot;:&quot;2025-10-04T01:34:37.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;This is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper. \\n\\nThe following papers were recommended by the Semantic Scholar API \\n\\n* [QTMRL: An Agent for Quantitative Trading Decision-Making Based on Multi-Indicator Guided Reinforcement Learning](https://huggingface.co/papers/2508.20467) (2025)\\n* [AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions](https://huggingface.co/papers/2508.11152) (2025)\\n* [Adaptive Alpha Weighting with PPO: Enhancing Prompt-Based LLM-Generated Alphas in Quant Trading](https://huggingface.co/papers/2509.01393) (2025)\\n* [QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading](https://huggingface.co/papers/2509.09995) (2025)\\n* [TradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis](https://huggingface.co/papers/2508.17565) (2025)\\n* [Sentiment-Aware Mean-Variance Portfolio Optimization for Cryptocurrencies](https://huggingface.co/papers/2508.16378) (2025)\\n* [MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial Trading](https://huggingface.co/papers/2509.05080) (2025)\\n\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n\\n If you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\\n\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`&quot;,&quot;html&quot;:&quot;This is an automated message from the Librarian Bot. I found the following papers similar to this paper. \\nThe following papers were recommended by the Semantic Scholar API \\n\\nQTMRL: An Agent for Quantitative Trading Decision-Making Based on Multi-Indicator Guided Reinforcement Learning (2025)\\nAlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions (2025)\\nAdaptive Alpha Weighting with PPO: Enhancing Prompt-Based LLM-Generated Alphas in Quant Trading (2025)\\nQuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading (2025)\\nTradingGroup: A Multi-Agent Trading System with Self-Reflection and Data-Synthesis (2025)\\nSentiment-Aware Mean-Variance Portfolio Optimization for Cryptocurrencies (2025)\\nMM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial Trading (2025)\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n If you want recommendations for any Paper on Hugging Face checkout this Space\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: \\n\\n@librarian-bot\\n\\t recommend\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T01:34:37.023Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7226567268371582},&quot;editors&quot;:[&quot;librarian-bot&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68e0ead7e83908c1fc417e60&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;648a374f00f7a3374ee64b99&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/648a374f00f7a3374ee64b99/YPwSOrronoozwHbJchPn3.jpeg&quot;,&quot;fullname&quot;:&quot;Caleb Fahlgren&quot;,&quot;name&quot;:&quot;cfahlgren1&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:true,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:264},&quot;createdAt&quot;:&quot;2025-10-04T09:37:27.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;really cool paper!&quot;,&quot;html&quot;:&quot;really cool paper!\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T09:37:27.179Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;648a374f00f7a3374ee64b99&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/648a374f00f7a3374ee64b99/YPwSOrronoozwHbJchPn3.jpeg&quot;,&quot;fullname&quot;:&quot;Caleb Fahlgren&quot;,&quot;name&quot;:&quot;cfahlgren1&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:true,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:264}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9305157661437988},&quot;editors&quot;:[&quot;cfahlgren1&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/648a374f00f7a3374ee64b99/YPwSOrronoozwHbJchPn3.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2510.02209&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68df3031df49fb0df1e03c3a&quot;,&quot;name&quot;:&quot;Yanxu Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3031df49fb0df1e03c3b&quot;,&quot;name&quot;:&quot;Zijun Yao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3031df49fb0df1e03c3c&quot;,&quot;name&quot;:&quot;Yantao Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3031df49fb0df1e03c3d&quot;,&quot;name&quot;:&quot;Jin Ye&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3031df49fb0df1e03c3e&quot;,&quot;name&quot;:&quot;Jianing Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3031df49fb0df1e03c3f&quot;,&quot;name&quot;:&quot;Lei Hou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3031df49fb0df1e03c40&quot;,&quot;name&quot;:&quot;Juanzi Li&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-10-02T16:54:57.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-03T00:38:58.116Z&quot;,&quot;title&quot;:&quot;StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\\n Markets?&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Large language models (LLMs) have recently demonstrated strong capabilities\\nas autonomous agents, showing promise in reasoning, tool use, and sequential\\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\\nsuch as software engineering and scientific discovery, the finance domain\\nremains underexplored, despite its direct relevance to economic value and\\nhigh-stakes decision-making. Existing financial benchmarks primarily test\\nstatic knowledge through question answering, but they fall short of capturing\\nthe dynamic and iterative nature of trading. To address this gap, we introduce\\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\\nrealistic, multi-month stock trading environments. Agents receive daily market\\nsignals -- including prices, fundamentals, and news -- and must make sequential\\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\\nagents struggle to outperform the simple buy-and-hold baseline, several models\\ndemonstrate the potential to deliver higher returns and manage risk more\\neffectively. These findings highlight both the challenges and opportunities in\\ndeveloping LLM-powered financial agents, showing that excelling at static\\nfinancial knowledge tasks does not necessarily translate into successful\\ntrading strategies. We release StockBench as an open-source resource to support\\nreproducibility and advance future research in this domain.&quot;,&quot;upvotes&quot;:32,&quot;discussionId&quot;:&quot;68df3031df49fb0df1e03c41&quot;,&quot;ai_summary&quot;:&quot;StockBench evaluates large language models in realistic stock trading environments, revealing challenges and opportunities in developing LLM-powered financial agents.&quot;,&quot;ai_keywords&quot;:[&quot;large language models&quot;,&quot;LLM agents&quot;,&quot;reasoning&quot;,&quot;tool use&quot;,&quot;sequential decision-making&quot;,&quot;software engineering&quot;,&quot;scientific discovery&quot;,&quot;finance domain&quot;,&quot;financial benchmarks&quot;,&quot;question answering&quot;,&quot;dynamic trading&quot;,&quot;contamination-free benchmark&quot;,&quot;daily market signals&quot;,&quot;prices&quot;,&quot;fundamentals&quot;,&quot;news&quot;,&quot;sequential buy&quot;,&quot;sell&quot;,&quot;hold decisions&quot;,&quot;cumulative return&quot;,&quot;maximum drawdown&quot;,&quot;Sortino ratio&quot;,&quot;GPT-5&quot;,&quot;Claude-4&quot;,&quot;Qwen3&quot;,&quot;Kimi-K2&quot;,&quot;GLM-4.5&quot;,&quot;buy-and-hold baseline&quot;,&quot;static financial knowledge tasks&quot;,&quot;trading strategies&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;646def60df618b303b419323&quot;,&quot;avatarUrl&quot;:&quot;/avatars/97aa761d5255abf230304cfeade87835.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Lei Wang&quot;,&quot;user&quot;:&quot;demolei&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62e25e2247678ea5ce1b1786&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1bb32e7597a9b1c89c434cbf550b5382.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yantao&quot;,&quot;user&quot;:&quot;RicardoL1u&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;648c48d8c0ddeee6df5b6d22&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/648c48d8c0ddeee6df5b6d22/BlrYDv3eQxZ-Y5vtVGegX.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shangqing Tu&quot;,&quot;user&quot;:&quot;tsq2000&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64b74920fe6a108d03fed767&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a2c05b809c36fa5fab8e1a43b3e67051.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Minki Kang&quot;,&quot;user&quot;:&quot;Nardien&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6613a7d542da659656d85d28&quot;,&quot;avatarUrl&quot;:&quot;/avatars/d052f49b3ae62708c5bcdb4bc34ffc5a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Fatty&quot;,&quot;user&quot;:&quot;FattyFatty&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;652542861e9db26e407aa1fc&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4c47ef4564f498a7f34b4a17a1e209a8.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Lee Zhicheng&quot;,&quot;user&quot;:&quot;ZhiCheng0326&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64ed568ccf6118a9379a61b8&quot;,&quot;avatarUrl&quot;:&quot;/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yushi Bai&quot;,&quot;user&quot;:&quot;bys0318&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;648c4b46e549be47af1aafcd&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/648c4b46e549be47af1aafcd/YgOHbmUM2EDM-lb7GdqXz.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zijun&quot;,&quot;user&quot;:&quot;TranSirius&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66235c6544bc96a4aa5f1bcb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e3cc4503cc130ec093dd1cf471ba7b91.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;sp&quot;,&quot;user&quot;:&quot;Sparidae&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65ba0cee23ca901c2f1cae2f&quot;,&quot;avatarUrl&quot;:&quot;/avatars/9af64253c12142bfd0c6205fb19079dd.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Galling&quot;,&quot;user&quot;:&quot;GallingYu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;660bf98c3336a7e128a0e918&quot;,&quot;avatarUrl&quot;:&quot;/avatars/3e3f2886bd4a730ec19b13aecc99279f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Amy Xin&quot;,&quot;user&quot;:&quot;amyxx2001&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0}\"> Papers arxiv:2510.02209","authors":[],"published":"2025-10-05","updated":"2025-10-05T08:04:59.761Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.02209","pdf_url":"","scraped_at":"2025-10-05T08:04:59.761Z","abstract_quality":6,"analysis":{"introduction":"ðŸš€ Can LLMs actually trade stocks profitably in real markets? StockBench introduces a contamination-free benchmark to test LLM agents in realistic, multi-month trading environments. Agents get daily prices, fundamentals & news to make buy/sell/hold decisions â€” important for ML researchers & quant practitioners.","challenges":"ðŸŽ¯ Problems tackled: - Finance domain underexplored for LLM agents (most benchmarks test static QA). - Existing benchmarks fail to capture dynamic, sequential trading decisions. - Risk of data contamination/leakage in financial evals (harmed realism & fairness).","innovations":"âœ¨ What StockBench introduces: - Contamination-free benchmark design for fairness and reproducibility. - Realistic multi-month trading environments (sequential decision-making). - Daily multimodal signals: prices, fundamentals, and news. - Evaluation via financial metrics (cumulative return, max drawdown, Sortino). - Open-source release to spur follow-up work. Novelty: rigorous, realistic sequential evaluation tailored to LLM agents rather than static QA.","experiments":"ðŸ“Š Key experimental takeaway: Most evaluated LLM agents struggled to beat a simple buy-and-hold baseline, but several models (proprietary: GPT-5, Claude-4; open-weight: Qwen3, Kimi-K2, GLM-4.5) showed potential for higher returns and improved risk metrics (cumulative return, max drawdown, Sortino). Exact numeric improvements: Not specified in the paper.","insights":"ðŸ¤” Where to go next: - Research: combine StockBench with online/RL fine-tuning to teach LLM agents execution-aware trading and adaptivity. - Systems: add transaction costs, slippage, and market-impact modeling to evaluate deployability. Broader impact: could inform robo-advisors or alpha-discovery tools. Could LLM agents become practical trading partners?","keywords":["StockBench","LLM agents","financial benchmark","trading","contamination-free","cumulative return","max drawdown","Sortino ratio","GPT-5","Claude-4","Qwen3","Kimi-K2","GLM-4.5"],"category":"machine_learning","relevance_score":8,"technical_depth":"intermediate","summary":"**Introduction:** ðŸš€ Can LLMs actually trade stocks profitably in real markets? StockBench introduces a contamination-free benchmark to test LLM agents in realistic, multi-month trading environments. Agents get daily prices, fundamentals & news to make buy/sell/hold decisions â€” important for ML researchers & quant practitioners.\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Finance domain underexplored for LLM agents (most benchmarks test...","analyzed_at":"2025-10-05T08:08:14.828Z","model":"openai/gpt-5-mini"}},{"id":"hf_modernvbert__towards_smaller_visual_document_retrievers_1759651503050","title":"ModernVBERT: Towards Smaller Visual Document Retrievers","abstract":"Multimodal embedding models are gaining prevalence, notably for document retrieval as efficient alternatives to text-only pipelines. These models are typically built by finetuning large vision-language decoders (VLMs) with contrastive losses on text-image pairs. In this work, we show that, while cost-efficient, this repurposing approach often bottlenecks retrieval performance. Through controlled experiments, we establish a principled recipe for improving visual document retrieval models. We notably measure the impact of attention masking, image resolution, modality alignment data regimes, and late interaction centered contrastive objectives which emerge as central performance factors. Building on these insights, we release ModernVBERT, a compact 250M-parameter vision-language encoder that outperforms models up to 10 times larger when finetuned on document retrieval tasks. Models and code are made available.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T11:20:48.587Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;60f2e021adf471cbdf8bb660&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg&quot;,&quot;fullname&quot;:&quot;Manuel Faysse&quot;,&quot;name&quot;:&quot;manu&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:181}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.8902249336242676},&quot;editors&quot;:[&quot;manu&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ðŸ”¥&quot;,&quot;users&quot;:[&quot;fsommers&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68e07a7297c0804cbe6bf182&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267},&quot;createdAt&quot;:&quot;2025-10-04T01:37:54.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;This is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper. \\n\\nThe following papers were recommended by the Semantic Scholar API \\n\\n* [MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction](https://huggingface.co/papers/2509.18095) (2025)\\n* [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://huggingface.co/papers/2509.19203) (2025)\\n* [Training LLMs to be Better Text Embedders through Bidirectional Reconstruction](https://huggingface.co/papers/2509.03020) (2025)\\n* [CMRAG: Co-modality-based visual document retrieval and question answering](https://huggingface.co/papers/2509.02123) (2025)\\n* [SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models](https://huggingface.co/papers/2509.15432) (2025)\\n* [Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation](https://huggingface.co/papers/2508.17079) (2025)\\n* [DocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning](https://huggingface.co/papers/2509.23883) (2025)\\n\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n\\n If you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\\n\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`&quot;,&quot;html&quot;:&quot;This is an automated message from the Librarian Bot. I found the following papers similar to this paper. \\nThe following papers were recommended by the Semantic Scholar API \\n\\nMetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction (2025)\\nVision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions (2025)\\nTraining LLMs to be Better Text Embedders through Bidirectional Reconstruction (2025)\\nCMRAG: Co-modality-based visual document retrieval and question answering (2025)\\nSERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models (2025)\\nZero-shot Multimodal Document Retrieval via Cross-modal Question Generation (2025)\\nDocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning (2025)\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n If you want recommendations for any Paper on Hugging Face checkout this Space\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: \\n\\n@librarian-bot\\n\\t recommend\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T01:37:54.955Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.6963590383529663},&quot;editors&quot;:[&quot;librarian-bot&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2510.01149&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68de63af70ada21878c75026&quot;,&quot;name&quot;:&quot;Paul Teiletche&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68de63af70ada21878c75027&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;661e945eebe3616a1b09e279&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/661e945eebe3616a1b09e279/U3DL1BNouUpcusCKAPZm0.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Quentin MacÃ©&quot;,&quot;user&quot;:&quot;QuentinJG&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Quentin MacÃ©&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:52:04.668Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68de63af70ada21878c75028&quot;,&quot;name&quot;:&quot;Max Conti&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68de63af70ada21878c75029&quot;,&quot;name&quot;:&quot;Antonio Loison&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68de63af70ada21878c7502a&quot;,&quot;name&quot;:&quot;Gautier Viaud&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68de63af70ada21878c7502b&quot;,&quot;name&quot;:&quot;Pierre Colombo&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68de63af70ada21878c7502c&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;60f2e021adf471cbdf8bb660&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Manuel Faysse&quot;,&quot;user&quot;:&quot;manu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Manuel Faysse&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:52:47.596Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-10-01T17:41:17.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-03T09:50:48.566Z&quot;,&quot;title&quot;:&quot;ModernVBERT: Towards Smaller Visual Document Retrievers&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60f2e021adf471cbdf8bb660&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Manuel Faysse&quot;,&quot;user&quot;:&quot;manu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Multimodal embedding models are gaining prevalence, notably for document\\nretrieval as efficient alternatives to text-only pipelines. These models are\\ntypically built by finetuning large vision-language decoders (VLMs) with\\ncontrastive losses on text-image pairs. In this work, we show that, while\\ncost-efficient, this repurposing approach often bottlenecks retrieval\\nperformance. Through controlled experiments, we establish a principled recipe\\nfor improving visual document retrieval models. We notably measure the impact\\nof attention masking, image resolution, modality alignment data regimes, and\\nlate interaction centered contrastive objectives which emerge as central\\nperformance factors. Building on these insights, we release ModernVBERT, a\\ncompact 250M-parameter vision-language encoder that outperforms models up to 10\\ntimes larger when finetuned on document retrieval tasks. Models and code are\\nmade available at https://huggingface.co/ModernVBERT.&quot;,&quot;upvotes&quot;:26,&quot;discussionId&quot;:&quot;68de63b070ada21878c7502d&quot;,&quot;projectPage&quot;:&quot;https://huggingface.co/ModernVBERT&quot;,&quot;ai_summary&quot;:&quot;ModernVBERT, a compact vision-language encoder, outperforms larger models in document retrieval by optimizing attention masking, image resolution, modality alignment, and contrastive objectives.&quot;,&quot;ai_keywords&quot;:[&quot;multimodal embedding models&quot;,&quot;document retrieval&quot;,&quot;vision-language decoders&quot;,&quot;contrastive losses&quot;,&quot;attention masking&quot;,&quot;image resolution&quot;,&quot;modality alignment&quot;,&quot;contrastive objectives&quot;,&quot;vision-language encoder&quot;],&quot;organization&quot;:{&quot;_id&quot;:&quot;68dc126476aff34f469efbc4&quot;,&quot;name&quot;:&quot;ModernVBERT&quot;,&quot;fullname&quot;:&quot;ModernVBERT&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/6651baf4b34bbdaec88333e7/qxlWj1d9iagGW6T8yCkJV.png&quot;}},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;60f2e021adf471cbdf8bb660&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Manuel Faysse&quot;,&quot;user&quot;:&quot;manu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;661e945eebe3616a1b09e279&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/661e945eebe3616a1b09e279/U3DL1BNouUpcusCKAPZm0.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Quentin MacÃ©&quot;,&quot;user&quot;:&quot;QuentinJG&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6720a87e392e9cea0187fde6&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6720a87e392e9cea0187fde6/vW8DW31UvdKD809UyYCS4.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Max Conti&quot;,&quot;user&quot;:&quot;mlconti&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;618a65e17304dc918c6602ff&quot;,&quot;avatarUrl&quot;:&quot;/avatars/8af1112094169da80c65e24ab71c7e59.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Gautier Viaud&quot;,&quot;user&quot;:&quot;gautierviaud&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64be3b2b805e5b64572eec44&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VDCwEAgQ2G0fGCvB5L8cA.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Alexander Micklewright&quot;,&quot;user&quot;:&quot;alexmick&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66e16a677c2eb2da5109fb5c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66e16a677c2eb2da5109fb5c/wN2ZQ4iRQ5zmdC0WTQsnD.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Antoine EDY&quot;,&quot;user&quot;:&quot;antoineedy-illuin&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67a8dc9e560939c755f39fb5&quot;,&quot;avatarUrl&quot;:&quot;/avatars/9451f17dcd2894ede2c95dcfdfa43582.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Victor Xing&quot;,&quot;user&quot;:&quot;vxing&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67ab5868b028527841ce09f7&quot;,&quot;avatarUrl&quot;:&quot;/avatars/5e8b7a341a807bd3e08c2cb98ca20dd3.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Iker TARDIO&quot;,&quot;user&quot;:&quot;tardioik&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b708f9ae6ee066e29045c3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/824a8973e1013a615538df58ef9dfa17.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Marine Neyret&quot;,&quot;user&quot;:&quot;mneyret&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64a2e1ab812528832070a4b4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c2fef6d21cfd4b66085127e7f3223025.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Victor Alibert&quot;,&quot;user&quot;:&quot;victoralibert&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63b6cb6b029518c6bbfda056&quot;,&quot;avatarUrl&quot;:&quot;/avatars/28849f397a60eb33c359cf536f109485.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tom BrendlÃ©&quot;,&quot;user&quot;:&quot;tombrendle&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62b5db0a73ab76290041245a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/6c94a42e8f647578e7d31fcdfcbd591e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Paul des Garets&quot;,&quot;user&quot;:&quot;pdesgarets-illuin&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;organization&quot;:{&quot;_id&quot;:&quot;68dc126476aff34f469efbc4&quot;,&quot;name&quot;:&quot;ModernVBERT&quot;,&quot;fullname&quot;:&quot;ModernVBERT&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/6651baf4b34bbdaec88333e7/qxlWj1d9iagGW6T8yCkJV.png&quot;}}\"> Papers arxiv:2510.01149","authors":[],"published":"2025-10-05","updated":"2025-10-05T08:05:03.050Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.01149","pdf_url":"","scraped_at":"2025-10-05T08:05:03.050Z","abstract_quality":6,"analysis":{"introduction":"ðŸš€ Want faster, cheaper visual document search without huge models? ModernVBERT presents a compact 250M-parameter vision-language encoder that outperforms models up to 10Ã— larger for document retrieval â€” a win for efficient, real-world retrieval pipelines.","challenges":"ðŸŽ¯ Key problems tackled: - Repurposing large VLM decoders with contrastive finetuning often bottlenecks retrieval performance. - High compute/cost of large multimodal models for retrieval. - Lack of principled study on factors (attention masking, image resolution, alignment) that drive retrieval quality.","innovations":"âœ¨ Core contributions: - Controlled ablations measuring attention masking, image resolution, modality-alignment data regimes. - Emphasis on late-interactionâ€“centered contrastive objectives tailored for visual document retrieval. - A principled \"recipe\" to optimize those factors. - Release of ModernVBERT: a compact 250M-parameter vision-language encoder that leverages these insights.","experiments":"ðŸ“Š Main experimental takeaway: ModernVBERT (250M params) outperforms models up to 10Ã— larger when finetuned on document retrieval tasks â€” demonstrating compact, targeted encoders can beat bloated VLM repurposing.  Detailed benchmarks/metrics/datasets: Not specified in the paper.","insights":"ðŸ¤” What's next (inspired ideas): - Explore combining ModernVBERT with multi-vector/patch-level indexing or late-interaction re-ranking. - Test zero-shot/transfer performance across more document domains and industry-scale corpora. Could compact, optimized encoders replace heavy VLMs in production search stacks?","keywords":["ModernVBERT","visual document retrieval","vision-language encoder","contrastive learning","late interaction","attention masking","image resolution","modality alignment"],"category":"machine_learning","relevance_score":9,"technical_depth":"intermediate","summary":"**Introduction:** ðŸš€ Want faster, cheaper visual document search without huge models? ModernVBERT presents a compact 250M-parameter vision-language encoder that outperforms models up to 10Ã— larger for document retrieval â€” a win for efficient, real-world retrieval pipelines.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Repurposing large VLM decoders with contrastive finetuning often bottlenecks retrieval performance. - High compute/co...","analyzed_at":"2025-10-05T08:08:10.670Z","model":"openai/gpt-5-mini"}},{"id":"hf_rlp__reinforcement_as_a_pretraining_objective_1759651505970","title":"RLP: Reinforcement as a Pretraining Objective","abstract":"Letâ€™s discuss: Why Reinforcement Pretraining (RLP) for reasoning\\nMotivation.Todayâ€™s LLMs learn almost everything with nextâ€‘token prediction, then only after pretraining try to teach themselves to reason via SFT and RLHF/RLVR. That split means the base model never practices â€œthinking before predictingâ€ while itâ€™s learning from raw text. We asked: what if exploration happened during pretraining itself, and we rewarded thoughts that genuinely help predict the next tokenâ€”without any taskâ€‘specific verifiers?\\nWhat we built.RLP is a pretraining objective that treats a short chainâ€‘ofâ€‘thought (CoT) as an action taken before predicting each token.\\n\\nFor each position, the model samples a brief thought, then scores the true next token twice:\\n\\nwith the thought, and 2) with a â€œnoâ€‘thinkâ€ baseline (a slowly updated EMA teacher).\\n\\n\\nThe reward is the increase in the modelâ€™s logâ€‘likelihood of the observed next token with the thought compared to without it.\\n\\nWe update only the thought tokens using a clipped, groupâ€‘relative advantage objective; we do not backprop through the reward scores.\\n\\nThe signal is dense (every position gets a reward), verifierâ€‘free (works on ordinary text), and scales to full documents (no entropy filtering or curated checkers).\\n\\n\\nWhy earlier approaches (e.g., RPT) fell short.Prior â€œreinforcement pretrainingâ€ with prefixâ€‘matching rewards (RPT) usually relies on:\\n\\nSparse, binary rewards tied to nextâ€‘token correctness that ignore the content of the thought,\\nAuxiliary entropy filters to pick a subset of tokens to train on, and\\nExperiments on distilled checkpoints, leaving open whether it helps base models.RLP instead delivers a continuous, perâ€‘token improvement signal, trains on all tokens in full documents, needs no side model or heuristics, and is explicitly designed to shape baseâ€‘model thinking early.\\n\\nKey results.\\n\\nQwen3â€‘1.7B, base pretraining:\\n\\nOverall average across our mathâ€‘andâ€‘science suite improves by about 19% vs. the base model (36.03 vs. 30.32) and about 17% vs. continuous pretraining on the same tokens (36.03 vs. 30.85).\\nUnder compute matching, we gave the CPT baseline 35Ã— more tokens to equalize FLOPs; RLP still leads on the same setup by +5.32 points overall on the NC corpus (43.36 vs. 38.04).\\nVersus RPT with matched data/compute, RLP improves Overall Avg by +1.66 points (about +4% relative) and also leads on Math and Science aggregates.\\n\\n\\nAfter identical postâ€‘training (SFT + RLVR) for all models:\\n\\nGains compound: RLP+Post 42.51 vs. Base+Post 39.34 (about +8% relative) and vs. CPT+Post 39.90 (about +6.5% relative).\\nThe biggest lifts are on reasoningâ€‘heavy tasks such as AIME25 and MMLUâ€‘Pro.\\n\\n\\nScaling to a 12B hybrid (NeMoâ€‘12B):\\n\\nApplying only 250M RLP tokens to an intermediate checkpoint boosts the overall average from 42.81 to 61.32 (+18.51 points; about +43% relative).\\nScience Avg rises from 34.51 to 57.26 (about +23 points), showing strong crossâ€‘domain transfer, not just mathâ€‘specific gains.\\n\\n\\nDomain breadth and data practicality:\\n\\nRLP works on SFTâ€‘style reasoning corpora and general pretraining sources (academic papers, textbooks, web QA).\\nImprovements persist even when the baseline sees vastly more tokens to match FLOPs, indicating the benefits come from the objective rather than from a larger data budget.\\n\\n\\n\\nIn short, RLP rewards â€œuseful thoughtsâ€ during pretraining. The signal is simple, dense, verifierâ€‘free, and compatible with standard pipelinesâ€”yielding stronger base models whose gains survive and compound after alignment.\\nPaper: https://arxiv.org/pdf/2510.01265Code: https://github.com/NVlabs/RLP\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T02:25:02.436Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;64414b62603214724ebd2636&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg&quot;,&quot;fullname&quot;:&quot;Ali&quot;,&quot;name&quot;:&quot;ahatamiz&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:4}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.8267005681991577},&quot;editors&quot;:[&quot;ahatamiz&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;â¤ï¸&quot;,&quot;users&quot;:[&quot;SieraL&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68e07972462eb02f83342642&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267},&quot;createdAt&quot;:&quot;2025-10-04T01:33:38.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;This is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper. \\n\\nThe following papers were recommended by the Semantic Scholar API \\n\\n* [Reinforcement Mid-Training](https://huggingface.co/papers/2509.24375) (2025)\\n* [Reinforcement Learning on Pre-Training Data](https://huggingface.co/papers/2509.19249) (2025)\\n* [VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models](https://huggingface.co/papers/2509.19803) (2025)\\n* [Learning to Reason as Action Abstractions with Scalable Mid-Training RL](https://huggingface.co/papers/2509.25810) (2025)\\n* [Proximal Supervised Fine-Tuning](https://huggingface.co/papers/2508.17784) (2025)\\n* [One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient](https://huggingface.co/papers/2509.26313) (2025)\\n* [On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification](https://huggingface.co/papers/2508.05629) (2025)\\n\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n\\n If you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\\n\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`&quot;,&quot;html&quot;:&quot;This is an automated message from the Librarian Bot. I found the following papers similar to this paper. \\nThe following papers were recommended by the Semantic Scholar API \\n\\nReinforcement Mid-Training (2025)\\nReinforcement Learning on Pre-Training Data (2025)\\nVCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models (2025)\\nLearning to Reason as Action Abstractions with Scalable Mid-Training RL (2025)\\nProximal Supervised Fine-Tuning (2025)\\nOne-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient (2025)\\nOn the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification (2025)\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n If you want recommendations for any Paper on Hugging Face checkout this Space\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: \\n\\n@librarian-bot\\n\\t recommend\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T01:33:38.340Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7649551630020142},&quot;editors&quot;:[&quot;librarian-bot&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2510.01265&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03ba8&quot;,&quot;name&quot;:&quot;Ali Hatamizadeh&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03ba9&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6338dd1776421c0543150467&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4539dcec644e40be33f4a0d419fa66cb.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Syeda Nahida Akter&quot;,&quot;user&quot;:&quot;SieraL&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Syeda Nahida Akter&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:33:13.536Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03baa&quot;,&quot;name&quot;:&quot;Shrimai Prabhumoye&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03bab&quot;,&quot;name&quot;:&quot;Jan Kautz&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03bac&quot;,&quot;name&quot;:&quot;Mostofa Patwary&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03bad&quot;,&quot;name&quot;:&quot;Mohammad Shoeybi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03bae&quot;,&quot;name&quot;:&quot;Bryan Catanzaro&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03baf&quot;,&quot;name&quot;:&quot;Yejin Choi&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-26T17:53:54.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-03T00:55:02.431Z&quot;,&quot;title&quot;:&quot;RLP: Reinforcement as a Pretraining Objective&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64414b62603214724ebd2636&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ali&quot;,&quot;user&quot;:&quot;ahatamiz&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;The dominant paradigm for training large reasoning models starts with\\npre-training using next-token prediction loss on vast amounts of data.\\nReinforcement learning, while powerful in scaling reasoning, is introduced only\\nas the very last phase of post-training, preceded by supervised fine-tuning.\\nWhile dominant, is this an optimal way of training? In this paper, we present\\nRLP, an information-driven reinforcement pretraining objective, that brings the\\ncore spirit of reinforcement learning -- exploration -- to the last phase of\\npretraining. The key idea is to treat chain-of-thought as an exploratory\\naction, with rewards computed based on the information gain it provides for\\npredicting future tokens. This training objective essentially encourages the\\nmodel to think for itself before predicting what comes next, thus teaching an\\nindependent thinking behavior earlier in the pretraining. More concretely, the\\nreward signal measures the increase in log-likelihood of the next token when\\nconditioning on both context and a sampled reasoning chain, compared to\\nconditioning on context alone. This approach yields a verifier-free dense\\nreward signal, allowing for efficient training for the full document stream\\nduring pretraining. Specifically, RLP reframes reinforcement learning for\\nreasoning as a pretraining objective on ordinary text, bridging the gap between\\nnext-token prediction and the emergence of useful chain-of-thought reasoning.\\nPretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an\\neight-benchmark math-and-science suite by 19%. With identical post-training,\\nthe gains compound, with the largest improvements on reasoning-heavy tasks such\\nas AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2\\nincreases the overall average from 42.81% to 61.32% and raises the average on\\nscientific reasoning by 23%, demonstrating scalability across architectures and\\nmodel sizes.&quot;,&quot;upvotes&quot;:26,&quot;discussionId&quot;:&quot;68df26d8df49fb0df1e03bb0&quot;,&quot;projectPage&quot;:&quot;https://t.co/6PNJYfiAoJ&quot;,&quot;githubRepo&quot;:&quot;https://github.com/NVlabs/RLP&quot;,&quot;ai_summary&quot;:&quot;RLP, an information-driven reinforcement pretraining objective, enhances reasoning models by integrating exploration into pretraining, leading to significant performance improvements across various benchmarks.&quot;,&quot;ai_keywords&quot;:[&quot;reinforcement learning&quot;,&quot;pre-training&quot;,&quot;next-token prediction&quot;,&quot;chain-of-thought&quot;,&quot;information gain&quot;,&quot;log-likelihood&quot;,&quot;dense reward signal&quot;,&quot;Qwen3-1.7B-Base&quot;,&quot;AIME25&quot;,&quot;MMLU-Pro&quot;,&quot;Nemotron-Nano-12B-v2&quot;,&quot;scientific reasoning&quot;],&quot;githubStars&quot;:81,&quot;organization&quot;:{&quot;_id&quot;:&quot;60262b67268c201cdc8b7d43&quot;,&quot;name&quot;:&quot;nvidia&quot;,&quot;fullname&quot;:&quot;NVIDIA&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png&quot;}},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;64414b62603214724ebd2636&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ali&quot;,&quot;user&quot;:&quot;ahatamiz&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6338dd1776421c0543150467&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4539dcec644e40be33f4a0d419fa66cb.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Syeda Nahida Akter&quot;,&quot;user&quot;:&quot;SieraL&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;5df85abada6d0311fd3d5408&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Li Dong&quot;,&quot;user&quot;:&quot;unilm&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62ea79dd01ed9b0e8f61ccd3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chengsong Huang&quot;,&quot;user&quot;:&quot;ChengsongHuang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;684d57f26e04c265777ead3f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/cuOj-bQqukSZreXgUJlfm.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Joakim Lee&quot;,&quot;user&quot;:&quot;Reinforcement4All&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66980b9c9baa4382e1678809&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1a516bb7aa7871834c19de708cdd853a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shrimai Prabhumoye&quot;,&quot;user&quot;:&quot;shrimai19&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;681b86baef3886afeea020ea&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fqxZlJxIh803nWvndGOfW.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Anwar&quot;,&quot;user&quot;:&quot;abdoali5672&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64d98ef7a4839890b25eb78b&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Fangyuan Yu&quot;,&quot;user&quot;:&quot;Ksgk-fy&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65bb837dbfb878f46c77de4c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/65bb837dbfb878f46c77de4c/PKyQ_-wTNH1Hyv5HxhWdX.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Prithiv Sakthi&quot;,&quot;user&quot;:&quot;prithivMLmods&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;620783f24e28382272337ba4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/620783f24e28382272337ba4/zkUveQPNiDfYjgGhuFErj.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;GuoLiangTang&quot;,&quot;user&quot;:&quot;Tommy930&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6612aedf09f16e7347dfa7e1&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6612aedf09f16e7347dfa7e1/bPYjBXCedY_1fSIPjoBTY.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Nishith Jain&quot;,&quot;user&quot;:&quot;KingNish&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;organization&quot;:{&quot;_id&quot;:&quot;60262b67268c201cdc8b7d43&quot;,&quot;name&quot;:&quot;nvidia&quot;,&quot;fullname&quot;:&quot;NVIDIA&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png&quot;}}\"> Papers arxiv:2510.01265","authors":[],"published":"2025-10-05","updated":"2025-10-05T08:05:05.970Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.01265","pdf_url":"","scraped_at":"2025-10-05T08:05:05.970Z","abstract_quality":6,"analysis":{"introduction":"ðŸš€ What if LLMs learned to â€œthink before predictingâ€ during pretraining? RLP (Reinforcement as a Pretraining Objective) treats short chain-of-thoughts as actions and rewards thoughts that improve next-token likelihood â€” bringing exploration into core pretraining for stronger reasoning base models.","challenges":"ðŸŽ¯ Key problems addressed: - Base models only learn next-token prediction; reasoning is deferred to post-training (SFT/RL), so they never practice internal thinking early. - Prior RL-pretraining (RPT) used sparse binary rewards and heavy heuristics. - Many approaches rely on distilled checkpoints or curated verifiers.","innovations":"âœ¨ Core innovations: - Treat a brief chain-of-thought (CoT) sampled at each token position as an action. - Reward = increase in log-likelihood of the true next token vs a â€œno-thinkâ€ EMA teacher (information gain). - Update only the thought tokens with a clipped, group-relative advantage objective; do not backprop through rewards. - Dense, verifier-free per-token signal that scales to full documents.","experiments":"ðŸ“Š Most compelling result: Applying RLP to a 12B hybrid (NeMo-12B) for 250M RLP tokens raised overall avg from 42.81 to 61.32 (+18.51 points, ~+43% relative), showing large gains in reasoning performance and cross-domain transfer (Science avg 34.51 â†’ 57.26).","insights":"ðŸ¤” What's next (potential directions & apps): - Investigate RLP over longer pretraining schedules or with multimodal inputs to seed cross-modal reasoning. - Study how RLP affects calibration, hallucination, and reliability of CoT outputs. Potential apps: stronger base models for scientific QA, math/problem solving, and complex decision-support. Could RLP reduce reliance on heavy post-training RL?","keywords":["reinforcement learning","pretraining","next-token prediction","chain-of-thought","information gain","log-likelihood","dense reward","Qwen3-1.7B","NeMo-12B","reasoning"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ What if LLMs learned to â€œthink before predictingâ€ during pretraining? RLP (Reinforcement as a Pretraining Objective) treats short chain-of-thoughts as actions and rewards thoughts that improve next-token likelihood â€” bringing exploration into core pretraining for stronger reasoning base models.\n\n**Challenges:** ðŸŽ¯ Key problems addressed: - Base models only learn next-token prediction; reasoning is deferred to po...","analyzed_at":"2025-10-05T08:08:12.361Z","model":"openai/gpt-5-mini"}},{"id":"hf_clue__non_parametric_verification_from_experience_via_hidden_state_clustering_1759651508961","title":"CLUE: Non-parametric Verification from Experience via Hidden-State Clustering","abstract":"This is an automated message from the Librarian Bot. I found the following papers similar to this paper. \\nThe following papers were recommended by the Semantic Scholar API \\n\\nDeep Think with Confidence (2025)\\nThe LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations (2025)\\nLatent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning (2025)\\nCan LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models (2025)\\nLearning to Refine: Self-Refinement of Parallel Reasoning in LLMs (2025)\\nEvolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation (2025)\\nBridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions (2025)\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n If you want recommendations for any Paper on Hugging Face checkout this Space\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: \\n\\n@librarian-bot\\n\\t recommend\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T01:34:29.173Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7361921668052673},&quot;editors&quot;:[&quot;librarian-bot&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2510.01591&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bb7&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;62ffa3f8311cad266f9af236&quot;,&quot;avatarUrl&quot;:&quot;/avatars/203dac40bc546ee25a01d8715a4b3049.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhenwen Liang&quot;,&quot;user&quot;:&quot;invokerliang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhenwen Liang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:33:10.962Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bb8&quot;,&quot;name&quot;:&quot;Ruosen Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bb9&quot;,&quot;name&quot;:&quot;Yujun Zhou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bba&quot;,&quot;name&quot;:&quot;Linfeng Song&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bbb&quot;,&quot;name&quot;:&quot;Dian Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bbc&quot;,&quot;name&quot;:&quot;Xinya Du&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bbd&quot;,&quot;name&quot;:&quot;Haitao Mi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bbe&quot;,&quot;name&quot;:&quot;Dong Yu&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-10-02T02:14:33.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-03T00:04:05.892Z&quot;,&quot;title&quot;:&quot;CLUE: Non-parametric Verification from Experience via Hidden-State\\n Clustering&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;62ffa3f8311cad266f9af236&quot;,&quot;avatarUrl&quot;:&quot;/avatars/203dac40bc546ee25a01d8715a4b3049.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhenwen Liang&quot;,&quot;user&quot;:&quot;invokerliang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Assessing the quality of Large Language Model (LLM) outputs presents a\\ncritical challenge. Previous methods either rely on text-level information\\n(e.g., reward models, majority voting), which can overfit to superficial cues,\\nor on calibrated confidence from token probabilities, which would fail on\\nless-calibrated models. Yet both of these signals are, in fact, partial\\nprojections of a richer source of information: the model's internal hidden\\nstates. Early layers, closer to token embeddings, preserve semantic and lexical\\nfeatures that underpin text-based judgments, while later layers increasingly\\nalign with output logits, embedding confidence-related information. This paper\\nexplores hidden states directly as a unified foundation for verification. We\\nshow that the correctness of a solution is encoded as a geometrically separable\\nsignature within the trajectory of hidden activations. To validate this, we\\npresent Clue (Clustering and Experience-based Verification), a deliberately\\nminimalist, non-parametric verifier. With no trainable parameters, CLUE only\\nsummarizes each reasoning trace by an hidden state delta and classifies\\ncorrectness via nearest-centroid distance to ``success'' and ``failure''\\nclusters formed from past experience. The simplicity of this method highlights\\nthe strength of the underlying signal. Empirically, CLUE consistently\\noutperforms LLM-as-a-judge baselines and matches or exceeds modern\\nconfidence-based methods in reranking candidates, improving both top-1 and\\nmajority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24\\nwith a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%\\n(top-maj@16).&quot;,&quot;upvotes&quot;:20,&quot;discussionId&quot;:&quot;68df27e9df49fb0df1e03bbf&quot;,&quot;ai_summary&quot;:&quot;Hidden states in Large Language Models encode correctness as a separable signature, enabling a minimalist verifier (CLUE) to outperform text-level and confidence-based methods in reranking and accuracy.&quot;,&quot;ai_keywords&quot;:[&quot;Large Language Model&quot;,&quot;hidden states&quot;,&quot;token embeddings&quot;,&quot;semantic features&quot;,&quot;lexical features&quot;,&quot;output logits&quot;,&quot;confidence-related information&quot;,&quot;Clue&quot;,&quot;Clustering and Experience-based Verification&quot;,&quot;nearest-centroid distance&quot;,&quot;AIME&quot;,&quot;GPQA&quot;],&quot;organization&quot;:{&quot;_id&quot;:&quot;66543b6e420092799d2f625c&quot;,&quot;name&quot;:&quot;tencent&quot;,&quot;fullname&quot;:&quot;Tencent&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png&quot;}},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;62ffa3f8311cad266f9af236&quot;,&quot;avatarUrl&quot;:&quot;/avatars/203dac40bc546ee25a01d8715a4b3049.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhenwen Liang&quot;,&quot;user&quot;:&quot;invokerliang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65a05abf07184d32fa002d41&quot;,&quot;avatarUrl&quot;:&quot;/avatars/3a23e7e568d2024381ed31b56c1c461a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yujun Zhou&quot;,&quot;user&quot;:&quot;yujunzhou&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65037565da2d88e201f63b7a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/d1b6ce17236360e9583b8bb4cb87e506.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Runpeng Dai&quot;,&quot;user&quot;:&quot;Leo-Dai&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6850ab3fb73a72cdc6159f6f&quot;,&quot;avatarUrl&quot;:&quot;/avatars/74c58336655cdfe7d3fb9854d426240d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Haolin Liu&quot;,&quot;user&quot;:&quot;lhl616&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66e4aa8d4926518abbf5cae2&quot;,&quot;avatarUrl&quot;:&quot;/avatars/dcff2521e0292b602f86c76fc4b5bbae.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;XiangqiWang&quot;,&quot;user&quot;:&quot;qisein&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62ea79dd01ed9b0e8f61ccd3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chengsong Huang&quot;,&quot;user&quot;:&quot;ChengsongHuang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6623ea65b642e29cdf90a1b4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e32e90574c1162b2be87ed78604e3e4d.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;TongZheng&quot;,&quot;user&quot;:&quot;TongZheng1999&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64770d4f5ef58684691e4e57&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7afe56ead9b1233b027e15d0e974f6af.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;KehanGuo&quot;,&quot;user&quot;:&quot;kguo2&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;642447e873f7a0d40b30d677&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/642447e873f7a0d40b30d677/hbUnfIhKZCeSSPv9wGAzk.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;LZX&quot;,&quot;user&quot;:&quot;zli12321&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65147a1426fbd558dbd08f1b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/86574ee2d5c22e940be1c4e50be88675.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Haitao Mi&quot;,&quot;user&quot;:&quot;haitaominlp&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62d58fd53bf5e059f7cc3245&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7a4f3ee4a37245f67efd26749d66a706.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Dian Yu&quot;,&quot;user&quot;:&quot;yudian&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;686c18db78fcc7beecc6634c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/686c18db78fcc7beecc6634c/yWGyDby4xhMboII07I9nX.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tencent-IMO&quot;,&quot;user&quot;:&quot;Tencent-IMO&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;organization&quot;:{&quot;_id&quot;:&quot;66543b6e420092799d2f625c&quot;,&quot;name&quot;:&quot;tencent&quot;,&quot;fullname&quot;:&quot;Tencent&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png&quot;}}\"> Papers arxiv:2510.01591","authors":[],"published":"2025-10-05","updated":"2025-10-05T08:05:08.961Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.01591","pdf_url":"","scraped_at":"2025-10-05T08:05:08.961Z","abstract_quality":6,"analysis":{"introduction":"ðŸš€ Question: Can a model's hidden activations tell us if its answer is right?   CLUE answers yes â€” a tiny, non-parametric verifier that reads hidden-state trajectories and checks correctness by clustering past â€œsuccessâ€ vs â€œfailureâ€ traces.   Why care? Better verification & reranking for LLM outputs.","challenges":"ðŸŽ¯ Key problems tackled: - Existing text-level methods (reward models, majority voting) can overfit superficial cues. - Token-prob confidence is unreliable for less-calibrated models. - No simple unified signal that captures both semantic and confidence info.","innovations":"âœ¨ Core ideas / what's new: - CLUE: a non-parametric verifier using hidden-state \"deltas\" from reasoning traces. - Classifies correctness by nearest-centroid distance to success/failure clusters built from past experience. - No trainable parameters â€” novelty: hidden trajectories form a geometrically separable correctness signature.","experiments":"ðŸ“Š Most compelling result: - On AIME 24 with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0% (top-maj@16). - Empirically outperforms LLM-as-judge baselines and matches/exceeds confidence-based methods on AIME 24/25 and GPQA.","insights":"ðŸ¤” What's next (potential directions & impact): - Explore adapting CLUE across models/tasks or multimodal traces; test continual \"experience banks\" that grow over time. - Combine non-parametric hidden-state signals with lightweight learned verifiers for robustness. Could this make LLM assistants measurably more reliable?","keywords":["CLUE","hidden states","non-parametric verifier","nearest-centroid","hidden-state delta","LLM verification","AIME","GPQA"],"category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Question: Can a model's hidden activations tell us if its answer is right?   CLUE answers yes â€” a tiny, non-parametric verifier that reads hidden-state trajectories and checks correctness by clustering past â€œsuccessâ€ vs â€œfailureâ€ traces.   Why care? Better verification & reranking for LLM outputs.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Existing text-level methods (reward models, majority voting) can overfit...","analyzed_at":"2025-10-05T08:08:43.158Z","model":"openai/gpt-5-mini"}},{"id":"hf_the_rogue_scalpel__activation_steering_compromises_llm_safety_1759651511991","title":"The Rogue Scalpel: Activation Steering Compromises LLM Safety","abstract":"Activation steering - a technique that controls AI models by adding vectors to their internal representations - actually makes models less safe, not more. Even random steering directions can increase harmful compliance from 0% to 27%, and using \\&quot;interpretable\\&quot; directions from sparse autoencoders makes it worse. We show that having precise control over a model's internals doesn't guarantee safe behavior, challenging the idea that interpretability equals safety.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T10:56:05.552Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;60cd95ee15ecba5f2200304a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg&quot;,&quot;fullname&quot;:&quot;Alexey Dontsov&quot;,&quot;name&quot;:&quot;therem&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:5}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9265779256820679},&quot;editors&quot;:[&quot;therem&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ðŸ‘&quot;,&quot;users&quot;:[&quot;igardner&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68e07a21198a4f889ff3b347&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267},&quot;createdAt&quot;:&quot;2025-10-04T01:36:33.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;This is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper. \\n\\nThe following papers were recommended by the Semantic Scholar API \\n\\n* [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://huggingface.co/papers/2508.20766) (2025)\\n* [LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation](https://huggingface.co/papers/2509.19839) (2025)\\n* [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://huggingface.co/papers/2508.12535) (2025)\\n* [Steering MoE LLMs via Expert (De)Activation](https://huggingface.co/papers/2509.09660) (2025)\\n* [Mitigating Jailbreaks with Intent-Aware LLMs](https://huggingface.co/papers/2508.12072) (2025)\\n* [Safety Alignment Should Be Made More Than Just A Few Attention Heads](https://huggingface.co/papers/2508.19697) (2025)\\n* [ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack](https://huggingface.co/papers/2509.25843) (2025)\\n\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n\\n If you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\\n\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`&quot;,&quot;html&quot;:&quot;This is an automated message from the Librarian Bot. I found the following papers similar to this paper. \\nThe following papers were recommended by the Semantic Scholar API \\n\\nTurning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection (2025)\\nLatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation (2025)\\nCorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection (2025)\\nSteering MoE LLMs via Expert (De)Activation (2025)\\nMitigating Jailbreaks with Intent-Aware LLMs (2025)\\nSafety Alignment Should Be Made More Than Just A Few Attention Heads (2025)\\nASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack (2025)\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n If you want recommendations for any Paper on Hugging Face checkout this Space\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: \\n\\n@librarian-bot\\n\\t recommend\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T01:36:33.680Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7323068976402283},&quot;editors&quot;:[&quot;librarian-bot&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.22067&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dec5ae70ada21878c75169&quot;,&quot;name&quot;:&quot;Anton Korznikov&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dec5ae70ada21878c7516a&quot;,&quot;name&quot;:&quot;Andrey Galichin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dec5ae70ada21878c7516b&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;60cd95ee15ecba5f2200304a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Alexey Dontsov&quot;,&quot;user&quot;:&quot;therem&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Alexey Dontsov&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:33:28.616Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dec5ae70ada21878c7516c&quot;,&quot;name&quot;:&quot;Oleg Y. Rogov&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dec5ae70ada21878c7516d&quot;,&quot;name&quot;:&quot;Ivan Oseledets&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dec5ae70ada21878c7516e&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;662f8d645c4db70c77a203b0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/72f9a3c39b3ba5114388d16a35524835.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Elena Tutubalina&quot;,&quot;user&quot;:&quot;tlenusik&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Elena Tutubalina&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:33:26.134Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-26T08:49:47.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-03T09:26:05.516Z&quot;,&quot;title&quot;:&quot;The Rogue Scalpel: Activation Steering Compromises LLM Safety&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60cd95ee15ecba5f2200304a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Alexey Dontsov&quot;,&quot;user&quot;:&quot;therem&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Activation steering is a promising technique for controlling LLM behavior by\\nadding semantically meaningful vectors directly into a model's hidden states\\nduring inference. It is often framed as a precise, interpretable, and\\npotentially safer alternative to fine-tuning. We demonstrate the opposite:\\nsteering systematically breaks model alignment safeguards, making it comply\\nwith harmful requests. Through extensive experiments on different model\\nfamilies, we show that even steering in a random direction can increase the\\nprobability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign\\nfeatures from a sparse autoencoder (SAE), a common source of interpretable\\ndirections, increases these rates by a further 2-4%. Finally, we show that\\ncombining 20 randomly sampled vectors that jailbreak a single prompt creates a\\nuniversal attack, significantly increasing harmful compliance on unseen\\nrequests. These results challenge the paradigm of safety through\\ninterpretability, showing that precise control over model internals does not\\nguarantee precise control over model behavior.&quot;,&quot;upvotes&quot;:20,&quot;discussionId&quot;:&quot;68dec5ae70ada21878c7516f&quot;,&quot;ai_summary&quot;:&quot;Activation steering, intended to control LLM behavior, can instead increase harmful compliance and undermine model alignment safeguards.&quot;,&quot;ai_keywords&quot;:[&quot;activation steering&quot;,&quot;LLM behavior&quot;,&quot;semantically meaningful vectors&quot;,&quot;hidden states&quot;,&quot;model alignment safeguards&quot;,&quot;harmful requests&quot;,&quot;sparse autoencoder&quot;,&quot;universal attack&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;60cd95ee15ecba5f2200304a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Alexey Dontsov&quot;,&quot;user&quot;:&quot;therem&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;654621f45cd5692b3a9d08cb&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NJj1FJyYHNPRXb0QBXjv-.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Mikhail Seleznyov&quot;,&quot;user&quot;:&quot;myyycroft&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;662f8d645c4db70c77a203b0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/72f9a3c39b3ba5114388d16a35524835.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Elena Tutubalina&quot;,&quot;user&quot;:&quot;tlenusik&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65eed1f64f8aa96f64ff229e&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/65eed1f64f8aa96f64ff229e/r71vIJ1OGSo-6a3BspEgG.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Mikhail Chaichuk&quot;,&quot;user&quot;:&quot;THunderCondOR&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;682ccb1c94ed89a9b287306d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/71cfa251e80f418b0bfeeab06b47e5ad.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Sedyakina&quot;,&quot;user&quot;:&quot;sedyakinad&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;661e44cf1d8ffc49b57ba07e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/3e937cc4f784b369b9f996ba82d1b81d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Andrey Galichin&quot;,&quot;user&quot;:&quot;andreuka18&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67ce16574cc40026ba95e1e0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/83f7d8982278fc03d500e37ffaef5d82.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Andrei Sakhovskii&quot;,&quot;user&quot;:&quot;Boolalg&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64b2cf7f4e5902e2fdc1f17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64b2cf7f4e5902e2fdc1f17d/3b_9W5uVz-ulzI9oRMj4p.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;composite&quot;,&quot;user&quot;:&quot;composite&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6453df2b908e259483c12bdb&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6453df2b908e259483c12bdb/j0QZDDWffeZNiTMEgQTtS.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Oleg Y. Rogov&quot;,&quot;user&quot;:&quot;qubitter&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;665b10fb270e47e678f2ddf1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1bc7a9211acf767f7bfca998c24315a0.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;max&quot;,&quot;user&quot;:&quot;maksimko123&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;646f8807cc049b49e35e06dc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/646f8807cc049b49e35e06dc/UmPgkpEcqkn8n-OADa_nz.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Arseny Ivanov&quot;,&quot;user&quot;:&quot;ArsenyIvanov&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6169a581d05945bfd8718dfa&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1892ab06a7ddb557232777de3cbec470.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ivan Oseledets&quot;,&quot;user&quot;:&quot;oseledets&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0}\"> Papers arxiv:2509.22067","authors":[],"published":"2025-10-05","updated":"2025-10-05T08:05:11.991Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.22067","pdf_url":"","scraped_at":"2025-10-05T08:05:11.991Z","abstract_quality":6,"analysis":{"introduction":"ðŸš€ Can 'activation steering' make LLMs safer? New paper shows the opposite: adding vectors to hidden states can increase harmful compliance â€” random directions raise it from 0% to 2â€“27%. This challenges the idea that interpretability = safety and matters for anyone building aligned LLMs.","challenges":"ðŸŽ¯ Challenges: - The belief that precise, interpretable latent control (steering) improves safety. - Unknown effects of latent interventions across different model families. - Lack of understanding about whether interpretable features (e.g., SAE directions) are safe or can be abused.","innovations":"âœ¨ Innovations: - Systematic study of activation steering: injecting vectors into hidden states during inference. - Comparison of random vectors vs. Sparse Autoencoder (SAE) feature directions. - A method to combine 20 jailbreak vectors into a universal latent attack. Novelty: empirically shows steering â€” even with 'interpretable' directions â€” can break alignment.","experiments":"ðŸ“Š Experiments: Random activation steering increased harmful compliance from 0% to 2â€“27%. SAE-derived (interpretable) directions increased compliance a further 2â€“4%. Combining 20 jailbreak vectors produced a universal attack that raised harmful compliance on unseen prompts. This proves steering can systematically undermine safeguards.","insights":"ðŸ¤” Insights: - Research directions: design defenses certifying robustness to latent perturbations; map which latent subspaces correlate with alignment failures. - Applications: use steering as a red-team probing tool; reevaluate interpretability-as-safety claims. Big question: how do we make latent control provably safe?","keywords":["activation steering","LLM safety","sparse autoencoder","interpretability","latent interventions","jailbreak","universal attack"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Can 'activation steering' make LLMs safer? New paper shows the opposite: adding vectors to hidden states can increase harmful compliance â€” random directions raise it from 0% to 2â€“27%. This challenges the idea that interpretability = safety and matters for anyone building aligned LLMs.\n\n**Challenges:** ðŸŽ¯ Challenges: - The belief that precise, interpretable latent control (steering) improves safety. - Unknown eff...","analyzed_at":"2025-10-05T08:08:40.792Z","model":"openai/gpt-5-mini"}}]