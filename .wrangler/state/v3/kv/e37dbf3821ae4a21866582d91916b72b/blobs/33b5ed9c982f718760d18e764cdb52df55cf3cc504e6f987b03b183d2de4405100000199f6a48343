[{"id":"arxiv_2510.14981v1","arxiv_id":"2510.14981v1","title":"Coupled Diffusion Sampling for Training-Free Multi-View Image Editing","abstract":"We present an inference-time diffusion sampling method to perform multi-view\nconsistent image editing using pre-trained 2D image editing models. These\nmodels can independently produce high-quality edits for each image in a set of\nmulti-view images of a 3D scene or object, but they do not maintain consistency\nacross views. Existing approaches typically address this by optimizing over\nexplicit 3D representations, but they suffer from a lengthy optimization\nprocess and instability under sparse view settings. We propose an implicit 3D\nregularization approach by constraining the generated 2D image sequences to\nadhere to a pre-trained multi-view image distribution. This is achieved through\ncoupled diffusion sampling, a simple diffusion sampling technique that\nconcurrently samples two trajectories from both a multi-view image distribution\nand a 2D edited image distribution, using a coupling term to enforce the\nmulti-view consistency among the generated images. We validate the\neffectiveness and generality of this framework on three distinct multi-view\nimage editing tasks, demonstrating its applicability across various model\narchitectures and highlighting its potential as a general solution for\nmulti-view consistent editing.","authors":["Hadi Alzayer","Yunzhi Zhang","Chen Geng","Jia-Bin Huang","Jiajun Wu"],"published":"2025-10-16T17:59:59Z","updated":"2025-10-16T17:59:59Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.14981v1","pdf_url":"http://arxiv.org/pdf/2510.14981v1.pdf","scraped_at":"2025-10-18T09:23:51.132Z","analysis":{"introduction":"ðŸš€ Want edits that stay consistent across all views of a 3D scene? Coupled Diffusion Sampling is an inference-time method that uses pre-trained 2D editors to produce multi-view consistent image edits without retraining. Important because it enforces implicit 3D regularization and avoids costly explicit 3D optimization.","challenges":"ðŸŽ¯ Key problems tackled: - Independent 2D edits produce view-inconsistent results. - Existing approaches optimize explicit 3D representations -> long optimization times. - Optimization-based 3D approaches become unstable under sparse view settings.","innovations":"âœ¨ Core ideas / techniques: - Inference-time diffusion sampling that is training-free (uses pre-trained 2D editors). - Coupled diffusion sampling: concurrently sample two diffusion trajectories â€” one from a multi-view image distribution and one from a 2D edited image distribution â€” with a coupling term. - Implicit 3D regularization via enforcing adherence to a pre-trained multi-view image distribution. Novel twist: coupling two diffusion trajectories to impose multi-view consistency at sampling time rather than optimizing explicit 3D structures.","experiments":"ðŸ“Š Evidence & main result: - Validated on three distinct multi-view image editing tasks, demonstrating applicability across various model architectures (as stated in the paper). - Quantitative single best result: Not specified in the paper.","insights":"ðŸ¤” Potential next steps & applications: - Extend coupled sampling to dynamic scenes / video to enforce temporal + multi-view consistency. - Combine coupling with explicit 3D representations (e.g., NeRF) for hybrid speed+accuracy gains. Possible real-world uses: consistent 3D asset editing for AR/VR, multi-view dataset augmentation for 3D learning. Could coupling be the general inference-time tool for other cross-view consistency problems?","keywords":["coupled diffusion sampling","multi-view consistency","diffusion models","inference-time sampling","training-free editing","implicit 3D regularization","2D image editing models"],"category":"computer_vision","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want edits that stay consistent across all views of a 3D scene? Coupled Diffusion Sampling is an inference-time method that uses pre-trained 2D editors to produce multi-view consistent image edits without retraining. Important because it enforces implicit 3D regularization and avoids costly explicit 3D optimization.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Independent 2D edits produce view-inconsistent result...","analyzed_at":"2025-10-18T09:24:50.299Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.14979v1","arxiv_id":"2510.14979v1","title":"From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale","abstract":"The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.","authors":["Haiwen Diao","Mingxuan Li","Silei Wu","Linjun Dai","Xiaohua Wang","Hanming Deng","Lewei Lu","Dahua Lin","Ziwei Liu"],"published":"2025-10-16T17:59:58Z","updated":"2025-10-16T17:59:58Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.14979v1","pdf_url":"http://arxiv.org/pdf/2510.14979v1.pdf","scraped_at":"2025-10-18T09:23:51.132Z","analysis":{"introduction":"ðŸš€ Curious if a single model can learn pixels and words natively? Meet NEO: a family of native Vision-Language Models built from first principles that align pixelâ†”word in a shared space and aim to rival modular VLMs. Important for researchers and practitioners seeking compact, unified multimodal systems.","challenges":"ðŸŽ¯ Key problems tackled: - Unclear fundamental limits of native VLMs vs modular VLMs and whether those barriers can be overcome. - Lack of accessibility and democratization for native VLM research. - Difficulty aligning pixel and word representations and integrating vision+language strengths in a single model.","innovations":"âœ¨ Core contributions: - NEO: a novel family of native VLMs designed from first principles. - Introduces native VLM primitives that align pixel and word representations in a shared semantic space. - Builds a dense, monolithic model that integrates vision and language capabilities and embodies cross-modal properties. - Provides reusable components to foster a cost-effective, extensible ecosystem (code & models released).","experiments":"ðŸ“Š Experimental highlight: - Quantitative: NEO is trained with only 390M imageâ€“text examples and reportedly develops visual perception from scratch while mitigating visionâ€“language conflicts. - What it proves: Native, monolithic VLMs can be competitive with top-tier modular counterparts. - Note: The abstract does not report specific benchmark numbers or % improvements.","insights":"ðŸ¤” What's next (potential directions & impact): - Research direction: Study scaling laws and trade-offs between monolithic native primitives and modular architectures (efficiency, robustness, interpretability). - Application idea: Apply native VLM primitives to embodied agents, multimodal search, or low-cost deployment scenarios to test real-world gains. Could native native VLMs simplify multimodal stacks across industries?","keywords":["NEO","native VLM","vision-language","pixel-word alignment","monolithic model","cross-modal primitives","multimodal","open-source","390M image-text"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Curious if a single model can learn pixels and words natively? Meet NEO: a family of native Vision-Language Models built from first principles that align pixelâ†”word in a shared space and aim to rival modular VLMs. Important for researchers and practitioners seeking compact, unified multimodal systems.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Unclear fundamental limits of native VLMs vs modular VLMs and whethe...","analyzed_at":"2025-10-18T09:24:49.652Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.14980v1","arxiv_id":"2510.14980v1","title":"Agentic Design of Compositional Machines","abstract":"The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning.","authors":["Wenqian Zhang","Weiyang Liu","Zhen Liu"],"published":"2025-10-16T17:59:58Z","updated":"2025-10-16T17:59:58Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.14980v1","pdf_url":"http://arxiv.org/pdf/2510.14980v1.pdf","scraped_at":"2025-10-18T09:23:51.132Z","analysis":{"introduction":"ðŸš€ Can LLMs learn to invent physical machines? The paper asks this and answers by introducing BesiegeField â€” a testbed that lets LLMs assemble part-based machines in a physics game to meet tasks like locomotion/manipulation. Big step toward AI-driven design for robotics and creative engineering.","challenges":"ðŸŽ¯ Key problems tackled: - Lack of standardized testbeds for part-based, reward-driven machine design. - Current LLMs struggle with spatial/physical reasoning and strategic assembly for functional machines. - Open-source models underperform on instruction-following in embodied design tasks.","innovations":"âœ¨ Core contributions: - BesiegeField: a Besiege-based testbed enabling part-based construction + physics simulation + reward evaluation. - Benchmarking pipeline: agentic workflows for prompting/controlling LLMs to build machines. - RL finetuning path: curated cold-start dataset and RL experiments to improve design capability. Novel twist: applying agentic LLM workflows to compositional physical design in a realistic game sim.","experiments":"ðŸ“Š Quantitative result: Not specified in the paper. Demonstrated by experiments: benchmarking showed open-source LLMs fall short on compositional machine design; identified crucial capabilities (spatial reasoning, strategic assembly, instruction-following) and explored RL finetuning as a promising route to improvement.","insights":"ðŸ¤” Future directions & implications: - Combine LLMs with learned physics predictors or differentiable simulators to close the sim-to-design gap. - Transfer compositional design skills to modular robotics or rapid prototyping tools for engineers and educators. Could agentic LLMs become practical co-design partners for real-world machine building?","keywords":["BesiegeField","compositional design","large language models","agentic workflows","reinforcement learning","physical simulation","part-based construction","cold-start dataset","spatial reasoning"],"category":"machine_learning","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Can LLMs learn to invent physical machines? The paper asks this and answers by introducing BesiegeField â€” a testbed that lets LLMs assemble part-based machines in a physics game to meet tasks like locomotion/manipulation. Big step toward AI-driven design for robotics and creative engineering.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Lack of standardized testbeds for part-based, reward-driven machine design. -...","analyzed_at":"2025-10-18T09:24:46.837Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.14978v1","arxiv_id":"2510.14978v1","title":"Learning an Image Editing Model without Image Editing Pairs","abstract":"Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.","authors":["Nupur Kumari","Sheng-Yu Wang","Nanxuan Zhao","Yotam Nitzan","Yuheng Li","Krishna Kumar Singh","Richard Zhang","Eli Shechtman","Jun-Yan Zhu","Xun Huang"],"published":"2025-10-16T17:59:57Z","updated":"2025-10-16T17:59:57Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.14978v1","pdf_url":"http://arxiv.org/pdf/2510.14978v1.pdf","scraped_at":"2025-10-18T09:23:51.132Z","analysis":{"introduction":"ðŸš€ Tired of chasing curated inputâ€“target image pairs? This paper trains an image editing diffusion model with NO paired data â€” it unrolls a few-step diffusion during training and uses visionâ€“language models for direct feedback. Big win for scalable, instruction-driven editing.","challenges":"ðŸŽ¯ Key problems tackled: - Paired inputâ€“target image editing data are scarce and hard to collect. - Synthetic training pairs can propagate and magnify pretrained-model artifacts. - Reliance on supervised fine-tuning with large paired datasets limits scalability.","innovations":"âœ¨ Core innovations: - Unroll a few-step diffusion model during training for end-to-end optimization. - Use a visionâ€“language model (VLM) to score edits on instruction-following and content preservation, providing direct gradients. - Introduce Distribution Matching Loss (DMD) to keep outputs on the pretrained image manifold. Novel twist: removes need for any paired image-edit examples by combining unrolled diffusion + VLM feedback.","experiments":"ðŸ“Š Main empirical claim: Without any paired data, the method performs on par with image-editing diffusion models trained on large supervised paired datasets in the few-step setting, and it outperforms RL-based Flow-GRPO when using the same VLM. (No numerical % improvements provided.)","insights":"ðŸ¤” What's next? - Research directions: scale to longer multi-step/high-res edits and explore stronger or specialized VLM critics; study stability/robustness of VLM gradients. - Applications: scalable photo retouching, content-preserving creative tools, assistive editing for non-experts. Could VLM-guided, unrolled training replace supervised pair collections for more editing tasks?","keywords":["diffusion models","image editing","vision-language model","unrolled training","distribution matching loss","DMD","no-paired-data","few-step editing","Flow-GRPO"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Tired of chasing curated inputâ€“target image pairs? This paper trains an image editing diffusion model with NO paired data â€” it unrolls a few-step diffusion during training and uses visionâ€“language models for direct feedback. Big win for scalable, instruction-driven editing.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Paired inputâ€“target image editing data are scarce and hard to collect. - Synthetic training pair...","analyzed_at":"2025-10-18T09:25:17.470Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.14976v1","arxiv_id":"2510.14976v1","title":"Ponimator: Unfolding Interactive Pose for Versatile Human-human\n  Interaction Animation","abstract":"Close-proximity human-human interactive poses convey rich contextual\ninformation about interaction dynamics. Given such poses, humans can\nintuitively infer the context and anticipate possible past and future dynamics,\ndrawing on strong priors of human behavior. Inspired by this observation, we\npropose Ponimator, a simple framework anchored on proximal interactive poses\nfor versatile interaction animation. Our training data consists of\nclose-contact two-person poses and their surrounding temporal context from\nmotion-capture interaction datasets. Leveraging interactive pose priors,\nPonimator employs two conditional diffusion models: (1) a pose animator that\nuses the temporal prior to generate dynamic motion sequences from interactive\nposes, and (2) a pose generator that applies the spatial prior to synthesize\ninteractive poses from a single pose, text, or both when interactive poses are\nunavailable. Collectively, Ponimator supports diverse tasks, including\nimage-based interaction animation, reaction animation, and text-to-interaction\nsynthesis, facilitating the transfer of interaction knowledge from high-quality\nmocap data to open-world scenarios. Empirical experiments across diverse\ndatasets and applications demonstrate the universality of the pose prior and\nthe effectiveness and robustness of our framework.","authors":["Shaowei Liu","Chuan Guo","Bing Zhou","Jian Wang"],"published":"2025-10-16T17:59:56Z","updated":"2025-10-16T17:59:56Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.14976v1","pdf_url":"http://arxiv.org/pdf/2510.14976v1.pdf","scraped_at":"2025-10-18T09:23:51.132Z","analysis":{"introduction":"ðŸš€ Ever peeked at a hug and predicted what happened next? Ponimator anchors on close-proximity interactive poses and uses learned pose priors to animate and create two-person interactions. It brings mocap-quality interaction dynamics to images, text or single poses.","challenges":"ðŸŽ¯ Problems tackled: - Generating realistic dynamic motion from sparse or single-frame interactive cues. - Synthesizing plausible interactive poses when ground-truth interactions are unavailable (single pose or text only). - Bridging high-quality mocap interaction data to open-world scenarios.","innovations":"âœ¨ Core ideas: - Two conditional diffusion models: a pose animator (temporal prior) and a pose generator (spatial prior). - Anchor on proximal interactive poses as explicit priors for dynamics. - Novelty: explicit separation of spatial vs. temporal interactive priors enabling both animation from interaction poses and generation when interactions are missing.","experiments":"ðŸ“Š Not specified in the paper. The abstract states experiments across diverse datasets show the universality of the pose prior and that Ponimator is effective and robust, but no concrete quantitative metric or % improvement is reported in the provided text.","insights":"ðŸ¤” Directions & applications: - Future research: integrate contact-aware physics or appearance (cloth/hands) for more physical realism; scale to multi-person (>2) and crowded interactions. - Applications: VR/AR social presence, responsive NPCs in games, social-robot reaction planning. Could learned interaction priors enable safer, more natural robot behavior?","keywords":["interactive pose","human-human interaction","diffusion model","motion synthesis","pose prior","motion capture","two-person interaction","text-to-interaction"],"category":"computer_vision","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Ever peeked at a hug and predicted what happened next? Ponimator anchors on close-proximity interactive poses and uses learned pose priors to animate and create two-person interactions. It brings mocap-quality interaction dynamics to images, text or single poses.\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Generating realistic dynamic motion from sparse or single-frame interactive cues. - Synthesizing plausible inte...","analyzed_at":"2025-10-18T09:25:09.655Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.14977v1","arxiv_id":"2510.14977v1","title":"Terra: Explorable Native 3D World Model with Point Latents","abstract":"World models have garnered increasing attention for comprehensive modeling of\nthe real world. However, most existing methods still rely on pixel-aligned\nrepresentations as the basis for world evolution, neglecting the inherent 3D\nnature of the physical world. This could undermine the 3D consistency and\ndiminish the modeling efficiency of world models. In this paper, we present\nTerra, a native 3D world model that represents and generates explorable\nenvironments in an intrinsic 3D latent space. Specifically, we propose a novel\npoint-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into\na latent point representation, which is subsequently decoded as 3D Gaussian\nprimitives to jointly model geometry and appearance. We then introduce a sparse\npoint flow matching network (SPFlow) for generating the latent point\nrepresentation, which simultaneously denoises the positions and features of the\npoint latents. Our Terra enables exact multi-view consistency with native 3D\nrepresentation and architecture, and supports flexible rendering from any\nviewpoint with only a single generation process. Furthermore, Terra achieves\nexplorable world modeling through progressive generation in the point latent\nspace. We conduct extensive experiments on the challenging indoor scenes from\nScanNet v2. Terra achieves state-of-the-art performance in both reconstruction\nand generation with high 3D consistency.","authors":["Yuanhui Huang","Weiliang Chen","Wenzhao Zheng","Xin Tao","Pengfei Wan","Jie Zhou","Jiwen Lu"],"published":"2025-10-16T17:59:56Z","updated":"2025-10-16T17:59:56Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.14977v1","pdf_url":"http://arxiv.org/pdf/2510.14977v1.pdf","scraped_at":"2025-10-18T09:23:51.132Z","analysis":{"introduction":"ðŸš€ What if world models were truly 3D-native instead of pixel-aligned? Terra proposes a native 3D world model that uses point latents to represent and generate explorable environments in an intrinsic 3D latent space â€” enabling exact multi-view consistency and flexible single-pass rendering. Benefits: better 3D consistency for VR/robotics/simulations.","challenges":"ðŸŽ¯ Key problems tackled: - Existing world models rely on pixel-aligned representations, neglecting the physical world's 3D nature. - This undermines 3D consistency across views and reduces modeling efficiency. - Limited support for explorable, progressive 3D generation in latent space.","innovations":"âœ¨ Core contributions: - Point-to-Gaussian VAE (P2G-VAE): encodes 3D inputs into latent point representations and decodes them as 3D Gaussian primitives to jointly model geometry & appearance. - Sparse Point Flow (SPFlow): a flow/matching network that generates point latents while denoising positions and features. - Native 3D latent architecture enabling exact multi-view consistency and single-generation flexible rendering. - Progressive generation in point latent space to enable explorable worlds. Novelty: a point-latent â†’ Gaussian-primitive pipeline plus a sparse flow generator, bringing a true 3D-native latent world model.","experiments":"ðŸ“Š Results: Terra was evaluated on challenging indoor scenes from ScanNet v2 and achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency. Specific numeric improvements or % gains: Not specified in the paper.","insights":"ðŸ¤” Where this could lead: - Future research: extend Terra to dynamic scenes (temporal point-latents) and scale to larger/outdoor environments or denser primitives. - Applications: AR/VR scene authoring, robot navigation/sim-to-real environments, game content generation. Could point-latent world models become the standard backbone for interactive 3D agents and content creation?","keywords":["point latents","P2G-VAE","SPFlow","3D world model","Gaussian primitives","multi-view consistency","ScanNet v2","explorable environments"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ What if world models were truly 3D-native instead of pixel-aligned? Terra proposes a native 3D world model that uses point latents to represent and generate explorable environments in an intrinsic 3D latent space â€” enabling exact multi-view consistency and flexible single-pass rendering. Benefits: better 3D consistency for VR/robotics/simulations.","analyzed_at":"2025-10-18T09:25:08.154Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.14975v1","arxiv_id":"2510.14975v1","title":"WithAnyone: Towards Controllable and ID Consistent Image Generation","abstract":"Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.","authors":["Hengyuan Xu","Wei Cheng","Peng Xing","Yixiao Fang","Shuhan Wu","Rui Wang","Xianfang Zeng","Daxin Jiang","Gang Yu","Xingjun Ma","Yu-Gang Jiang"],"published":"2025-10-16T17:59:54Z","updated":"2025-10-16T17:59:54Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.14975v1","pdf_url":"http://arxiv.org/pdf/2510.14975v1.pdf","scraped_at":"2025-10-18T09:23:51.132Z","analysis":{"introduction":"ðŸš€ Tired of text-to-image models that either ignore a reference face or literally copy-paste it? WithAnyone presents a new path for identity-consistent generation: a diffusion model + new dataset and training loss that keeps identity but lets pose/expression vary. Why it matters: better controllable, realistic identity-conditioned images.","challenges":"ðŸŽ¯ Key problems tackled: - Copy-paste failure: models replicate the reference face instead of preserving identity across variations. - Data scarcity: lack of large-scale paired multi-image datasets per identity. - Trade-off: balancing identity fidelity vs. expressive variation is hard.","innovations":"âœ¨ Core innovations: - MultiID-2M: a large-scale paired dataset tailored for multi-person scenarios. - New benchmark: quantifies copy-paste artifacts and the fidelity-vs-variation trade-off. - Contrastive identity loss: training paradigm that balances identity fidelity with diversity. - WithAnyone: a diffusion-based model integrating above elements to mitigate copy-paste.","experiments":"ðŸ“Š Not specified in the paper: no single quantitative % improvement is reported in the abstract.  What experiments demonstrate: WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose/expression, maintains strong perceptual quality, and user studies report high identity fidelity.","insights":"ðŸ¤” Whatâ€™s next (inspired ideas): - Extend identity-contrastive training to video for temporally consistent identity control. - Explore privacy-preserving or fairness-aware versions to avoid misuse and bias. Potential applications: more controllable avatars, character design, and photo-realistic editing. Could this reshape identity-conditioned content creation?","keywords":["identity-consistent generation","copy-paste","MultiID-2M","contrastive identity loss","diffusion model","controllable generation","benchmark","pose/expression control"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Tired of text-to-image models that either ignore a reference face or literally copy-paste it? WithAnyone presents a new path for identity-consistent generation: a diffusion model + new dataset and training loss that keeps identity but lets pose/expression vary. Why it matters: better controllable, realistic identity-conditioned images.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Copy-paste failure: models replic...","analyzed_at":"2025-10-18T09:25:32.942Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.14974v1","arxiv_id":"2510.14974v1","title":"pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation","abstract":"Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard $\\ell_2$ flow matching loss. By simply\nmimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.","authors":["Hansheng Chen","Kai Zhang","Hao Tan","Leonidas Guibas","Gordon Wetzstein","Sai Bi"],"published":"2025-10-16T17:59:51Z","updated":"2025-10-16T17:59:51Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.14974v1","pdf_url":"http://arxiv.org/pdf/2510.14974v1.pdf","scraped_at":"2025-10-18T09:23:51.132Z","analysis":{"introduction":"ðŸš€ Ever wish high-quality generative models could sample in just a few steps? pi-Flow introduces a policy-based flow student that predicts a network-free policy to generate dynamic velocities â€” enabling fast, few-step ODE sampling that preserves teacher quality. Who wins: image generative models and large-model deployment.","challenges":"ðŸŽ¯ Problems tackled: - Format mismatch: teacher velocity predictors vs. student \"shortcut\" predictors causes complex distillation. - Qualityâ€“diversity trade-off in existing few-step distillation. - High cost of extra network evaluations for substep integration.","innovations":"âœ¨ Core ideas: - pi-Flow: student flow modifies output layer to predict a network-free policy at one timestep. - Policy generates dynamic flow velocities for future substeps with negligible overhead. - Imitation distillation: match policy's velocity to the teacher's along the policy trajectory using an L2 flow-matching loss. Novelty: network-free dynamic policy enabling accurate ODE substeps without extra network calls, avoiding prior qualityâ€“diversity compromises.","experiments":"ðŸ“Š Key result: On ImageNet 256^2 pi-Flow achieves a 1-NFE FID of 2.85, outperforming MeanFlow with the same DiT backbone. Also: on FLUX.1-12B and Qwen-Image-20B at 4 NFEs, pi-Flow attains substantially better diversity than state-of-the-art few-step methods while maintaining teacher-level quality.","insights":"ðŸ¤” Next steps & applications: - Explore conditional or multimodal extensions (e.g., text-to-image) and hierarchical policies for even fewer NFEs. - Apply imitation-distilled policies to video or fast on-device sampling to cut inference cost. Could pi-Flow become the standard for practical, low-cost high-quality sampling?","keywords":["pi-Flow","imitation distillation","flow matching","few-step generation","ODE integration","ImageNet","FID","DiT","quality-diversity tradeoff","generative_models"],"category":"generative_models","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Ever wish high-quality generative models could sample in just a few steps? pi-Flow introduces a policy-based flow student that predicts a network-free policy to generate dynamic velocities â€” enabling fast, few-step ODE sampling that preserves teacher quality. Who wins: image generative models and large-model deployment.\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Format mismatch: teacher velocity predictors vs. stud...","analyzed_at":"2025-10-18T09:25:34.780Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.14973v1","arxiv_id":"2510.14973v1","title":"Attention Is All You Need for KV Cache in Diffusion LLMs","abstract":"This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.","authors":["Quan Nguyen-Tri","Mukul Ranjan","Zhiqiang Shen"],"published":"2025-10-16T17:59:48Z","updated":"2025-10-16T17:59:48Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.14973v1","pdf_url":"http://arxiv.org/pdf/2510.14973v1.pdf","scraped_at":"2025-10-18T09:23:51.132Z","analysis":{"introduction":"ðŸš€ Tired of slow diffusion LLM decoding? Elastic-Cache adaptively refreshes KV caches (when + where) to cut redundant QKV recomputation â€” training-free and architecture-agnostic. Big win: much lower decoding latency for diffusion LLMs (math & code tasks).","challenges":"ðŸŽ¯ Problems tackled: - Existing decoders recompute QKV for all tokens at every denoising step & layer â†’ massive redundancy. - KV states change little across steps (esp. shallow layers) but are still recomputed. - Latency/throughput bottlenecks make diffusion LLMs impractical for deployment.","innovations":"âœ¨ Key ideas: - Attention-aware drift test on the most-attended token to decide WHEN to refresh. - Depth-aware schedule: recompute from a chosen deeper layer onward (reuse shallow-layer caches). - Block-wise caching for off-window MASK tokens. Novel twist: jointly deciding WHEN+WHERE using the most-attended token as a conservative KV-drift bound.","experiments":"ðŸ“Š Standout results: - Up to 45.1Ã— decoding speedup on longer sequences; 8.7Ã— on GSM8K (256 tokens); 4.8Ã— on HumanEval. - Throughput: 6.8Ã— on GSM8K. Tests on LLaDA-Instruct / LLaDA-1.5 / LLaDA-V (math & code) with negligible quality loss and higher accuracy than baseline.","insights":"ðŸ¤” Next steps & impacts: - Research: extend adaptive cache refresh to other transformer decoders; combine Elastic-Cache with confidence-based heuristics or hardware-aware scheduling. - Applications: lower-latency, energy-efficient inference for real-time assistants and code/math tools. Could this enable wider deployment of diffusion LLMs?","keywords":["Elastic-Cache","KV cache","diffusion LLMs","attention-aware drift","depth-aware schedule","MASK caching","LLaDA","decoding latency"],"category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Tired of slow diffusion LLM decoding? Elastic-Cache adaptively refreshes KV caches (when + where) to cut redundant QKV recomputation â€” training-free and architecture-agnostic. Big win: much lower decoding latency for diffusion LLMs (math & code tasks).\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Existing decoders recompute QKV for all tokens at every denoising step & layer â†’ massive redundancy. - KV states change li...","analyzed_at":"2025-10-18T09:25:33.821Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.14972v1","arxiv_id":"2510.14972v1","title":"TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar","abstract":"Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs.","authors":["Yinxi Li","Yuntian Deng","Pengyu Nie"],"published":"2025-10-16T17:59:45Z","updated":"2025-10-16T17:59:45Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.14972v1","pdf_url":"http://arxiv.org/pdf/2510.14972v1.pdf","scraped_at":"2025-10-18T09:23:51.132Z","analysis":{"introduction":"ðŸš€ Did you know tiny formatting or identifier changes can flip a code LLM's behavior? TokDrift exposes that subword tokenizers (e.g., BPE) are driven by stats, not grammar â€” causing identical code to be tokenized differently. This threatens reliable code generation and model design.","challenges":"ðŸŽ¯ Key problems tackled: - Subword tokenizers (BPE) learned from mixed text/code are driven by statistics, not grammar. - Semantically identical code can be tokenized differently due to whitespace or identifier naming. - Tokenization mismatch causes unpredictable shifts in model behavior.","innovations":"âœ¨ Core contributions: - TokDrift: a framework applying semantic-preserving rewrite rules to produce code variants that differ only in tokenization. - Controlled evaluation across models to isolate tokenization effects. - Layer-wise analysis showing the issue originates in early embeddings. Novelty: isolates tokenization as the causal factor by keeping semantics fixed while varying subword segmentation.","experiments":"ðŸ“Š Results: Across nine code LLMs (including models >30B parameters), minor formatting/tokenization-only changes produced substantial shifts in model behavior â€” demonstrating that misaligned subword tokenization meaningfully impacts even very large models. Layer analysis ties the problem to early embeddings.","insights":"Not provided","keywords":[],"category":"machine_learning","relevance_score":5,"summary":"**Introduction:** ðŸš€ Did you know tiny formatting or identifier changes can flip a code LLM's behavior? TokDrift exposes that subword tokenizers (e.g., BPE) are driven by stats, not grammar â€” causing identical code to be tokenized differently. This threatens reliable code generation and model design.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Subword tokenizers (BPE) learned from mixed text/code are driven by statistics, not gramma...","analyzed_at":"2025-10-18T09:26:00.944Z","model":"openai/gpt-5-mini"}},{"id":"hf_when_models_lie__we_learn__multilingual_span_level_hallucination_detection_with_psiloqa_1760779439435","title":"When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA","abstract":"Hallucination detection remains a fundamental challenge for the safe and reliable deployment of large language models (LLMs), especially in applications requiring factual accuracy. Existing hallucination benchmarks often operate at the sequence level and are limited to English, lacking the fine-grained, multilingual supervision needed for a comprehensive evaluation. In this work, we introduce PsiloQA, a large-scale, multilingual dataset annotated with span-level hallucinations across 14 languages. PsiloQA is constructed through an automated three-stage pipeline: generating question-answer pairs from Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse LLMs in a no-context setting, and automatically annotating hallucinated spans using GPT-4o by comparing against golden answers and retrieved context. We evaluate a wide range of hallucination detection methods -- including uncertainty quantification, LLM-based tagging, and fine-tuned encoder models -- and show that encoder-based models achieve the strongest performance across languages. Furthermore, PsiloQA demonstrates effective cross-lingual generalization and supports robust knowledge transfer to other benchmarks, all while being significantly more cost-efficient than human-annotated datasets. Our dataset and results advance the development of scalable, fine-grained hallucination detection in multilingual settings.","authors":[],"published":"2025-10-18","updated":"2025-10-18T09:23:59.435Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.04849","pdf_url":"","scraped_at":"2025-10-18T09:23:59.435Z","abstract_quality":7,"analysis":{"introduction":"ðŸš€ Want LLMs to stop confidently inventing facts? Meet PsiloQA â€” a new multilingual, span-level hallucination dataset across 14 languages built with an automated GPT-4o pipeline. It aims to pinpoint exactly which spans lie, helping safer, more factual LLMs for global users.","challenges":"ðŸŽ¯ Key problems tackled: - Existing benchmarks are mostly sequence-level and English-only, so they miss fine-grained errors. - Lack of multilingual, span-level supervision makes detection and evaluation brittle. - Human annotation at scale is costly and slow.","innovations":"âœ¨ Core contributions: - PsiloQA: large-scale multilingual dataset with span-level hallucination annotations (14 languages). - Automated three-stage pipeline: (1) QA generation from Wikipedia with GPT-4o, (2) elicit no-context answers from diverse LLMs, (3) automatic span-level annotation via GPT-4o vs gold + retrieved context. - Systematic eval of uncertainty metrics, LLM tagging, and fine-tuned encoder detectors. Novelty: fine-grained, multilingual span annotations created cost-efficiently with an automated pipeline.","experiments":"ðŸ“Š Quantitative highlight: Not specified in the paper. Qualitative findings: encoder-based models achieved the strongest performance across languages; PsiloQA supports cross-lingual generalization and transfers to other benchmarks while being far more cost-efficient than human annotation.","insights":"ðŸ¤” Next directions & applications: - Research: adapt span-level detectors to context-rich or long-document settings and stress-test on low-resource languages. - Applications: integrate span-level alerts in retrieval-augmented generation systems and fact-checking pipelines. Could automated multilingual span labels enable real-time hallucination mitigation in deployed LLMs?","keywords":["hallucination detection","PsiloQA","multilingual","span-level","LLMs","GPT-4o","encoder models","cross-lingual generalization","automated annotation"],"category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want LLMs to stop confidently inventing facts? Meet PsiloQA â€” a new multilingual, span-level hallucination dataset across 14 languages built with an automated GPT-4o pipeline. It aims to pinpoint exactly which spans lie, helping safer, more factual LLMs for global users.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Existing benchmarks are mostly sequence-level and English-only, so they miss fine-grained errors. -...","analyzed_at":"2025-10-18T09:25:51.019Z","model":"openai/gpt-5-mini"}},{"id":"hf_agentic_entropy_balanced_policy_optimization_1760779443188","title":"Agentic Entropy-Balanced Policy Optimization","abstract":"Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.","authors":[],"published":"2025-10-18","updated":"2025-10-18T09:24:03.188Z","category":"reinforcement_learning","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.14545","pdf_url":"","scraped_at":"2025-10-18T09:24:03.188Z","abstract_quality":7,"analysis":{"introduction":"ðŸš€ Want web agents that use tools reliably without collapsing during training? Agentic Entropy-Balanced Policy Optimization (AEPO) is a new RL recipe that balances entropy during rollouts and updates to prevent over-exploration and training collapse â€” improving multi-turn, long-horizon tool use for web agents.","challenges":"ðŸŽ¯ Key problems addressed: - Over-reliance on entropy-driven exploration causes training collapse. - Unchecked high-entropy tool-call steps lead to over-branching and wasted samples. - Standard policy updates mis-handle high-uncertainty tokens, hurting learning efficiency.","innovations":"âœ¨ Core ideas / novelty: - Dynamic entropy-balanced rollout: entropy pre-monitoring to adapt global vs branch sampling budget + branch penalty to curb consecutive high-entropy tool calls. - Entropy-Balanced Policy Optimization: stop-gradient in high-entropy clipping to preserve/rescale gradients on high-entropy tokens + entropy-aware advantage estimation to prioritize uncertain tokens. - Novel twist: combining pre-rollout entropy control with a gradient-level fix (stop-gradient) to keep exploration diverse while stabilizing updates.","experiments":"ðŸ“Š Proof: AEPO outperforms 7 mainstream RL algorithms across 14 datasets. Most compelling â€” with just 1K RL samples, Qwen3-14B + AEPO achieves GAIA Pass@1 = 47.6% (Pass@5 = 65.0%), Humanity's Last Exam Pass@1 = 11.2% (Pass@5 = 26.0%), WebWalker Pass@1 = 43.0% (Pass@5 = 70.0%).","insights":"ðŸ¤” What's next? (inspired ideas) - Explore per-token adaptive entropy control (fine-grained exploration budgets) to further boost long-horizon tool chains. - Combine AEPO with model-based planning or offline pretraining to reduce RL sample needs. Broader impact: better, stable web automation, safer multi-step tool use agents. Could this generalize to robotics tool chains?","keywords":["Agentic RL","AEPO","entropy balancing","policy optimization","rollout sampling","stop-gradient","entropy-aware advantage","Qwen3-14B","GAIA","WebWalker","tool-use agents"],"category":"reinforcement_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want web agents that use tools reliably without collapsing during training? Agentic Entropy-Balanced Policy Optimization (AEPO) is a new RL recipe that balances entropy during rollouts and updates to prevent over-exploration and training collapse â€” improving multi-turn, long-horizon tool use for web agents.\n\n**Challenges:** ðŸŽ¯ Key problems addressed: - Over-reliance on entropy-driven exploration causes training ...","analyzed_at":"2025-10-18T09:25:51.070Z","model":"openai/gpt-5-mini"}},{"id":"hf_ai_for_service__proactive_assistance_with_ai_glasses_1760779450103","title":"AI for Service: Proactive Assistance with AI Glasses","abstract":"In an era where AI is evolving from a passive tool into an active and adaptive companion, we introduce AI for Service (AI4Service), a new paradigm that enables proactive and real-time assistance in daily life. Existing AI services remain largely reactive, responding only to explicit user commands. We argue that a truly intelligent and helpful assistant should be capable of anticipating user needs and taking actions proactively when appropriate. To realize this vision, we propose Alpha-Service, a unified framework that addresses two fundamental challenges: Know When to intervene by detecting service opportunities from egocentric video streams, and Know How to provide both generalized and personalized services. Inspired by the von Neumann computer architecture and based on AI glasses, Alpha-Service consists of five key components: an Input Unit for perception, a Central Processing Unit for task scheduling, an Arithmetic Logic Unit for tool utilization, a Memory Unit for long-term personalization, and an Output Unit for natural human interaction. As an initial exploration, we implement Alpha-Service through a multi-agent system deployed on AI glasses. Case studies, including a real-time Blackjack advisor, a museum tour guide, and a shopping fit assistant, demonstrate its ability to seamlessly perceive the environment, infer user intent, and provide timely and useful assistance without explicit prompts.","authors":[],"published":"2025-10-18","updated":"2025-10-18T09:24:10.103Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.14359","pdf_url":"","scraped_at":"2025-10-18T09:24:10.103Z","abstract_quality":7,"analysis":{"introduction":"ðŸš€ Tired of assistants that only work when you ask? Alpha-Service turns AI glasses into proactive helpers. It detects â€˜service opportunitiesâ€™ from your egocentric video and acts in real time to assistâ€”anticipating needs for everyday users and accessibility scenarios.","challenges":"ðŸŽ¯ Challenges: - Reactive assistants: AI today waits for explicit user commands. - Opportunity detection: Identifying when to intervene from egocentric video streams. - Generalization vs personalization: Delivering both broad services and long-term user-specific help.","innovations":"âœ¨ Innovations: - Alpha-Service: a von Neumannâ€“inspired five-unit design (Input Unit, Central Processing Unit for scheduling, Arithmetic Logic Unit for tool use, Memory Unit for personalization, Output Unit for interaction). - Implemented as a multi-agent system on AI glasses. - Novelty: unified wearable framework that proactively detects when to intervene and how to provide generalized + personalized services.","experiments":"ðŸ“Š Experiment: - Quantitative result: Not specified in the paper. - Demonstrated breakthrough: Through case studies (real-time Blackjack advisor, museum tour guide, shopping fit assistant) the system can perceive the environment, infer user intent, and provide timely, useful assistance without explicit prompts.","insights":"ðŸ¤” Insights: - Future research: (1) On-device, privacy-preserving personalization and continual learning; (2) Learning intervention policies to minimize false positives and user annoyance. - Applications: hands-free accessibility aids, retail/fit assistants, AR-guided tours. Could proactive wearables reshape daily interactions?","keywords":["proactive AI","egocentric vision","wearable AI","AI glasses","multi-agent system","personalization","von Neumann architecture","human-computer interaction"],"category":"machine_learning","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Tired of assistants that only work when you ask? Alpha-Service turns AI glasses into proactive helpers. It detects â€˜service opportunitiesâ€™ from your egocentric video and acts in real time to assistâ€”anticipating needs for everyday users and accessibility scenarios.\n\n**Challenges:** ðŸŽ¯ Challenges: - Reactive assistants: AI today waits for explicit user commands. - Opportunity detection: Identifying when to interve...","analyzed_at":"2025-10-18T09:26:15.657Z","model":"openai/gpt-5-mini"}},{"id":"hf_imagerysearch__adaptive_test_time_search_for_video_generation_beyond_semantic_dependency_constraints_1760779456794","title":"ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints","abstract":"Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation.","authors":[],"published":"2025-10-18","updated":"2025-10-18T09:24:16.794Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.14847","pdf_url":"","scraped_at":"2025-10-18T09:24:16.794Z","abstract_quality":7,"analysis":{"introduction":"ðŸš€ Ever seen video AIs choke on wildly imaginative prompts? ImagerySearch is a prompt-guided adaptive test-time search that dynamically adjusts inference search space and reward based on prompt semantics â€” enabling more coherent, creative video generation for hard prompts.","challenges":"ðŸŽ¯ Problems tackled: - Existing video models fail on imaginative prompts with rarely co-occurring concepts and long-distance semantic relations. - Such prompts lie outside training distributions, so outputs become incoherent. - Fixed test-time scaling & static rewards lack adaptability for these cases.","innovations":"âœ¨ Core contributions: - Prompt-guided adaptive test-time search that changes the inference search space per prompt. - Dynamic reward-function adjustment conditioned on semantic relationships in the prompt. - LDT-Bench: a new benchmark of 2,839 long-distance concept pairs + automated creative-assessment protocol. Novelty: jointly adapting search space and reward at test time based on prompt semantics.","experiments":"ðŸ“Š Single most compelling quantitative result: Not specified in the paper. Main demonstrated breakthrough: ImagerySearch consistently outperforms strong video-generation baselines and prior test-time scaling methods on LDT-Bench, and achieves competitive improvements on VBench.","insights":"ðŸ¤” Whatâ€™s next? - Future research: integrate adaptive test-time search with training-time curricula or learnable reward predictors to reduce test-time cost. - Applications: creative content tools, previsualization for film/AR, and assistive tools for designers. Could adaptive, prompt-aware inference become standard for generative video?","keywords":["ImagerySearch","adaptive test-time search","video generation","long-distance semantic prompts","LDT-Bench","VBench","prompt-guided inference","dynamic reward","creative generation"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Ever seen video AIs choke on wildly imaginative prompts? ImagerySearch is a prompt-guided adaptive test-time search that dynamically adjusts inference search space and reward based on prompt semantics â€” enabling more coherent, creative video generation for hard prompts.\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Existing video models fail on imaginative prompts with rarely co-occurring concepts and long-distance se...","analyzed_at":"2025-10-18T09:26:23.539Z","model":"openai/gpt-5-mini"}},{"id":"hf_laser__reinforcement_learning_with_last_token_self_rewarding_1760779460127","title":"LaSeR: Reinforcement Learning with Last-Token Self-Rewarding","abstract":"Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.","authors":[],"published":"2025-10-18","updated":"2025-10-18T09:24:20.127Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.14943","pdf_url":"","scraped_at":"2025-10-18T09:24:20.127Z","abstract_quality":7,"analysis":{"introduction":"ðŸš€ Want faster, self-verifying LLM reasoning?  LaSeR shows a surprising shortcut: the true reasoning reward can be read from a model's last-token next-token log-prob difference.  Result: unify reasoning + verification with only one extra token inference â€” faster and test-time usable.","challenges":"ðŸŽ¯ Problems tackled: - No verification signals available at test time for RLVR. - Prior approaches require sequential solution+verifier generation (two prompts) â€” slow and inefficient. - Hard to jointly train reasoning and self-verification without extra runtime cost.","innovations":"âœ¨ Key ideas: - Theoretical reduction: RL self-verification reward = last-token self-reward score (next-token log-prob diff scaled by KL). - LaSeR algorithm: augment RLVR loss with an MSE loss to align last-token self-reward scores to verifier rewards. - Minimal runtime cost: uses next-token distribution at the final token (one extra token inference).","experiments":"ðŸ“Š Not specified in the paper. Qualitative result: experiments report improved reasoning performance and a strong learned self-rewarding capability, which boosts inference-time scaling performance compared to prior RLVR practice.","insights":"ðŸ¤” What's next? - Research: extend from a single last-token signal to multi-token self-rewarding or combine LaSeR with external verifiers for hybrid verification. - Applications: more efficient on-device LLM reasoning, safer autonomous agents that self-assess answers. Could this enable cheap, reliable test-time verification?","keywords":["RLVR","LaSeR","self-rewarding","last-token","reinforcement learning","LLM verification","MSE alignment","next-token probability"],"category":"machine_learning","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want faster, self-verifying LLM reasoning?  LaSeR shows a surprising shortcut: the true reasoning reward can be read from a model's last-token next-token log-prob difference.  Result: unify reasoning + verification with only one extra token inference â€” faster and test-time usable.\n\n**Challenges:** ðŸŽ¯ Problems tackled: - No verification signals available at test time for RLVR. - Prior approaches require sequentia...","analyzed_at":"2025-10-18T09:26:17.001Z","model":"openai/gpt-5-mini"}},{"id":"hf_information_gain_based_policy_optimization__a_simple_and_effective_approach_for_multi_turn_llm_agents_1760779463323","title":"Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents","abstract":"Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.","authors":[],"published":"2025-10-18","updated":"2025-10-18T09:24:23.323Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.14967","pdf_url":"","scraped_at":"2025-10-18T09:24:23.323Z","abstract_quality":7,"analysis":{"introduction":"ðŸš€ Stuck with sparse final-reward training for LLM agents? IGPO reframes multi-turn interaction as incremental info acquisition: give dense, intrinsic turn-level rewards based on the model's own belief updates to train better multi-turn agents. Helps multi-turn reasoning & efficiency.","challenges":"ðŸŽ¯ Key problems tackled: - Reward sparsity: outcome-only rewards at final answer give little signal. - Advantage collapse: long trajectories make rollouts indistinguishable. - Poor credit assignment: turn-level dependencies vanish in long-horizon tasks.","innovations":"âœ¨ Core ideas: - Introduced Information Gain-based Policy Optimization (IGPO). - Turn-level intrinsic rewards = marginal increase in policy probability of the correct answer (belief updates). - Derived from the model's own belief updates (no external reward model or costly Monte Carlo) and combined with outcome-level rewards for dense trajectories.","experiments":"ðŸ“Š Quantitative highlight: Not specified in the paper. Qualitative result: Experiments (in-domain & out-of-domain) show IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.","insights":"ðŸ¤” What's next? - Research: Analyze theoretical properties of belief-update rewards; extend IGPO to mixed-modality agents or hierarchical planners. - Applications: better search-based assistants, more sample-efficient multi-step tool-using agents. Could IGPO generalize to robotics or multimodal planning?","keywords":["IGPO","information gain","intrinsic rewards","reinforcement learning","LLM agents","multi-turn","credit assignment","sparse rewards","sample efficiency"],"category":"reinforcement_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Stuck with sparse final-reward training for LLM agents? IGPO reframes multi-turn interaction as incremental info acquisition: give dense, intrinsic turn-level rewards based on the model's own belief updates to train better multi-turn agents. Helps multi-turn reasoning & efficiency.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Reward sparsity: outcome-only rewards at final answer give little signal. - Advantage co...","analyzed_at":"2025-10-18T09:26:37.820Z","model":"openai/gpt-5-mini"}},{"id":"hf_bitnet_distillation_1760779467647","title":"BitNet Distillation","abstract":"In this paper, we present BitNet Distillation (BitDistill), a lightweight\npipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into\n1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream\ntasks, achieving strong task-specific performance with minimal computational\ncost. Specifically, BitDistill incorporates three key techniques: the SubLN\nmodule, as introduced in BitNet; multi-head attention distillation, based on\nMiniLM; and continual pre-training, which serves as a crucial warm-up step to\nmitigate the scalability issue of the performance gap between finetuned\nfull-precision and 1.58-bit LLMs on specific tasks. Experimental results show\nthat BitDistill achieves performance comparable to the full-precision\ncounterpart models across model size, while enabling up to 10x memory savings\nand 2.65x faster inference on CPUs. Code is available at\nhttps://github.com/microsoft/BitNet.","authors":[],"published":"2025-10-18","updated":"2025-10-18T09:24:27.647Z","category":"machine-learning","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.13998","pdf_url":"","scraped_at":"2025-10-18T09:24:27.647Z","abstract_quality":6,"analysis":{"introduction":"ðŸš€ Want to run big LLMs with tiny memory? BitDistill fine-tunes off-the-shelf full-precision LLMs into 1.58-bit ternary models (weights = {-1,0,1}) for downstream tasks â€” keeping performance while slashing compute and memory. Great for CPU/edge deployments.","challenges":"ðŸŽ¯ Key problems addressed: - Large memory & compute cost of full-precision LLMs for task-specific use. - Big performance gap when naively quantizing fine-tuned models to very low bits. - Need for a lightweight pipeline to get task-specific ternary models efficiently.","innovations":"âœ¨ Core ideas & whatâ€™s novel: - SubLN module (from BitNet) integrated into fine-tuning. - Multi-head attention distillation (inspired by MiniLM) to transfer task knowledge. - Continual pre-training as a warm-up to close the scalability/performance gap. Novel twist: combining these to produce 1.58-bit ternary LLMs that match full-precision task performance.","experiments":"ðŸ“Š Main empirical highlight: BitDistill yields 1.58-bit (ternary) models that achieve performance comparable to full-precision counterparts across model sizes, while offering up to 10x memory savings and 2.65x faster CPU inference.","insights":"ðŸ¤” Where next? - Research: explore hardware-aware quantization (activations/mixed-precision) and combining BitDistill with pruning or LoRA for even smaller personalized models. - Applications: on-device assistants, low-cost cloud inference for task-specialized LLMs. Could this enable mainstream edge LLMs?","keywords":["quantization","ternary","1.58-bit","LLM","BitDistill","SubLN","multi-head attention distillation","continual pre-training","model_compression","inference_efficiency"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want to run big LLMs with tiny memory? BitDistill fine-tunes off-the-shelf full-precision LLMs into 1.58-bit ternary models (weights = {-1,0,1}) for downstream tasks â€” keeping performance while slashing compute and memory. Great for CPU/edge deployments.\n\n**Challenges:** ðŸŽ¯ Key problems addressed: - Large memory & compute cost of full-precision LLMs for task-specific use. - Big performance gap when naively quant...","analyzed_at":"2025-10-18T09:26:40.600Z","model":"openai/gpt-5-mini"}},{"id":"hf_paddleocr_vl__boosting_multilingual_document_parsing_via_a_0_9b_ultra_compact_vision_language_model_1760779471414","title":"PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model","abstract":"In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios.","authors":[],"published":"2025-10-18","updated":"2025-10-18T09:24:31.414Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.14528","pdf_url":"","scraped_at":"2025-10-18T09:24:31.414Z","abstract_quality":7,"analysis":{"introduction":"ðŸš€ Ever wanted a powerful document parser that runs cheaply and supports dozens of languages?  PaddleOCR-VL introduces a 0.9B ultra-compact vision-language model combining a NaViT-style dynamic-resolution visual encoder with ERNIE-4.5-0.3B to enable SOTA multilingual document parsing for 109 languages â€” fast and resource-efficient.","challenges":"ðŸŽ¯ Problems tackled: - Large VLMs consume heavy compute and memory, hindering deployment on constrained hardware. - Multilingual document parsing across many scripts/languages is hard to scale. - Reliable recognition of complex elements (tables, formulas, charts) remains challenging.","innovations":"âœ¨ Core contributions: - PaddleOCR-VL-0.9B: an ultra-compact (0.9B) vision-language model. - NaViT-style dynamic-resolution visual encoder to handle variable image detail efficiently. - Integration with ERNIE-4.5-0.3B language model for robust multimodal understanding. - Supports 109 languages while keeping minimal resource consumption. Novelty: combining dynamic-resolution visual encoding with a small LLM to reach SOTA document parsing with low compute.","experiments":"ðŸ“Š Results & proof: The paper reports SOTA performance on widely used public benchmarks and internal benchmarks for both page-level document parsing and element-level recognition, significantly outperforming existing solutions and matching competitiveness with top-tier VLMs while running fast and resource-efficiently. Exact numeric gains Not specified in the paper.","insights":"ðŸ¤” What's next (potential ideas): - Explore on-device/edge deployment and quantization strategies tailored to the NaViT + ERNIE stack. - Investigate few-shot or continual learning to adapt the compact VLM to new document layouts or low-resource languages. Potential broader impact: real-time form processing, mobile document assistants, automated compliance/audit pipelines. Could this enable universal, low-cost document understanding everywhere?","keywords":["PaddleOCR-VL","vision-language","NaViT","ERNIE-4.5-0.3B","document parsing","multilingual OCR","compact models","0.9B","109 languages"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Ever wanted a powerful document parser that runs cheaply and supports dozens of languages?  PaddleOCR-VL introduces a 0.9B ultra-compact vision-language model combining a NaViT-style dynamic-resolution visual encoder with ERNIE-4.5-0.3B to enable SOTA multilingual document parsing for 109 languages â€” fast and resource-efficient.\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Large VLMs consume heavy compute and memory,...","analyzed_at":"2025-10-18T09:26:40.703Z","model":"openai/gpt-5-mini"}}]