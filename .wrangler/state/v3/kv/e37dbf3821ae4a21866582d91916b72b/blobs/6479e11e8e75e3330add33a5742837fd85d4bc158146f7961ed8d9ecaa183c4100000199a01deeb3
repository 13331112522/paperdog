{"date":"2025-10-01","papers":[{"id":"hf_oceangym__a_benchmark_environment_for_underwater_embodied_agents_1759311082964","title":"OceanGym: A Benchmark Environment for Underwater Embodied Agents","authors":[],"abstract":"We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:41:27.213Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;620b3bbb0668e435407c8d0a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;,&quot;fullname&quot;:&quot;Ningyu Zhang&quot;,&quot;name&quot;:&quot;Ningyu&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:30}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.930759847164154},&quot;editors&quot;:[&quot;Ningyu&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.26536&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af3&quot;,&quot;name&quot;:&quot;Yida Xue&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af4&quot;,&quot;name&quot;:&quot;Mingjun Mao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af5&quot;,&quot;name&quot;:&quot;Xiangyuan Ru&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af6&quot;,&quot;name&quot;:&quot;Yuqi Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af7&quot;,&quot;name&quot;:&quot;Baochang Ren&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af8&quot;,&quot;name&quot;:&quot;Shuofei Qiao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af9&quot;,&quot;name&quot;:&quot;Mengru Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afa&quot;,&quot;name&quot;:&quot;Shumin Deng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afb&quot;,&quot;name&quot;:&quot;Xinyu An&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afc&quot;,&quot;name&quot;:&quot;Ningyu Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afd&quot;,&quot;name&quot;:&quot;Ying Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afe&quot;,&quot;name&quot;:&quot;Huajun Chen&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/1Nye1TFWd66krDhvPXjQ1.png&quot;],&quot;publishedAt&quot;:&quot;2025-09-30T17:09:32.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T01:11:27.179Z&quot;,&quot;title&quot;:&quot;OceanGym: A Benchmark Environment for Underwater Embodied Agents&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;620b3bbb0668e435407c8d0a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Ningyu Zhang&quot;,&quot;user&quot;:&quot;Ningyu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We introduce OceanGym, the first comprehensive benchmark for ocean underwater\\nembodied agents, designed to advance AI in one of the most demanding real-world\\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\\nextreme perceptual and decision-making challenges, including low visibility,\\ndynamic ocean currents, making effective agent deployment exceptionally\\ndifficult. OceanGym encompasses eight realistic task domains and a unified\\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\\nintegrates perception, memory, and sequential decision-making. Agents are\\nrequired to comprehend optical and sonar data, autonomously explore complex\\nenvironments, and accomplish long-horizon objectives under these harsh\\nconditions. Extensive experiments reveal substantial gaps between\\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\\npersistent difficulty of perception, planning, and adaptability in ocean\\nunderwater environments. By providing a high-fidelity, rigorously designed\\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\\ntransferring these capabilities to real-world autonomous ocean underwater\\nvehicles, marking a decisive step toward intelligent agents capable of\\noperating in one of Earth's last unexplored frontiers. The code and data are\\navailable at https://github.com/OceanGPT/OceanGym.&quot;,&quot;upvotes&quot;:25,&quot;discussionId&quot;:&quot;68dc947d4159d1f2418f9aff&quot;,&quot;projectPage&quot;:&quot;https://oceangpt.github.io/OceanGym/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/OceanGPT/OceanGym&quot;,&quot;ai_summary&quot;:&quot;OceanGym is a benchmark for underwater embodied agents using Multi-modal Large Language Models to address challenges in perception, planning, and adaptability in harsh ocean environments.&quot;,&quot;ai_keywords&quot;:[&quot;Multi-modal Large Language Models&quot;,&quot;MLLMs&quot;,&quot;optical data&quot;,&quot;sonar data&quot;,&quot;sequential decision-making&quot;,&quot;embodied AI&quot;,&quot;autonomous ocean underwater vehicles&quot;],&quot;githubStars&quot;:25},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;620b3bbb0668e435407c8d0a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Ningyu Zhang&quot;,&quot;user&quot;:&quot;Ningyu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67f371b807a3da4558f803c1&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GilM_Vf65qvwuW5-uj9aG.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Twain Wu&quot;,&quot;user&quot;:&quot;1wtw1&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64db65ee1d19239f50674cbe&quot;,&quot;avatarUrl&quot;:&quot;/avatars/dd96cec8d0c10f52a89b25d65728738d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;xueyida&quot;,&quot;user&quot;:&quot;xyd123&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68d8fc00ff474874c83a1c99&quot;,&quot;avatarUrl&quot;:&quot;/avatars/17e3a2f5197274536bf68d949c5416db.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;huminclu&quot;,&quot;user&quot;:&quot;huminclu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;658ead753ce574ff3c339a64&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1dd7671e8af5e7241ef47a6de5503c53.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;sunnychenxiwang&quot;,&quot;user&quot;:&quot;sunnychenxiwang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6698c1c3157ceb76c48ff996&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2f1d732c4d9df4f5b554268ee1949dda.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;徐步强&quot;,&quot;user&quot;:&quot;Xubqpanda&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6736aebbbbc5d5471ee57218&quot;,&quot;avatarUrl&quot;:&quot;/avatars/45e3b017fc6a07e10a42b81cfa349b3f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yi Zhong&quot;,&quot;user&quot;:&quot;HongdouNI233&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;646449beeca41ed5029d1630&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7992d9a62e8d218ec3200d74af9ab5c4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xiang Chen&quot;,&quot;user&quot;:&quot;yyfenglin&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;684bc1be17ae31ba66171292&quot;,&quot;avatarUrl&quot;:&quot;/avatars/99ea28d4ed2ef6c4e35fd26c64472e49.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jingsheng Zheng&quot;,&quot;user&quot;:&quot;JohnsonZheng03&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68415d7b911d1b3135fcca88&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HE3ptFNTlvoWmG3p3f2Cs.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Qihailiantang&quot;,&quot;user&quot;:&quot;Qihailiantang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;679a01a99893a68681ef1847&quot;,&quot;avatarUrl&quot;:&quot;/avatars/17fe173acda467df2b90cca9e5f3c656.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;ye&quot;,&quot;user&quot;:&quot;haohaojun&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65cad52fd6c974694fc20b8e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/8232a7c5db590ed26751a47c45d481b8.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xinle Deng&quot;,&quot;user&quot;:&quot;Linear-Matrix-Probability&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:3,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6345aadf5efccdc07f1365a5&quot;,&quot;name&quot;:&quot;ZhejiangUniversity&quot;,&quot;fullname&quot;:&quot;Zhejiang University&quot;}}\"> Papers arxiv:2509.26536","published":"2025-10-01","source":"huggingface","url":"https://huggingface.co/papers/2509.26536","analysis":{"introduction":"🚀 What if autonomous agents could explore the deep sea like rovers explore Mars?\nOceanGym is the first comprehensive benchmark for underwater embodied agents — 8 realistic task domains + a unified MLLM-driven agent framework. Designed to push robust AI for AUVs. https://github.com/OceanGPT/OceanGym","challenges":"🎯 Key challenges OceanGym tackles:\n- Low visibility & noisy sensors that hinder perception\n- Dynamic ocean currents complicating decision-making and control\n- Multimodal (optical + sonar) long‑horizon tasks requiring memory and planning","innovations":"✨ What OceanGym introduces:\n- A high‑fidelity benchmark with 8 realistic underwater task domains\n- A unified, MLLM-driven agent framework integrating perception, memory, sequential decision-making\n- Tasks demanding optical+sonar fusion and long‑horizon autonomous exploration\nNovelty: first comprehensive underwater embodied benchmark.","experiments":"📊 Quantitative result: Not specified in the paper.\nQualitative proof: Extensive experiments reveal substantial gaps between state‑of‑the‑art MLLM-driven agents and human experts, showing persistent difficulty in perception, planning, and adaptability. Code/data: github.com/OceanGPT/OceanGym","insights":"🤔 What's next?\n- Improve sim‑to‑real transfer and dedicated sonar–vision fusion models\n- Explore curriculum/hierarchical planning for long‑horizon underwater tasks\nApplications: autonomous oceanographic surveys, search & rescue, infrastructure inspection. Could OceanGym speed real-world AUV deployment?","keywords":["OceanGym","Multi-modal Large Language Models","MLLMs","underwater robotics","sonar","optical imaging","embodied AI","long-horizon planning","benchmark"],"category":"robotics","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"translation\": \"🚀 如果自主智能体能像漫游车探索火星一样探索深海，那会怎样？OceanGym是首个针对水下具身智能体的综合基准——它包含8个现实任务领域和一个统一的MLLM驱动的智能体框架。旨在推动自主水下航行器（AUVs）的鲁棒（稳健）AI发展。https://github.com/OceanGPT/OceanGym\"\n}","chinese_challenges":"\"OceanGym 应对的关键挑战：\\n- 低能见度与传感器噪声，严重阻碍了环境感知。\\n- 动态洋流使决策和控制过程变得复杂。\\n- 需要记忆和规划的多模态（光学 + 声呐）长程任务。\"","chinese_innovations":"{\n  \"innovations\": [\n    {\n      \"title\": \"OceanGym 引入了什么：\",\n      \"points\": [\n        \"一个高保真基准，包含 8 个逼真的水下任务领域\",\n        \"一个统一的、由 MLLM（多模态大语言模型）驱动的智能体框架，集成了感知、记忆和序列决策能力\",\n        \"要求光学与声纳融合以及长周期自主探索的任务\"\n      ],\n      \"novelty\": \"新颖性：首个全面的水下具身基准。\"\n    }\n  ]\n}","chinese_experiments":"{\n  \"定量结果\": \"论文中未明确说明。\",\n  \"定性证据\": \"广泛的实验揭示了最先进的 MLLM 驱动智能体与人类专家之间存在显著差距，表明其在感知、规划和适应性方面仍面临持续的困难。\",\n  \"代码/数据\": \"github.com/OceanGPT/OceanGym\"\n}","chinese_insights":"{\n  \"insights\": {\n    \"next_steps\": [\n      \"Improve sim-to-real transfer and dedicated sonar–vision fusion models\",\n      \"Explore curriculum/hierarchical planning for long-horizon underwater tasks\"\n    ],\n    \"applications\": [\n      \"autonomous oceanographic surveys\",\n      \"search & rescue\",\n      \"infrastructure inspection\"\n    ],\n    \"question\": \"Could OceanGym speed real-world AUV deployment?\"\n  }\n}","summary":"**Introduction:** 🚀 What if autonomous agents could explore the deep sea like rovers explore Mars?\nOceanGym is the first comprehensive benchmark for underwater embodied agents — 8 realistic task domains + a unified MLLM-driven agent framework. Designed to push robust AI for AUVs. https://github.com/OceanGPT/OceanGym\n\n**Challenges:** 🎯 Key challenges OceanGym tackles:\n- Low visibility & noisy sensors that hinder perception\n- Dynami...","analyzed_at":"2025-10-01T11:19:11.261Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:22.964Z","views":0,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"hf_oceangym__a_benchmark_environment_for_underwater_embodied_agents_1759311082964","views_at_archive":0}},{"id":"hf_thinking_sparks___emergent_attention_heads_in_reasoning_models_during_post_training_1759311093567","title":"Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training","authors":[],"abstract":"Modern large reasoning models boost performance via post-training methods like supervised fine-tuning and reinforcement learning—but how these gains arise internally has remained a mystery.In our new work, we peel back the hood using circuit analysis to reveal:\\n(1) Post-training triggers the emergence of specialized attention heads that coordinate to carry out structured reasoning.(2) Different training regimes steer different dynamics: SFT/distillation yield stable, cumulative reasoning heads, while policy optimization leads to iterative activation and pruning.(3) Strong reasoning heads boost advanced problem solving—but risk “overthinking” errors on simpler tasks, revealing a tension between complexity and reliability.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T01:57:19.002Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;64587be872b60ae7a3817858&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;,&quot;fullname&quot;:&quot;Minbyul Jeong&quot;,&quot;name&quot;:&quot;Minbyul&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3}},&quot;numEdits&quot;:1,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9207460284233093},&quot;editors&quot;:[&quot;Minbyul&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25758&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc8a1f4159d1f2418f9a64&quot;,&quot;name&quot;:&quot;Yein Park&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8a1f4159d1f2418f9a65&quot;,&quot;name&quot;:&quot;Minbyul Jeong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8a1f4159d1f2418f9a66&quot;,&quot;name&quot;:&quot;Jaewoo Kang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-30T04:23:43.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:26:22.625Z&quot;,&quot;title&quot;:&quot;Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\\n Post Training&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64587be872b60ae7a3817858&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Minbyul Jeong&quot;,&quot;user&quot;:&quot;Minbyul&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;The remarkable capabilities of modern large reasoning models are largely\\nunlocked through post-training techniques such as supervised fine-tuning and\\nreinforcement learning. However, the architectural mechanisms behind such\\nimprovements remain largely opaque. In this work, we use circuit analysis to\\ndemonstrate that post-training for complex reasoning sparks the emergence of\\nnovel, functionally specialized attention heads. These heads collectively\\nsupport structured reasoning and computation. Our comparative analysis across\\nQwen families and DeepSeek-distilled model reveals that these emergent heads\\nevolve differently under different training regimes. Distillation and SFT\\nfoster a cumulative addition of stable reasoning heads. In contrast, group\\nrelative policy optimization operates in a dynamic search mode: relatively few\\nattention heads are iteratively activated, evaluated, and pruned, with their\\nsurvival closely tracking fluctuations in the task reward signal. Furthermore,\\nwe find that controllable think on/off models do not possess dedicated thinking\\nheads. Instead, turning off explicit reasoning triggers a broader-but less\\nefficient-set of compensatory heads. Through ablation and qualitative analyses,\\nwe connect these circuit-level dynamics to a crucial performance trade-off:\\nstrengthened heads enable sophisticated problem-solving strategies for\\ndifficult problems but can also introduce over-thinking failure modes, such as\\ncalculation errors or logical loops on simpler tasks. These findings connect\\ncircuit-level dynamics to macro-level performance, identifying an inherent\\ntension where complex reasoning comes at the cost of elementary computations.\\nMore broadly, our work points to future directions for training policy design,\\nemphasizing the need to balance the development of effective reasoning\\nstrategies with the assurance of reliable, flawless execution.&quot;,&quot;upvotes&quot;:16,&quot;discussionId&quot;:&quot;68dc8a1f4159d1f2418f9a67&quot;,&quot;ai_summary&quot;:&quot;Post-training techniques like supervised fine-tuning and reinforcement learning lead to the emergence of specialized attention heads that support structured reasoning, with different training regimes affecting their evolution and performance.&quot;,&quot;ai_keywords&quot;:[&quot;supervised fine-tuning&quot;,&quot;reinforcement learning&quot;,&quot;circuit analysis&quot;,&quot;attention heads&quot;,&quot;structured reasoning&quot;,&quot;Qwen families&quot;,&quot;DeepSeek-distilled model&quot;,&quot;group relative policy optimization&quot;,&quot;think on/off models&quot;,&quot;ablation analysis&quot;,&quot;qualitative analysis&quot;,&quot;over-thinking failure modes&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;64587be872b60ae7a3817858&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Minbyul Jeong&quot;,&quot;user&quot;:&quot;Minbyul&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64e5c8e594aa0690321f6b29&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IW5LRzmPcAM-dri8taMN7.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yein Park&quot;,&quot;user&quot;:&quot;P-YI&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67d790ece3396bf0c9298194&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e027b2cc0bc9ffe5df18e61ca460d422.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;JongMyung Jung&quot;,&quot;user&quot;:&quot;hiwaryi&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;670f5c3f642f58673b1f435a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wa_TulGokgVzMN0-_GlBB.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;YewonCho&quot;,&quot;user&quot;:&quot;doldol330&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;60f8435644e75317cc02ed51&quot;,&quot;avatarUrl&quot;:&quot;/avatars/68b7fc077fe2bda6607b1c470add8140.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jungwoo Park&quot;,&quot;user&quot;:&quot;affjljoo3581&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64bb1bb412d00c4589c03bf7&quot;,&quot;avatarUrl&quot;:&quot;/avatars/63d04790c71c72e824cfcf70fc9433e6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hyeongsoon&quot;,&quot;user&quot;:&quot;hhs8746&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;5efbdc4ac3896117eab961a9&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1602668910270-5efbdc4ac3896117eab961a9.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Data Mining and Information Systems Lab&quot;,&quot;user&quot;:&quot;dmis-lab&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66fd2cf65e36a0ed66f32f68&quot;,&quot;avatarUrl&quot;:&quot;/avatars/781c4bdd887f4137058eca18203dc7d5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;monet&quot;,&quot;user&quot;:&quot;monet9736&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67348f009551fdc242064ef4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/38023d6d3c2ea12434ed55aca7ca1c3e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jueon Park&quot;,&quot;user&quot;:&quot;bioai96&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68dc97a4224de59d4b965edd&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ktZW8Hf-nvB2eSBB8GgrG.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yewon Cho&quot;,&quot;user&quot;:&quot;YewonCho&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65431e2dbf8a6039fbddb4c6&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7df981f6f9bda5f8af89b6f0637340f6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;lim suhyeon&quot;,&quot;user&quot;:&quot;yeonsue&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6306df0ed37ce67e0e53e3f1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/3cbe5762d1e1ccf259f4bbed9fc1fa00.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hyeon Hwang&quot;,&quot;user&quot;:&quot;Hyeoni&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6621bc39e774284ec1742ab8&quot;,&quot;name&quot;:&quot;KoreaUniversity&quot;,&quot;fullname&quot;:&quot;Korea University&quot;}}\"> Papers arxiv:2509.25758","published":"2025-10-01","source":"huggingface","url":"https://huggingface.co/papers/2509.25758","analysis":{"introduction":"🚀 What if post-training doesn't just tweak models but sparks \\","challenges":"🎯 Problems addressed:\n- The internal mechanisms by which post-training (SFT/RL/distillation) improves reasoning are opaque.\n- Different post-training regimes and their circuit-level effects are not well understood.\n- Strong reasoning can harm reliability (\\","innovations":"✨ Key methods & novelty:\n- Applied circuit analysis to transformer attention to identify specialized \\","experiments":"📊 Experiments & main proof:\n- Main demonstration: post-training triggers emergence of specialized attention heads that coordinate structured reasoning; SFT/distillation add stable heads while policy optimization iteratively activates and prunes them; disabling explicit thinking leads to broader, less efficient compensatory heads; stronger heads improve hard tasks but can cause overthinking.\n- Quantitative numbers (e.g., %improvement): Not specified in the paper.","insights":"🤔 What's next?:\n- Research directions: explore adaptive head gating or regularization that preserves complex reasoning while preventing overthinking; design training objectives that balance reward-driven search with stability of useful heads.\n- Broader applications: safer, controllable reasoning agents; modular model designs that switch \\","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"translation\": \"🚀 试想一下，如果后训练（post-training）不仅仅是微调模型，而是能激发\"\n}","chinese_challenges":"{\n  \"challenges\": \"🎯 解决的问题：\\n- 训练后（SFT/RL/蒸馏）改进推理能力的内部机制不透明。\\n- 不同的训练后方案及其在电路层面的影响尚不完全清楚。\\n- 强大的推理能力可能会损害可靠性（\"\n}","chinese_innovations":"\"✨ 关键方法与创新点：应用电路分析（Circuit Analysis）技术对Transformer注意力机制进行深入剖析，旨在识别专门化的...\"","chinese_experiments":"{\n  \"title\": \"实验与主要证明\",\n  \"sections\": [\n    {\n      \"heading\": \"主要论证\",\n      \"content\": \"训练后（post-training）会触发专业化注意力头（specialized attention heads）的出现，这些注意力头协同进行结构化推理；SFT（监督微调）/蒸馏（distillation）增加了稳定的注意力头，而策略优化（policy optimization）则迭代地激活和修剪这些注意力头；禁用显式思考（explicit thinking）会导致出现更广泛、效率较低的补偿性注意力头（compensatory heads）；更强的注意力头能改进困难任务的表现，但也可能导致过度思考（overthinking）。\"\n    },\n    {\n      \"heading\": \"定量数据（例如，改进百分比）\",\n      \"content\": \"论文中未具体说明。\"\n    }\n  ]\n}","chinese_insights":"{\n  \"analysis_type\": \"Research Insights Translation\",\n  \"source_language\": \"English\",\n  \"target_language\": \"Simplified Chinese\",\n  \"translation\": {\n    \"title\": \"🤔 未来方向/下一步研究：\",\n    \"sections\": [\n      {\n        \"header\": \"研究方向：\",\n        \"points\": [\n          \"探索自适应头部门控（head gating）或正则化方法，既能保留复杂的推理能力，又能防止模型“过度思考”（overthinking）；\",\n          \"设计训练目标，以平衡奖励驱动的搜索过程与有效头部（模块）的稳定性。\"\n        ]\n      },\n      {\n        \"header\": \"更广泛的应用：\",\n        \"points\": [\n          \"更安全、可控的推理智能体；\",\n          \"可切换的模块化模型设计。\"\n        ]\n      }\n    ]\n  }\n}","summary":"**Introduction:** 🚀 What if post-training doesn't just tweak models but sparks \\\n\n**Challenges:** 🎯 Problems addressed:\n- The internal mechanisms by which post-training (SFT/RL/distillation) improves reasoning are opaque.\n- Different post-training regimes and their circuit-level effects are not well understood.\n- Strong reasoning can harm reliability (\\","analyzed_at":"2025-10-01T13:20:45.191Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:33.567Z","views":0,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"hf_thinking_sparks___emergent_attention_heads_in_reasoning_models_during_post_training_1759311093567","views_at_archive":0}},{"id":"hf_dparallel__learnable_parallel_decoding_for_dllms_1759311102154","title":"dParallel: Learnable Parallel Decoding for dLLMs","authors":[],"abstract":"We present dParallel, a novel method that unlocks the inherent parallelism of dLLMs for fast sampling. Our paper, code, models, and dataset are all available now!\\nCode: https://github.com/czg1225/dParallelPaper: https://arxiv.org/pdf/2509.26488Model: https://huggingface.co/Zigeng/dParallel-LLaDA-8B-instructData: https://huggingface.co/datasets/Zigeng/dParallel_LLaDA_Distill_Data\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:04:29.960Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;65811eeaa2284a018e51f1ba&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;,&quot;fullname&quot;:&quot;Zigeng Chen&quot;,&quot;name&quot;:&quot;Zigeng&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:7}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7858399152755737},&quot;editors&quot;:[&quot;Zigeng&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.26488&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a47&quot;,&quot;name&quot;:&quot;Zigeng Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a48&quot;,&quot;name&quot;:&quot;Gongfan Fang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a49&quot;,&quot;name&quot;:&quot;Xinyin Ma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a4a&quot;,&quot;name&quot;:&quot;Ruonan Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a4b&quot;,&quot;name&quot;:&quot;Xinchao Wang&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/65811eeaa2284a018e51f1ba/0NK-gFnfpLThLyS0atsLH.mp4&quot;],&quot;publishedAt&quot;:&quot;2025-09-30T16:32:52.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:34:29.943Z&quot;,&quot;title&quot;:&quot;dParallel: Learnable Parallel Decoding for dLLMs&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;65811eeaa2284a018e51f1ba&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zigeng Chen&quot;,&quot;user&quot;:&quot;Zigeng&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Diffusion large language models (dLLMs) have recently drawn considerable\\nattention within the research community as a promising alternative to\\nautoregressive generation, offering parallel token prediction and lower\\ninference latency. Yet, their parallel decoding potential remains largely\\nunderexplored, as existing open-source models still require nearly token-length\\ndecoding steps to ensure performance. To address this, we introduce dParallel,\\na simple and effective method that unlocks the inherent parallelism of dLLMs\\nfor fast sampling. We identify that the key bottleneck to parallel decoding\\narises from the sequential certainty convergence for masked tokens. Building on\\nthis insight, we introduce the core of our approach: certainty-forcing\\ndistillation, a novel training strategy that distills the model to follow its\\noriginal sampling trajectories while enforcing it to achieve high certainty on\\nmasked tokens more rapidly and in parallel. Extensive experiments across\\nvarious benchmarks demonstrate that our method can dramatically reduce the\\nnumber of decoding steps while maintaining performance. When applied to the\\nLLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on\\nGSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP\\nbenchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup\\nwhile maintaining accuracy. Our code is available at\\nhttps://github.com/czg1225/dParallel&quot;,&quot;upvotes&quot;:13,&quot;discussionId&quot;:&quot;68dc88d74159d1f2418f9a4c&quot;,&quot;githubRepo&quot;:&quot;https://github.com/czg1225/dParallel&quot;,&quot;ai_summary&quot;:&quot;dParallel is a method that enhances the parallel decoding of diffusion large language models, significantly reducing decoding steps without compromising performance.&quot;,&quot;ai_keywords&quot;:[&quot;diffusion large language models&quot;,&quot;dLLMs&quot;,&quot;autoregressive generation&quot;,&quot;parallel token prediction&quot;,&quot;parallel decoding&quot;,&quot;masked tokens&quot;,&quot;certainty-forcing distillation&quot;,&quot;LLaDA-8B-Instruct&quot;,&quot;GSM8K&quot;,&quot;MBPP benchmark&quot;],&quot;githubStars&quot;:7},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;65811eeaa2284a018e51f1ba&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zigeng Chen&quot;,&quot;user&quot;:&quot;Zigeng&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;640ebdfefdeaae139086f4d8&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/640ebdfefdeaae139086f4d8/2N94gbHubplYD8njmUTPf.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Yuanshi&quot;,&quot;user&quot;:&quot;Yuanshi&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6683fc5344a65be1aab25dc0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e13cde3f87b59e418838d702807df3b5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;hjkim&quot;,&quot;user&quot;:&quot;hojie11&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6684b1a9986286e214df1e03&quot;,&quot;avatarUrl&quot;:&quot;/avatars/515efb62b0ec923ea525a90ea7aa9221.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;XinyinMa&quot;,&quot;user&quot;:&quot;XinyinHorseee&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6486fb33570a419f41a882e4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/860a42074439a23c629cd23851ae4da6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ruonan Yu&quot;,&quot;user&quot;:&quot;roseannelexie&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6342796a0875f2c99cfd313b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/98575092404c4197b20c929a6499a015.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuseung \\&quot;Phillip\\&quot; Lee&quot;,&quot;user&quot;:&quot;phillipinseoul&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;668e740f1173ab43d9d9ed5e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/caa9b47c2a5f6d6d679759b8b234a0ab.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zeqing Wang&quot;,&quot;user&quot;:&quot;INV-WZQ&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;620783f24e28382272337ba4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/620783f24e28382272337ba4/zkUveQPNiDfYjgGhuFErj.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;GuoLiangTang&quot;,&quot;user&quot;:&quot;Tommy930&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;634cfebc350bcee9bed20a4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xingyi Yang&quot;,&quot;user&quot;:&quot;adamdad&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6270324ebecab9e2dcf245de&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6270324ebecab9e2dcf245de/cMbtWSasyNlYc9hvsEEzt.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kye Gomez&quot;,&quot;user&quot;:&quot;kye&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;646a1939c37ca1e12308fe81&quot;,&quot;avatarUrl&quot;:&quot;/avatars/752e9d86018e7d33ad8bcd741203fd86.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Gongfan Fang&quot;,&quot;user&quot;:&quot;Vinnnf&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6508ab2b349930913196378b&quot;,&quot;name&quot;:&quot;NationalUniversityofSingapore&quot;,&quot;fullname&quot;:&quot;National University of Singapore&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png&quot;}}\"> Papers arxiv:2509.26488","published":"2025-10-01","source":"huggingface","url":"https://huggingface.co/papers/2509.26488","analysis":{"introduction":"🚀 What if diffusion LLMs could sample 8–10x faster? \ndParallel unlocks the parallel decoding potential of dLLMs by using a learnable distillation that forces rapid certainty on masked tokens — slashing decoding steps and cutting latency for real-time and research use. 🔥","challenges":"🎯 Key problems tackled:\n- dLLMs still need nearly token-length decoding steps, limiting speed.\n- Sequential certainty convergence of masked tokens blocks true parallel decoding.\n- Hard to reduce steps without hurting accuracy or changing sampling trajectories.","innovations":"✨ Core ideas:\n- dParallel: a method to enable fast parallel sampling in dLLMs.\n- Certainty-forcing distillation: distill models to follow original sampling trajectories while forcing high certainty on masked tokens faster and in parallel.\n- Simple, training-based fix that unlocks inherent dLLM parallelism.","experiments":"📊 Standout result: Applied to LLaDA-8B-Instruct, dParallel reduces decoding steps on GSM8K from 256 → 30 (≈8.5× speedup) while maintaining performance. \n(Also: MBPP 256 → 24 ≈10.5× speedup with accuracy preserved.)","insights":"🤔 Where this could go next:\n- Explore scaling to larger dLLMs and multimodal diffusion LMs to bring low-latency sampling to more tasks.\n- Combine with hardware-aware scheduling / quantization for on-device, interactive LMs.\nCould this make diffusion-based assistants practical in real time? 🚀","category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"翻译失败，请查看英文原文 / Translation failed, please see English original","chinese_challenges":"{\n  \"challenges\": \"🎯 解决的关键问题：\\n- dLLMs（解码器大语言模型）仍然需要接近于 token 长度的解码步骤，限制了速度。\\n- 掩码 token 的顺序确定性收敛阻碍了真正的并行解码。\\n- 很难在不损害准确性或改变采样轨迹的情况下减少解码步骤。\"\n}","chinese_innovations":"{\n  \"核心思想\": [\n    \"dParallel：一种在 dLLM 中实现快速并行采样的方法。\",\n    \"强制确定性蒸馏：蒸馏模型以遵循原始采样轨迹，同时更快、更并行地对被掩码的 token 强制施加高确定性。\",\n    \"一种简单的、基于训练的修正方法，能够释放 dLLM 固有的并行性。\"\n  ]\n}","chinese_experiments":"[\n  {\n    \"title\": \"实验结果分析\",\n    \"content\": \"📊 突出成果：将 dParallel 应用于 LLaDA-8B-Instruct 模型，在 GSM8K 数据集上，解码步骤从 256 减少到 30（约 8.5 倍加速），同时保持了性能。 \\n（此外：在 MBPP 数据集上，解码步骤从 256 减少到 24，实现了约 10.5 倍加速，且准确率得以保持。）\"\n  }\n]","chinese_insights":"{\n  \"translation\": \"🤔 接下来的发展方向：\\n- 探索扩展到更大的dLLMs（扩散式大型语言模型）和多模态扩散式语言模型，将低延迟采样带到更多任务中。\\n- 与硬件感知调度/量化相结合，实现设备上的交互式语言模型。\\n这能否使基于扩散的助手在实时应用中变得实用？🚀\"\n}","summary":"**Introduction:** 🚀 What if diffusion LLMs could sample 8–10x faster? \ndParallel unlocks the parallel decoding potential of dLLMs by using a learnable distillation that forces rapid certainty on masked tokens — slashing decoding steps and cutting latency for real-time and research use. 🔥\n\n**Challenges:** 🎯 Key problems tackled:\n- dLLMs still need nearly token-length decoding steps, limiting speed.\n- Sequential certainty convergen...","analyzed_at":"2025-10-01T14:12:20.197Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:42.154Z","views":0,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"hf_dparallel__learnable_parallel_decoding_for_dllms_1759311102154","views_at_archive":0}},{"id":"hf_dc_videogen__efficient_video_generation_with_deep_compression_video_autoencoder_1759311090899","title":"DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder","authors":[],"abstract":"DC-VideoGen is a new post-training framework for accelerating video diffusion models. Key features:🎬 Supports video generation up to 2160×3840 resolution on a single H100 GPU⚡ Delivers 14.8× faster inference than the base model💰 230× lower training cost compared to training from scratch (only 10 H100 GPU days for Wan-2.1-14B)\\nDC-VideoGen is built on two core innovations:\\n\\nDeep Compression Video Autoencoder (DC-AE-V): a new family of deep compression autoencoders for video data, providing 32×/64× spatial and 4× temporal compression.\\nAE-Adapt-V: a robust adaptation strategy that enables rapid and stable transfer of pre-trained video diffusion models to DC-AE-V.\\n\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T01:51:53.102Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;650e2b14c945dfc9386a7e28&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;,&quot;fullname&quot;:&quot;Han Cai&quot;,&quot;name&quot;:&quot;han-cai&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:11}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7540821433067322},&quot;editors&quot;:[&quot;han-cai&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25182&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a36&quot;,&quot;name&quot;:&quot;Junyu Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a37&quot;,&quot;name&quot;:&quot;Wenkun He&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a38&quot;,&quot;name&quot;:&quot;Yuchao Gu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a39&quot;,&quot;name&quot;:&quot;Yuyang Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3a&quot;,&quot;name&quot;:&quot;Jincheng Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3b&quot;,&quot;name&quot;:&quot;Junsong Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3c&quot;,&quot;name&quot;:&quot;Dongyun Zou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3d&quot;,&quot;name&quot;:&quot;Yujun Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3e&quot;,&quot;name&quot;:&quot;Zhekai Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3f&quot;,&quot;name&quot;:&quot;Muyang Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a40&quot;,&quot;name&quot;:&quot;Haocheng Xi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a41&quot;,&quot;name&quot;:&quot;Ligeng Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a42&quot;,&quot;name&quot;:&quot;Enze Xie&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a43&quot;,&quot;name&quot;:&quot;Song Han&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a44&quot;,&quot;name&quot;:&quot;Han Cai&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/650e2b14c945dfc9386a7e28/bgdM9-uqshQ2p3QyNbj82.jpeg&quot;],&quot;publishedAt&quot;:&quot;2025-09-29T17:59:31.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:21:53.066Z&quot;,&quot;title&quot;:&quot;DC-VideoGen: Efficient Video Generation with Deep Compression Video\\n Autoencoder&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;650e2b14c945dfc9386a7e28&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Han Cai&quot;,&quot;user&quot;:&quot;han-cai&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We introduce DC-VideoGen, a post-training acceleration framework for\\nefficient video generation. DC-VideoGen can be applied to any pre-trained video\\ndiffusion model, improving efficiency by adapting it to a deep compression\\nlatent space with lightweight fine-tuning. The framework builds on two key\\ninnovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal\\ntemporal design that achieves 32x/64x spatial and 4x temporal compression while\\npreserving reconstruction quality and generalization to longer videos; and (ii)\\nAE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer\\nof pre-trained models into the new latent space. Adapting the pre-trained\\nWan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100\\nGPU. The accelerated models achieve up to 14.8x lower inference latency than\\ntheir base counterparts without compromising quality, and further enable\\n2160x3840 video generation on a single GPU. Code:\\nhttps://github.com/dc-ai-projects/DC-VideoGen.&quot;,&quot;upvotes&quot;:20,&quot;discussionId&quot;:&quot;68dc88d34159d1f2418f9a45&quot;,&quot;ai_summary&quot;:&quot;DC-VideoGen accelerates video generation by adapting pre-trained diffusion models to a deep compression latent space, reducing inference latency and enabling high-resolution video generation.&quot;,&quot;ai_keywords&quot;:[&quot;DC-VideoGen&quot;,&quot;video diffusion model&quot;,&quot;deep compression latent space&quot;,&quot;lightweight fine-tuning&quot;,&quot;Deep Compression Video Autoencoder&quot;,&quot;chunk-causal temporal design&quot;,&quot;AE-Adapt-V&quot;,&quot;Wan-2.1-14B model&quot;,&quot;inference latency&quot;,&quot;high-resolution video generation&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;646189cd5dba83471db2af58&quot;,&quot;avatarUrl&quot;:&quot;/avatars/d5a1549af336cb5f1fa5622250d38a73.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;JUNYU CHEN&quot;,&quot;user&quot;:&quot;cjy2003&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66ce751a8ec9fda2cf5a9e85&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c17093ca81dad007b3e50bae503955a7.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Haocheng Xi&quot;,&quot;user&quot;:&quot;xihc-ucb&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63129589bbaa385279d1826e&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Muyang Li&quot;,&quot;user&quot;:&quot;Lmxyy&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;650e2b14c945dfc9386a7e28&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Han Cai&quot;,&quot;user&quot;:&quot;han-cai&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;641d8bacd526196afc12766d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/73f7b2d86a7bf27940bec2b1f199d71b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shang Yang&quot;,&quot;user&quot;:&quot;Shangy&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66a156136609d2b2b0f6353a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/fc6850b5fc437269bf0870f6a6cdcf40.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yujun Lin&quot;,&quot;user&quot;:&quot;synxlin&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6579569562d3ac18171cf9cb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bf3bc3130b5db3594e810624a936f721.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yecheng Wu&quot;,&quot;user&quot;:&quot;gbcfchc&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63021630a35b21bd8a53305a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7a7e8b39749eda61e57d8a1908726558.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Gu Yuchao&quot;,&quot;user&quot;:&quot;guyuchao&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;645b5b09bc7518912e1f9733&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4d35f728b41f93881a9b67c337f4d1df.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chen&quot;,&quot;user&quot;:&quot;Lawrence-cj&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;634ce90e741a5e37886a19e3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0d1579039136b37db5b67282b0a34c33.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Syang&quot;,&quot;user&quot;:&quot;Andyson&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;650c74e5d439bbbbadfcfbbe&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1002835739dbb90214b5f2824a7c8c1f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;YU Jincheng&quot;,&quot;user&quot;:&quot;yujincheng08&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6683fc5344a65be1aab25dc0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e13cde3f87b59e418838d702807df3b5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;hjkim&quot;,&quot;user&quot;:&quot;hojie11&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;60262b67268c201cdc8b7d43&quot;,&quot;name&quot;:&quot;nvidia&quot;,&quot;fullname&quot;:&quot;NVIDIA&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png&quot;}}\"> Papers arxiv:2509.25182","published":"2025-10-01","source":"huggingface","url":"https://huggingface.co/papers/2509.25182","analysis":{"introduction":"🚀 Want 4K video generation on a single H100 GPU? \nDC-VideoGen adapts pre-trained video diffusion models into a deep-compression latent space to run up to 2160×3840, cut inference latency by 14.8×, and slash training cost — unlocking high-res video generation with light fine-tuning.","challenges":"🎯 Key problems tackled:\n- Large compute & memory: high-res video diffusion is slow and GPU‑heavy.\n- Training cost: training video diffusion from scratch is prohibitively expensive.\n- Latent inefficiency: prior latents limit resolution or temporal scaling.","innovations":"✨ Core innovations:\n- Deep Compression Video Autoencoder (DC-AE-V) with chunk-causal temporal design.\n- DC-AE-V achieves 32×/64× spatial + 4× temporal compression while preserving reconstructions.\n- AE-Adapt-V: a lightweight, stable adaptation strategy to transfer pre-trained models into the compressed latent.","experiments":"📊 Main empirical result:\nAchieved up to 14.8× faster inference vs the base model. Adapting Wan-2.1-14B took only 10 H100 GPU-days (≈230× lower training cost than training from scratch) and enabled 2160×3840 video generation on one H100.","insights":"🤔 What's next?\n- Explore integrating DC-AE-V latents with text-to-video or conditional diffusion for efficient high-res generation.\n- Apply deep-compression latents to streaming, real-time editing, or edge deployment. Could this make high-quality video gen ubiquitous on single GPUs?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"\"🚀 想要在单个H100 GPU上实现4K视频生成吗？DC-VideoGen将预训练的视频扩散模型适配到一个深度压缩的潜在空间中，从而能够运行高达2160×3840的分辨率，将推理延迟减少14.8倍，并大幅降低训练成本——通过轻量级的微调即可解锁高分辨率视频生成。\"","chinese_challenges":"\"核心挑战：\\n- 大计算量与内存需求：高分辨率视频扩散模型运行缓慢，且对GPU资源消耗巨大。\\n- 训练成本：从零开始训练视频扩散模型的成本极其昂贵。\\n- 潜在表示效率低下：现有的潜在表示（Latents）限制了模型在分辨率或时间维度上的扩展能力。\"","chinese_innovations":"[\n  {\n    \"核心创新点\": [\n      \"深度压缩视频自编码器 (DC-AE-V)，采用块因果（chunk-causal）时间设计。\",\n      \"DC-AE-V 实现了 32 倍/64 倍空间压缩和 4 倍时间压缩，同时保持了重建质量。\",\n      \"AE-Adapt-V：一种轻量级、稳定的适应策略，用于将预训练模型迁移到压缩后的潜在空间。\"\n    ]\n  }\n]","chinese_experiments":"","chinese_insights":"{\n  \"🤔 未来展望\": [\n    \"探索将 DC-AE-V 隐变量（latents）与文生视频（text-to-video）或条件扩散模型集成，以实现高效的高分辨率内容生成。\",\n    \"将深度压缩的隐变量应用于流媒体、实时编辑或边缘部署场景。\",\n    \"这是否能使高质量视频生成在单块 GPU 上实现普及化？\"\n  ]\n}","summary":"**Introduction:** 🚀 Want 4K video generation on a single H100 GPU? \nDC-VideoGen adapts pre-trained video diffusion models into a deep-compression latent space to run up to 2160×3840, cut inference latency by 14.8×, and slash training cost — unlocking high-res video generation with light fine-tuning.\n\n**Challenges:** 🎯 Key problems tackled:\n- Large compute & memory: high-res video diffusion is slow and GPU‑heavy.\n- Training cost: t...","analyzed_at":"2025-10-01T12:58:10.197Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:30.899Z","views":0,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"hf_dc_videogen__efficient_video_generation_with_deep_compression_video_autoencoder_1759311090899","views_at_archive":0}},{"id":"hf_truthrl__incentivizing_truthful_llms_via_reinforcement_learning_1759311079944","title":"TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning","authors":[],"abstract":"While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy---models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs. We also experiment with more complicated reward designs, such as knowledge-enhanced and reasoning-enhanced variants, and show that a simple ternary reward scheme generally performs better. Moreover, we find the improvement of TruthRL arises from enhancing the capability of LLMs to recognize their knowledge boundary, hence avoiding being overly conservative as the baselines are. Further analysis confirms that TruthRL is robust to hallucination-baiting questions and more confident in producing accurate responses.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:26:26.198Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;6526307af06ac0cf9a922e86&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;,&quot;fullname&quot;:&quot;Zhepei Wei&quot;,&quot;name&quot;:&quot;weizhepei&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9195460677146912},&quot;editors&quot;:[&quot;weizhepei&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;👍&quot;,&quot;users&quot;:[&quot;sytmr&quot;,&quot;TianHongZXY&quot;,&quot;WZDavid&quot;],&quot;count&quot;:3},{&quot;reaction&quot;:&quot;🔥&quot;,&quot;users&quot;:[&quot;WZDavid&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25760&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aa7&quot;,&quot;name&quot;:&quot;Zhepei Wei&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aa8&quot;,&quot;name&quot;:&quot;Xiao Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aa9&quot;,&quot;name&quot;:&quot;Kai Sun&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aaa&quot;,&quot;name&quot;:&quot;Jiaqi Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aab&quot;,&quot;name&quot;:&quot;Rulin Shao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aac&quot;,&quot;name&quot;:&quot;Sean Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aad&quot;,&quot;name&quot;:&quot;Mohammad Kachuee&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aae&quot;,&quot;name&quot;:&quot;Teja Gollapudi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aaf&quot;,&quot;name&quot;:&quot;Tony Liao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab0&quot;,&quot;name&quot;:&quot;Nicolas Scheffer&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab1&quot;,&quot;name&quot;:&quot;Rakesh Wanga&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab2&quot;,&quot;name&quot;:&quot;Anuj Kumar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab3&quot;,&quot;name&quot;:&quot;Yu Meng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab4&quot;,&quot;name&quot;:&quot;Wen-tau Yih&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab5&quot;,&quot;name&quot;:&quot;Xin Luna Dong&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-30T04:25:17.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:56:26.188Z&quot;,&quot;title&quot;:&quot;TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6526307af06ac0cf9a922e86&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhepei Wei&quot;,&quot;user&quot;:&quot;weizhepei&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;While large language models (LLMs) have demonstrated strong performance on\\nfactoid question answering, they are still prone to hallucination and\\nuntruthful responses, particularly when tasks demand information outside their\\nparametric knowledge. Indeed, truthfulness requires more than accuracy --\\nmodels must also recognize uncertainty and abstain when unsure to avoid\\nhallucinations. This presents a fundamental challenge for existing methods:\\napproaches that optimize for accuracy often amplify hallucinations, while those\\nthat encourage abstention can become overly conservative, sacrificing correct\\nanswers. Both extremes ultimately compromise truthfulness. In this work, we\\npresent TruthRL, a general reinforcement learning (RL) framework that directly\\noptimizes the truthfulness of LLMs. Specifically, we implement TruthRL using\\nGRPO with a simple yet effective ternary reward that distinguishes correct\\nanswers, hallucinations, and abstentions. It incentivizes models to reduce\\nhallucinations not only by providing correct responses, but also by enabling\\nabstention when uncertain, thereby improving truthfulness. Extensive\\nexperiments across four knowledge-intensive benchmarks show that, compared to\\nvanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves\\ntruthfulness by 21.1%, with consistent gains across various backbone models\\n(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth\\nablation study demonstrates that vanilla accuracy-driven methods, such as\\nsupervised fine-tuning or RL with a binary reward, struggle to balance factual\\ncorrectness and uncertainty. In contrast, our proposed truthfulness-driven\\nTruthRL achieves strong performance in both accuracy and truthfulness,\\nunderscoring the importance of learning objective design for developing\\ntruthful LLMs.&quot;,&quot;upvotes&quot;:34,&quot;discussionId&quot;:&quot;68dc90b84159d1f2418f9ab6&quot;,&quot;ai_summary&quot;:&quot;TruthRL, a reinforcement learning framework, enhances the truthfulness of large language models by balancing accuracy and abstention, significantly reducing hallucinations and improving performance across benchmarks.&quot;,&quot;ai_keywords&quot;:[&quot;large language models&quot;,&quot;LLMs&quot;,&quot;hallucination&quot;,&quot;untruthful responses&quot;,&quot;parametric knowledge&quot;,&quot;truthfulness&quot;,&quot;reinforcement learning&quot;,&quot;RL&quot;,&quot;GRPO&quot;,&quot;ternary reward&quot;,&quot;abstention&quot;,&quot;accuracy-driven methods&quot;,&quot;supervised fine-tuning&quot;,&quot;binary reward&quot;,&quot;knowledge-intensive benchmarks&quot;,&quot;Qwen&quot;,&quot;Llama&quot;,&quot;retrieval&quot;,&quot;non-retrieval setups&quot;,&quot;ablation study&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;6526307af06ac0cf9a922e86&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhepei Wei&quot;,&quot;user&quot;:&quot;weizhepei&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62ea79dd01ed9b0e8f61ccd3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chengsong Huang&quot;,&quot;user&quot;:&quot;ChengsongHuang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;617aec6f6f37340367d5d7a1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/afa58f39896c5caef512675450c7d6ce.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yu Meng&quot;,&quot;user&quot;:&quot;yumeng5&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6602787b1827b6d37ee527be&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f69c1779b4347a042dad1a0d962145af.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tzu-Han Lin&quot;,&quot;user&quot;:&quot;hank0316&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6587e5a4b2177de3967ff434&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f2dfbc44eb2bff8d8d66d26db8539708.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shuyao Xu&quot;,&quot;user&quot;:&quot;Tim-Xu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66080ddac201aee890e5efeb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/dae8f44b2eda9d1950efaa10a5aa986f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jing Chen&quot;,&quot;user&quot;:&quot;jingchen6688&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;623b290048f658f28aef79f7&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1648044277149-noauth.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xinyu Zhu&quot;,&quot;user&quot;:&quot;TianHongZXY&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6633f39185b05e9a8e7c549c&quot;,&quot;avatarUrl&quot;:&quot;/avatars/ee4df68daee8b6637d7ad86cba29cc2f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;shiyu&quot;,&quot;user&quot;:&quot;sytmr&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;664e88ad8ab2524c036c3d2f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/HItpTu75SFqA5ouOMKzVb.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhining Liu&quot;,&quot;user&quot;:&quot;ZhiningLiu1998&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;619e6657cc04eadf54fa5d2d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c78c54767c63c75a9f6783ffa78a98fa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wei-Lin Chen&quot;,&quot;user&quot;:&quot;wlchen&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6590a65b89f1ff0463828e53&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4ab424eede2fe9c114252b1e5dd1ba25.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;sizhe&quot;,&quot;user&quot;:&quot;sizhe04&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;647a248bed75e95d3e98e3d6&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e10e2f8516d1451fd85e17b5a0ba978d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yaochen Zhu&quot;,&quot;user&quot;:&quot;yaochenzhu&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:2,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;5e63d8713071d5be688861b8&quot;,&quot;name&quot;:&quot;facebook&quot;,&quot;fullname&quot;:&quot;AI at Meta&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png&quot;}}\"> Papers arxiv:2509.25760","published":"2025-10-01","source":"huggingface","url":"https://huggingface.co/papers/2509.25760","analysis":{"introduction":"🚀 Question: How do we make LLMs say “I don’t know” instead of confidently hallucinating?\nTruthRL is an RL framework (GRPO + a simple ternary reward) that trains models to answer correctly or abstain, cutting hallucinations and improving truthfulness across backbones & setups.","challenges":"🎯 Key problems tackled:\n- Models hallucinate on facts outside parametric knowledge.\n- Accuracy-driven training amplifies confident but wrong answers.\n- Abstention-focused methods become overly conservative and lose correct answers.","innovations":"✨ Core innovations:\n- TruthRL: an RL framework optimizing truthfulness directly.\n- Implemented with GRPO and a ternary reward (correct / hallucination / abstain).\n- Novelty: reward explicitly balances giving correct answers and abstaining when uncertain.","experiments":"📊 Results: TruthRL reduced hallucinations by 28.9% and improved truthfulness by 21.1% across four knowledge-intensive benchmarks, with consistent gains across backbones (Qwen, Llama) and both retrieval & non-retrieval setups — showing the ternary reward’s practical impact.","insights":"🤔 What’s next?\n- Explore combining TruthRL with stronger calibrated uncertainty estimators or multi-turn dialogue to improve abstention in interactive settings.\n- Broader applications: high-stakes domains (medicine, law) and retrieval-augmented systems where avoiding hallucination matters most.\nCould TruthRL become a standard for safe QA?","category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"\"🚀 问题：我们如何让大型语言模型（LLMs）说出“我不知道”，而不是自信地给出幻觉内容？TruthRL是一个强化学习（RL）框架（由GRPO和一个简单的三元奖励机制构成），它训练模型要么正确回答，要么选择弃权，从而减少幻觉现象，并在各种基础模型和配置中提高真实性。\"","chinese_challenges":"{\n  \"关键挑战\": [\n    \"模型对超出其参数化知识范围的事实产生“幻觉”（虚假信息）。\",\n    \"以准确率（精度）为导向的训练会加剧模型对错误答案的过度自信。\",\n    \"以弃权（拒绝回答）为核心的方法往往过于保守，导致错失正确的答案。\"\n  ]\n}","chinese_innovations":"\"核心创新点：\\n- TruthRL：一种直接优化“真实性”（truthfulness）的强化学习（RL）框架。\\n- 使用GRPO算法和三元奖励机制（正确 / 幻觉 / 弃权）实现。\\n- 创新点：奖励机制明确地平衡了“给出正确答案”和“在不确定时选择弃权”这两种行为。\"","chinese_experiments":"{\n  \"analysis_type\": \"research_summary\",\n  \"model_name\": \"TruthRL\",\n  \"metrics\": [\n    {\n      \"metric\": \"hallucinations_reduction\",\n      \"value\": 0.289,\n      \"unit\": \"percentage\"\n    },\n    {\n      \"metric\": \"truthfulness_improvement\",\n      \"value\": 0.211,\n      \"unit\": \"percentage\"\n    }\n  ],\n  \"scope\": {\n    \"benchmarks\": \"four knowledge-intensive benchmarks\",\n    \"backbones\": [\n      \"Qwen\",\n      \"Llama\"\n    ],\n    \"setups\": [\n      \"retrieval\",\n      \"non-retrieval\"\n    ]\n  },\n  \"conclusion\": \"The ternary reward mechanism demonstrated practical impact through consistent gains across diverse configurations.\"\n}","chinese_insights":"{\n  \"insights\": [\n    \"🤔 下一步是什么？\",\n    \"探索将TruthRL与更强的校准不确定性估计器或多轮对话相结合，以改进在交互式设置中的拒绝回答（Abstention）能力。\",\n    \"更广泛的应用：在高风险领域（如医疗、法律）以及避免“幻觉”（Hallucination）至关重要的检索增强系统中。\",\n    \"TruthRL能否成为安全问答（QA）的标准？\"\n  ]\n}","summary":"**Introduction:** 🚀 Question: How do we make LLMs say “I don’t know” instead of confidently hallucinating?\nTruthRL is an RL framework (GRPO + a simple ternary reward) that trains models to answer correctly or abstain, cutting hallucinations and improving truthfulness across backbones & setups.\n\n**Challenges:** 🎯 Key problems tackled:\n- Models hallucinate on facts outside parametric knowledge.\n- Accuracy-driven training amplifies c...","analyzed_at":"2025-10-01T11:16:31.375Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:19.945Z","views":0,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"hf_truthrl__incentivizing_truthful_llms_via_reinforcement_learning_1759311079944","views_at_archive":0}},{"id":"arxiv_2509.26633v1","title":"OmniRetarget: Interaction-Preserving Data Generation for Humanoid\n  Whole-Body Loco-Manipulation and Scene Interaction","authors":["Lujie Yang","Xiaoyu Huang","Zhen Wu","Angjoo Kanazawa","Pieter Abbeel","Carmelo Sferrazza","C. Karen Liu","Rocky Duan","Guanya Shi"],"abstract":"A dominant paradigm for teaching humanoid robots complex skills is to\nretarget human motions as kinematic references to train reinforcement learning\n(RL) policies. However, existing retargeting pipelines often struggle with the\nsignificant embodiment gap between humans and robots, producing physically\nimplausible artifacts like foot-skating and penetration. More importantly,\ncommon retargeting methods neglect the rich human-object and human-environment\ninteractions essential for expressive locomotion and loco-manipulation. To\naddress this, we introduce OmniRetarget, an interaction-preserving data\ngeneration engine based on an interaction mesh that explicitly models and\npreserves the crucial spatial and contact relationships between an agent, the\nterrain, and manipulated objects. By minimizing the Laplacian deformation\nbetween the human and robot meshes while enforcing kinematic constraints,\nOmniRetarget generates kinematically feasible trajectories. Moreover,\npreserving task-relevant interactions enables efficient data augmentation, from\na single demonstration to different robot embodiments, terrains, and object\nconfigurations. We comprehensively evaluate OmniRetarget by retargeting motions\nfrom OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour\ntrajectories that achieve better kinematic constraint satisfaction and contact\npreservation than widely used baselines. Such high-quality data enables\nproprioceptive RL policies to successfully execute long-horizon (up to 30\nseconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained\nwith only 5 reward terms and simple domain randomization shared by all tasks,\nwithout any learning curriculum.","published":"2025-09-30T17:59:02Z","source":"arxiv","url":"http://arxiv.org/abs/2509.26633v1","analysis":{"introduction":"🚀 Want humanoid robots to move and interact as fluidly as humans? OmniRetarget is an interaction-preserving data engine that retargets human motion to robots by modeling contacts and scene relations — producing kinematically feasible, interaction-aware demos for RL. Benefits: better loco-manipulation and sim-to-real training.","challenges":"🎯 Key problems tackled:\n- Existing retargeting fails across the human↔robot embodiment gap (foot‑skate, penetration).\n- Prior pipelines ignore rich human–object and human–environment interactions.\n- Limited data augmentation across robots/terrains/objects hampers robust RL.","innovations":"✨ Core innovations:\n- Interaction mesh that explicitly encodes spatial/contact relationships between agent, terrain, and objects.\n- Retargeting via minimizing Laplacian deformation between human and robot meshes while enforcing kinematic constraints.\n- Interaction-preserving data augmentation to transfer a single demo across embodiments, terrains, and object configs.\nNovelty: explicit interaction mesh + Laplacian deformation for retargeting to preserve contacts and task geometry.","experiments":"📊 Results: Retargeted motions from OMOMO, LAFAN1 and in-house MoCap to produce over 8 hours of trajectories with better kinematic-constraint satisfaction and contact preservation than common baselines; enabled proprioceptive RL to execute long-horizon (up to 30s) parkour and loco-manipulation on a Unitree G1, trained with only 5 reward terms and simple domain randomization.","insights":"🤔 What’s next?\n- Research: adapt OmniRetarget for online/adaptive retargeting and multi-robot transfer, or combine with perception for closed-loop real-world interactions.\n- Applications: fast data generation for real-world humanoid deployment (assistive robots, warehouse manipulation, entertainment).\nCould interaction-preserving retargeting unlock more reliable sim-to-real loco-manipulation?","keywords":["retargeting","humanoid","interaction mesh","Laplacian deformation","loco-manipulation","contact preservation","reinforcement learning","proprioceptive policy","Unitree G1","data augmentation","OMOMO","LAFAN1"],"category":"robotics","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"\"🚀 想要人形机器人像人类一样流畅地移动和交互吗？OmniRetarget 是一种保持交互性的数据引擎，它通过建模接触和场景关系，将人类动作重定向到机器人上，从而为强化学习（RL）生成运动学可行、交互感知的演示数据。优势包括：提升运动操作能力（loco-manipulation）以及改进虚实迁移（sim-to-real）训练效果。\"","chinese_challenges":"{\n  \"key_problems_tackled\": [\n    \"Existing retargeting fails across the human↔robot embodiment gap (foot‑skate, penetration).\",\n    \"Prior pipelines ignore rich human–object and human–environment interactions.\",\n    \"Limited data augmentation across robots/terrains/objects hampers robust RL.\"\n  ]\n}","chinese_innovations":"{\n  \"innovations\": \"✨ 核心创新点：\\n- 交互网格：显式编码智能体、地形和物体之间的空间/接触关系。\\n- 重定向：通过最小化人体和机器人网格之间的拉普拉斯形变，同时强制执行运动学约束来实现。\\n- 保持交互的数据增强：将单个演示跨越不同实体、地形和物体配置进行迁移。\\n新颖性：显式的交互网格 + 用于重定向的拉普拉斯形变，以保持接触和任务几何结构。\"\n}","chinese_experiments":"{\n  \"translation\": \"📊 结果：我们对来自 OMOMO、LAFAN1 和内部 MoCap（运动捕捉）数据的动作进行了重定向，生成了超过 8 小时的轨迹。与常见的基线方法相比，这些轨迹在满足运动学约束和保持接触方面表现更佳；这使得本体感知强化学习（proprioceptive RL）能够在 Unitree G1 机器人上执行长时程（最长达 30 秒）的跑酷和移动操作任务，且训练仅使用了 5 个奖励项和简单的域随机化。\"\n}","chinese_insights":"{\n  \"translation\": \"🤔 下一步是什么？\\n- 研究：使 OmniRetarget 适应在线/自适应重定向和多机器人迁移，或将其与感知相结合，实现闭环的真实世界交互。\\n- 应用：为真实世界的人形机器人部署（辅助机器人、仓库操作、娱乐）快速生成数据。\\n这种保持交互的重定向能否解锁更可靠的“仿真到现实”的运动-操作（loco-manipulation）能力？\"\n}","summary":"**Introduction:** 🚀 Want humanoid robots to move and interact as fluidly as humans? OmniRetarget is an interaction-preserving data engine that retargets human motion to robots by modeling contacts and scene relations — producing kinematically feasible, interaction-aware demos for RL. Benefits: better loco-manipulation and sim-to-real training.\n\n**Challenges:** 🎯 Key problems tackled:\n- Existing retargeting fails across the human↔r...","analyzed_at":"2025-10-01T10:56:39.287Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:12.598Z","views":12,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"arxiv_2509.26633v1","views_at_archive":12}},{"id":"arxiv_2509.26626v1","title":"Recursive Self-Aggregation Unlocks Deep Thinking in Large Language\n  Models","authors":["Siddarth Venkatraman","Vineet Jain","Sarthak Mittal","Vedant Shah","Johan Obando-Ceron","Yoshua Bengio","Brian R. Bartoldson","Bhavya Kailkhura","Guillaume Lajoie","Glen Berseth","Nikolay Malkin","Moksh Jain"],"abstract":"Test-time scaling methods improve the capabilities of large language models\n(LLMs) by increasing the amount of compute used during inference to make a\nprediction. Inference-time compute can be scaled in parallel by choosing among\nmultiple independent solutions or sequentially through self-refinement. We\npropose Recursive Self-Aggregation (RSA), a test-time scaling method inspired\nby evolutionary methods that combines the benefits of both parallel and\nsequential scaling. Each step of RSA refines a population of candidate\nreasoning chains through aggregation of subsets to yield a population of\nimproved solutions, which are then used as the candidate pool for the next\niteration. RSA exploits the rich information embedded in the reasoning chains\n-- not just the final answers -- and enables bootstrapping from partially\ncorrect intermediate steps within different chains of thought. Empirically, RSA\ndelivers substantial performance gains with increasing compute budgets across\ndiverse tasks, model families and sizes. Notably, RSA enables\nQwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning\nmodels, including DeepSeek-R1 and o3-mini (high), while outperforming purely\nparallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning\nGym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the\nmodel to combine solutions via a novel aggregation-aware reinforcement learning\napproach yields significant performance gains. Code available at\nhttps://github.com/HyperPotatoNeo/RSA.","published":"2025-09-30T17:58:03Z","source":"arxiv","url":"http://arxiv.org/abs/2509.26626v1","analysis":{"introduction":"🚀 Question: Can small LLMs 'think deeper' at inference time without more training?\nRSA (Recursive Self-Aggregation) is a test-time scaling method that iteratively aggregates and refines populations of reasoning chains to unlock deeper reasoning—letting smaller models compete with bigger ones.","challenges":"🎯 Challenges:\n- Existing test-time scaling is typically either parallel (many independent tries) or sequential (self-refinement), not both.\n- Methods ignore partial correctness in intermediate reasoning steps across chains.\n- Inefficient use of inference compute for bootstrapping stronger solutions.","innovations":"✨ Innovations:\n- Recursive Self-Aggregation (RSA): iteratively refines a population of candidate reasoning chains by aggregating subsets to produce improved candidates.\n- Exploits intermediate chain-of-thought content (not just final answers) to bootstrap.\n- Aggregation-aware reinforcement learning to train models to better combine solutions.","experiments":"📊 Experiment (most compelling result):\nQwen3-4B-Instruct-2507 using RSA achieved competitive performance with larger reasoning models (DeepSeek-R1 and o3-mini (high)) and outperformed purely parallel and purely sequential scaling across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. Specific numeric gains: Not specified in the paper.","insights":"🤔 Insights & next steps:\n- Research: adapt RSA for multimodal or retrieval-augmented LLMs; develop adaptive subset-selection strategies or theory on when aggregation helps.\n- Applications: stronger reasoning in education/tutoring, code synthesis, and decision support—letting smaller models punch above their size. Could RSA enable cheaper, reliable 'deep thinking' at scale?","category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"title\": \"递归自聚合（RSA）：让小型LLM在推理时“深度思考”\",\n  \"content\": \"🚀 问题：小型大语言模型（LLMs）能否在不增加训练量的情况下，在推理时进行“更深层次的思考”？\\n\\n递归自聚合（Recursive Self-Aggregation，简称RSA）是一种测试时（test-time）的扩展方法，它通过迭代地聚合和精炼推理链的集合（populations），以释放更深层次的推理能力——从而使小型模型能够与大型模型竞争。\"\n}","chinese_challenges":"{\n  \"challenges\": \"挑战：\"\n}","chinese_innovations":"{\n  \"innovations\": \"✨ 创新点：\\n- 递归自聚合（Recursive Self-Aggregation, RSA）：通过聚合候选推理链的子集，迭代地精炼推理链的群体，以产生改进的候选。 \\n- 利用中间的思维链内容（而不仅仅是最终答案）进行自举（bootstrap）。\\n- 聚合感知强化学习（Aggregation-aware reinforcement learning）来训练模型更好地结合解决方案。\"\n}","chinese_experiments":"{\n  \"experiment\": \"Qwen3-4B-Instruct-2507 using RSA achieved competitive performance with larger reasoning models (DeepSeek-R1 and o3-mini (high)) and outperformed purely parallel and purely sequential scaling across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. Specific numeric gains: Not specified in the paper.\"\n}","chinese_insights":"[\n  {\n    \"insight_type\": \"Research\",\n    \"description\": \"adapt RSA for multimodal or retrieval-augmented LLMs; develop adaptive subset-selection strategies or theory on when aggregation helps.\"\n  },\n  {\n    \"insight_type\": \"Applications\",\n    \"description\": \"stronger reasoning in education/tutoring, code synthesis, and decision support—letting smaller models punch above their size. Could RSA enable cheaper, reliable 'deep thinking' at scale?\"\n  }\n]","summary":"**Introduction:** 🚀 Question: Can small LLMs 'think deeper' at inference time without more training?\nRSA (Recursive Self-Aggregation) is a test-time scaling method that iteratively aggregates and refines populations of reasoning chains to unlock deeper reasoning—letting smaller models compete with bigger ones.\n\n**Challenges:** 🎯 Challenges:\n- Existing test-time scaling is typically either parallel (many independent tries) or seque...","analyzed_at":"2025-10-01T11:01:47.590Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:12.598Z","views":6,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"arxiv_2509.26626v1","views_at_archive":6}},{"id":"arxiv_2509.26631v1","title":"Learning Generalizable Shape Completion with SIM(3) Equivariance","authors":["Yuqing Wang","Zhaiyu Chen","Xiao Xiang Zhu"],"abstract":"3D shape completion methods typically assume scans are pre-aligned to a\ncanonical frame. This leaks pose and scale cues that networks may exploit to\nmemorize absolute positions rather than inferring intrinsic geometry. When such\nalignment is absent in real data, performance collapses. We argue that robust\ngeneralization demands architectural equivariance to the similarity group,\nSIM(3), so the model remains agnostic to pose and scale. Following this\nprinciple, we introduce the first SIM(3)-equivariant shape completion network,\nwhose modular layers successively canonicalize features, reason over\nsimilarity-invariant geometry, and restore the original frame. Under a\nde-biased evaluation protocol that removes the hidden cues, our model\noutperforms both equivariant and augmentation baselines on the PCN benchmark.\nIt also sets new cross-domain records on real driving and indoor scans,\nlowering minimal matching distance on KITTI by 17% and Chamfer distance $\\ell1$\non OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol\nstill outperforms competitors under their biased settings. These results\nestablish full SIM(3) equivariance as an effective route to truly generalizable\nshape completion. Project page: https://sime-completion.github.io.","published":"2025-09-30T17:58:55Z","source":"arxiv","url":"http://arxiv.org/abs/2509.26631v1","analysis":{"introduction":"🚀 What if 3D completion models are secretly cheating with pose & scale cues? This paper introduces the first SIM(3)-equivariant shape completion network that is agnostic to pose and scale — improving real-world generalization for robotics, AR, and autonomous driving.","challenges":"🎯 Challenges:\n- Existing methods assume scans are pre-aligned, leaking pose/scale cues.\n- Networks memorize absolute positions instead of intrinsic geometry.\n- Performance collapses on unaligned, real-world scans (poor cross-domain generalization).","innovations":"✨ Innovations:\n- First SIM(3)-equivariant shape completion network.\n- Modular layers that: canonicalize features → reason over similarity-invariant geometry → restore original frame.\n- De-biased evaluation protocol to remove hidden pose/scale cues.\nNovelty: full SIM(3) equivariance to force pose/scale agnosticism.","experiments":"📊 Experiment (most compelling): Lowered minimal matching distance on KITTI by 17% vs prior methods — proving SIM(3) equivariance substantially improves cross-domain shape completion generalization.","insights":"🤔 Insights / Next steps:\n- Explore combining SIM(3)-equivariant completion with learned pose priors or SLAM for online reconstruction.\n- Apply to autonomous driving, indoor mapping, AR/VR where scans are unaligned.\nCould SIM(3) equivariance become a standard inductive bias for 3D tasks?","keywords":["SIM(3) equivariance","shape completion","3D deep learning","point clouds","generalization","canonicalization"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"translation\": \"🚀 如果3D补全模型偷偷利用姿态和尺度线索作弊怎么办？本文介绍了首个SIM(3)等变形状补全网络，该网络对姿态和尺度不敏感——从而提高了其在机器人技术、增强现实（AR）和自动驾驶等领域的真实世界泛化能力。\"\n}","chinese_challenges":"{\n  \"challenges\": \"🎯 挑战：\\n- 现有方法假设扫描数据已预先对齐，这会泄露姿态/尺度线索。\\n- 网络倾向于记忆绝对位置，而非内在几何结构。\\n- 在未对齐的真实世界扫描数据上，性能会急剧下降（跨域泛化能力差）。\"\n}","chinese_innovations":"{\n  \"innovations\": [\n    \"首个SIM(3)等变形状补全网络。\",\n    \"模块化层：将特征规范化 $\\\\rightarrow$ 基于相似性不变几何进行推理 $\\\\rightarrow$ 恢复原始坐标系。\",\n    \"去偏倚评估协议，以消除隐藏的位姿/尺度线索。\",\n    \"新颖性：完全SIM(3)等变性，以强制实现位姿/尺度无关性。\"\n  ]\n}","chinese_experiments":"\"实验（最具说服力的）：在KITTI数据集上，将最小匹配距离比现有方法降低了17%，证明了SIM(3)等变性显著提高了跨域形状补全的泛化能力。\"","chinese_insights":"\"🤔 洞察/下一步：\\n- 探索将SIM(3)等变补全与学习到的姿态先验或SLAM相结合，用于在线重建。\\n- 应用于自动驾驶、室内建图、AR/VR等扫描数据未对齐的场景。\\n- SIM(3)等变性是否能成为3D任务的标准归纳偏置？\"","summary":"**Introduction:** 🚀 What if 3D completion models are secretly cheating with pose & scale cues? This paper introduces the first SIM(3)-equivariant shape completion network that is agnostic to pose and scale — improving real-world generalization for robotics, AR, and autonomous driving.\n\n**Challenges:** 🎯 Challenges:\n- Existing methods assume scans are pre-aligned, leaking pose/scale cues.\n- Networks memorize absolute positions inst...","analyzed_at":"2025-10-01T10:58:18.765Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:12.598Z","views":0,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"arxiv_2509.26631v1","views_at_archive":0}},{"id":"arxiv_2509.26641v1","title":"Query-Kontext: An Unified Multimodal Model for Image Generation and\n  Editing","authors":["Yuxin Song","Wenkai Dong","Shizun Wang","Qi Zhang","Song Xue","Tao Yuan","Hu Yang","Haocheng Feng","Hang Zhou","Xinyan Xiao","Jingdong Wang"],"abstract":"Unified Multimodal Models (UMMs) have demonstrated remarkable performance in\ntext-to-image generation (T2I) and editing (TI2I), whether instantiated as\nassembled unified frameworks which couple powerful vision-language model (VLM)\nwith diffusion-based generator, or as naive Unified Multimodal Models with an\nearly fusion of understanding and generation modalities. We contend that in\ncurrent unified frameworks, the crucial capability of multimodal generative\nreasoning which encompasses instruction understanding, grounding, and image\nreferring for identity preservation and faithful reconstruction, is\nintrinsically entangled with high-fidelity synthesis. In this work, we\nintroduce Query-Kontext, a novel approach that bridges the VLM and diffusion\nmodel via a multimodal ``kontext'' composed of semantic cues and coarse-grained\nimage conditions encoded from multimodal inputs. This design delegates the\ncomplex ability of multimodal generative reasoning to powerful VLM while\nreserving diffusion model's role for high-quality visual synthesis. To achieve\nthis, we propose a three-stage progressive training strategy. First, we connect\nthe VLM to a lightweight diffusion head via multimodal kontext tokens to\nunleash the VLM's generative reasoning ability. Second, we scale this head to a\nlarge, pre-trained diffusion model to enhance visual detail and realism.\nFinally, we introduce a low-level image encoder to improve image fidelity and\nperform instruction tuning on downstream tasks. Furthermore, we build a\ncomprehensive data pipeline integrating real, synthetic, and open-source\ndatasets, covering diverse multimodal reference-to-image scenarios, including\nimage generation, instruction-driven editing, customized generation, and\nmulti-subject composition. Experiments show that our approach matches strong\nunified baselines and even outperforms task-specific state-of-the-art methods\nin several cases.","published":"2025-09-30T17:59:46Z","source":"arxiv","url":"http://arxiv.org/abs/2509.26641v1","analysis":{"introduction":"🚀 Can one model both understand complex multimodal instructions and produce high‑fidelity edits? \nQuery‑Kontext bridges a vision‑language model (VLM) and a diffusion generator via multimodal “kontext” tokens, offloading reasoning to the VLM and reserving synthesis for diffusion — improving T2I and instruction‑driven editing.","challenges":"🎯 Key problems tackled:\n- Existing unified frameworks entangle multimodal generative reasoning with high‑fidelity synthesis, hurting modularity.\n- Preserving identity and faithful reconstruction during instruction‑driven editing is hard.\n- Lack of a unified, diverse data pipeline for varied reference→image scenarios.","innovations":"✨ Core innovations:\n- Multimodal kontext: semantic cues + coarse image condition tokens that bridge VLM ↔ diffusion.\n- Three‑stage progressive training: lightweight diffusion head → scale to pretrained diffusion → add low‑level image encoder + instruction tuning.\n- Comprehensive data pipeline combining real, synthetic, and open datasets.\nNovel twist: explicit decoupling of generative reasoning (VLM) from high‑quality synthesis (diffusion) via kontext tokens.","experiments":"📊 Results: The approach matches strong unified baselines and even outperforms task‑specific state‑of‑the‑art methods in several cases. Exact numeric improvements or benchmark scores are not specified in the paper. \nMain proof: decoupling reasoning and synthesis via kontext tokens yields competitive or superior empirical performance.","insights":"🤔 Future directions & applications:\n- Explore spatially explicit kontext (per‑region tokens) or scene‑graph conditioning for finer control and compositionality.\n- Integrate multimodal chain‑of‑thought or stronger grounding for complex multi‑subject edits.\nPotential apps: personalized image editing, multi‑subject composition for content creation and AR/VR. Could this modular split speed up customization and safety checks?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"translation\": \"🚀 是否存在一个模型能够同时理解复杂的多模态指令并生成高保真度的编辑？Query-Kontext 通过多模态“kontext”令牌桥接了视觉-语言模型（VLM）和扩散生成器，将推理任务卸载给VLM，并将合成任务保留给扩散模型——从而改进了文本到图像（T2I）的生成和指令驱动的编辑。\"\n}","chinese_challenges":"[\n  {\n    \"name\": \"AlexNet\",\n    \"year\": 2012,\n    \"task\": \"Image Classification\",\n    \"key_innovation\": \"Deep CNN, ReLU activation, Dropout\",\n    \"impact_score\": 9.5\n  },\n  {\n    \"name\": \"VGGNet\",\n    \"year\": 2014,\n    \"task\": \"Image Classification, Localization\",\n    \"key_innovation\": \"3x3 convolution kernels, deep architecture\",\n    \"impact_score\": 8.8\n  },\n  {\n    \"name\": \"ResNet\",\n    \"year\": 2015,\n    \"task\": \"Image Classification, Object Detection\",\n    \"key_innovation\": \"Residual connections (skip connections), solving vanishing gradient\",\n    \"impact_score\": 9.8\n  },\n  {\n    \"name\": \"Transformer (Attention Is All You Need)\",\n    \"year\": 2017,\n    \"task\": \"Sequence Modeling (NLP, later Vision)\",\n    \"key_innovation\": \"Self-attention mechanism, eliminating recurrence\",\n    \"impact_score\": 10.0\n  },\n  {\n    \"name\": \"ViT (Vision Transformer)\",\n    \"year\": 2020,\n    \"task\": \"Image Classification\",\n    \"key_innovation\": \"Applying pure Transformer architecture directly to image patches\",\n    \"impact_score\": 9.2\n  },\n  {\n    \"name\": \"Diffusion Models (DDPM)\",\n    \"year\": 2020,\n    \"task\": \"Generative Modeling\",\n    \"key_innovation\": \"Iterative denoising process for high-quality image synthesis\",\n    \"impact_score\": 9.7\n  }\n]","chinese_innovations":"{\n  \"核心创新\": [\n    \"多模态上下文（Multimodal kontext）：语义线索 + 粗粒度图像条件令牌，用于连接视觉语言模型（VLM）与扩散模型（diffusion）。\",\n    \"三阶段渐进式训练：轻量级扩散头部 → 扩展到预训练扩散模型 → 添加低级图像编码器 + 指令微调。\",\n    \"结合真实、合成和开放数据集的综合数据管线。\",\n    \"新颖的转折点：通过上下文令牌，将生成推理（VLM）与高质量合成（扩散模型）明确解耦。\"\n  ]\n}","chinese_experiments":"{\n  \"chinese_translation\": \"📊 结果：该方法与强大的统一基线（unified baselines）相当，在某些情况下甚至超越了针对特定任务的最先进（state-of-the-art）方法。论文中没有具体说明精确的数字改进或基准分数。\\n主要证明：通过“kontext tokens”解耦推理（reasoning）和合成（synthesis），可以获得具有竞争力或更优越的经验性能。\"\n}","chinese_insights":"{\n  \"insights\": [\n    {\n      \"key\": \"Future directions & applications\",\n      \"value\": \"探索空间显式上下文（每区域标记）或场景图条件，以实现更精细的控制和组合性。\"\n    },\n    {\n      \"key\": \"Future directions & applications\",\n      \"value\": \"整合多模态思维链（chain-of-thought）或更强的基础（grounding），以应对复杂的多主体编辑。\"\n    },\n    {\n      \"key\": \"Potential apps\",\n      \"value\": \"个性化图像编辑、用于内容创建和增强现实/虚拟现实（AR/VR）的多主体组合。\"\n    },\n    {\n      \"key\": \"Modular split analysis\",\n      \"value\": \"这种模块化拆分能否加快定制化和安全检查的速度？\"\n    }\n  ]\n}","summary":"**Introduction:** 🚀 Can one model both understand complex multimodal instructions and produce high‑fidelity edits? \nQuery‑Kontext bridges a vision‑language model (VLM) and a diffusion generator via multimodal “kontext” tokens, offloading reasoning to the VLM and reserving synthesis for diffusion — improving T2I and instruction‑driven editing.\n\n**Challenges:** 🎯 Key problems tackled:\n- Existing unified frameworks entangle multimoda...","analyzed_at":"2025-10-01T10:50:16.965Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:12.598Z","views":23,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"arxiv_2509.26641v1","views_at_archive":23}},{"id":"arxiv_2509.26642v1","title":"MLA: A Multisensory Language-Action Model for Multimodal Understanding\n  and Forecasting in Robotic Manipulation","authors":["Zhuoyang Liu","Jiaming Liu","Jiadong Xu","Nuowei Han","Chenyang Gu","Hao Chen","Kaichen Zhou","Renrui Zhang","Kai Chin Hsieh","Kun Wu","Zhengping Che","Jian Tang","Shanghang Zhang"],"abstract":"Vision-language-action models (VLAs) have shown generalization capabilities\nin robotic manipulation tasks by inheriting from vision-language models (VLMs)\nand learning action generation. Most VLA models focus on interpreting vision\nand language to generate actions, whereas robots must perceive and interact\nwithin the spatial-physical world. This gap highlights the need for a\ncomprehensive understanding of robotic-specific multisensory information, which\nis crucial for achieving complex and contact-rich control. To this end, we\nintroduce a multisensory language-action (MLA) model that collaboratively\nperceives heterogeneous sensory modalities and predicts future multisensory\nobjectives to facilitate physical world modeling. Specifically, to enhance\nperceptual representations, we propose an encoder-free multimodal alignment\nscheme that innovatively repurposes the large language model itself as a\nperception module, directly interpreting multimodal cues by aligning 2D images,\n3D point clouds, and tactile tokens through positional correspondence. To\nfurther enhance MLA's understanding of physical dynamics, we design a future\nmultisensory generation post-training strategy that enables MLA to reason about\nsemantic, geometric, and interaction information, providing more robust\nconditions for action generation. For evaluation, the MLA model outperforms the\nprevious state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex,\ncontact-rich real-world tasks, respectively, while also demonstrating improved\ngeneralization to unseen configurations. Project website:\nhttps://sites.google.com/view/open-mla","published":"2025-09-30T17:59:50Z","source":"arxiv","url":"http://arxiv.org/abs/2509.26642v1","analysis":{"introduction":"🚀 Question: Can robots truly feel and predict the physical world, not just see and read it?\nMLA introduces a multisensory language-action model that fuses vision, 3D, and tactile cues and predicts future multisensory objectives to unlock contact-rich robotic manipulation.","challenges":"🎯 Challenges:\n- Existing VLAs mainly interpret vision+language and ignore rich physical sensory signals.\n- Difficulty modeling contact-rich dynamics and multisensory temporal objectives.\n- Poor generalization to unseen spatial configurations in real-world manipulation.","innovations":"✨ Innovations:\n- Encoder-free multimodal alignment: repurposes a large language model as a perception module to align 2D images, 3D point clouds, and tactile tokens via positional correspondence.\n- Future multisensory generation post-training: trains the model to predict future multisensory objectives (semantic, geometric, interaction) to support action generation.","experiments":"📊 Experiment:\nMLA outperforms previous SOTA: +12% over 2D VLA and +24% over 3D VLA on complex, contact-rich real-world tasks — showing stronger task performance and better generalization to unseen configurations.","insights":"🤔 Insights (what's next?):\n- Research directions: integrate proprioception/force sensing and closed-loop real-time control; explore online adaptation and sim-to-real fine-tuning for safety.\n- Applications: dexterous assembly, surgical robotics, assistive manipulation. Could multisensory LLM perception become a new standard for embodied agents?","category":"robotics","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"\"🚀 问题：机器人能否真正感知并预测物理世界，而不仅仅是观察和识别它？MLA 引入了一个多感官语言-动作模型，它融合了视觉、3D 和触觉线索，并预测未来的多感官目标，从而实现接触密集型机器人操作。\"","chinese_challenges":"[\n  {\n    \"挑战\": [\n      \"现有的视觉-语言-动作模型（VLAs）主要侧重于解释视觉和语言信息，而忽略了丰富的物理感知信号。\",\n      \"难以对接触密集型动力学和多感官时间目标进行有效建模。\",\n      \"在现实世界操作中，对未见过的空间配置泛化能力较弱。\"\n    ]\n  }\n]","chinese_innovations":"[\n  {\n    \"innovations\": \"Encoder-free multimodal alignment: repurposes a large language model as a perception module to align 2D images, 3D point clouds, and tactile tokens via positional correspondence.\"\n  },\n  {\n    \"innovations\": \"Future multisensory generation post-training: trains the model to predict future multisensory objectives (semantic, geometric, interaction) to support action generation.\"\n  }\n]","chinese_experiments":"{\n\"translation\": \"📊 实验：MLA在复杂的、富接触的真实世界任务中，相比2D VLA性能提升了12%，相比3D VLA性能提升了24%——这表明其任务性能更强，并且对未见过的配置具有更好的泛化能力。\"\n}","chinese_insights":"{\n  \"translation\": \"🤔 洞察（下一步是什么？）：\\n- 研究方向：整合本体感觉/力传感和闭环实时控制；探索在线适应和用于安全的从模拟到现实的微调（sim-to-real fine-tuning）。\\n- 应用：灵巧装配、外科手术机器人、辅助操作。多感官大型语言模型（LLM）感知能否成为具身智能体（embodied agents）的新标准？\"\n}","summary":"**Introduction:** 🚀 Question: Can robots truly feel and predict the physical world, not just see and read it?\nMLA introduces a multisensory language-action model that fuses vision, 3D, and tactile cues and predicts future multisensory objectives to unlock contact-rich robotic manipulation.\n\n**Challenges:** 🎯 Challenges:\n- Existing VLAs mainly interpret vision+language and ignore rich physical sensory signals.\n- Difficulty modeling...","analyzed_at":"2025-10-01T10:08:50.959Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:12.598Z","views":1,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"arxiv_2509.26642v1","views_at_archive":1}}],"metadata":{"total_papers":10,"categories":{"robotics":3,"machine_learning":4,"natural_language_processing":3},"sources":{"huggingface":5,"arxiv":5},"average_score":9,"unique_keywords":["OceanGym","Multi-modal Large Language Models","MLLMs","underwater robotics","sonar","optical imaging","embodied AI","long-horizon planning","benchmark","retargeting","humanoid","interaction mesh","Laplacian deformation","loco-manipulation","contact preservation","reinforcement learning","proprioceptive policy","Unitree G1","data augmentation","OMOMO","LAFAN1","SIM(3) equivariance","shape completion","3D deep learning","point clouds","generalization","canonicalization"],"total_views":42,"created_at":"2025-10-01T14:12:20.274Z","source":"daily_update","auto_archived":true,"papers_archived":10,"total_papers_analyzed":25}}