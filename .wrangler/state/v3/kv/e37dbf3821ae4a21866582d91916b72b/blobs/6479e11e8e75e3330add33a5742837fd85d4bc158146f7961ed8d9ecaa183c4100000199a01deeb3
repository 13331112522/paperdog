{"date":"2025-10-01","papers":[{"id":"hf_oceangym__a_benchmark_environment_for_underwater_embodied_agents_1759311082964","title":"OceanGym: A Benchmark Environment for Underwater Embodied Agents","authors":[],"abstract":"We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:41:27.213Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;620b3bbb0668e435407c8d0a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;,&quot;fullname&quot;:&quot;Ningyu Zhang&quot;,&quot;name&quot;:&quot;Ningyu&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:30}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.930759847164154},&quot;editors&quot;:[&quot;Ningyu&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.26536&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af3&quot;,&quot;name&quot;:&quot;Yida Xue&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af4&quot;,&quot;name&quot;:&quot;Mingjun Mao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af5&quot;,&quot;name&quot;:&quot;Xiangyuan Ru&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af6&quot;,&quot;name&quot;:&quot;Yuqi Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af7&quot;,&quot;name&quot;:&quot;Baochang Ren&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af8&quot;,&quot;name&quot;:&quot;Shuofei Qiao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af9&quot;,&quot;name&quot;:&quot;Mengru Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afa&quot;,&quot;name&quot;:&quot;Shumin Deng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afb&quot;,&quot;name&quot;:&quot;Xinyu An&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afc&quot;,&quot;name&quot;:&quot;Ningyu Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afd&quot;,&quot;name&quot;:&quot;Ying Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afe&quot;,&quot;name&quot;:&quot;Huajun Chen&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/1Nye1TFWd66krDhvPXjQ1.png&quot;],&quot;publishedAt&quot;:&quot;2025-09-30T17:09:32.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T01:11:27.179Z&quot;,&quot;title&quot;:&quot;OceanGym: A Benchmark Environment for Underwater Embodied Agents&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;620b3bbb0668e435407c8d0a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Ningyu Zhang&quot;,&quot;user&quot;:&quot;Ningyu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We introduce OceanGym, the first comprehensive benchmark for ocean underwater\\nembodied agents, designed to advance AI in one of the most demanding real-world\\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\\nextreme perceptual and decision-making challenges, including low visibility,\\ndynamic ocean currents, making effective agent deployment exceptionally\\ndifficult. OceanGym encompasses eight realistic task domains and a unified\\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\\nintegrates perception, memory, and sequential decision-making. Agents are\\nrequired to comprehend optical and sonar data, autonomously explore complex\\nenvironments, and accomplish long-horizon objectives under these harsh\\nconditions. Extensive experiments reveal substantial gaps between\\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\\npersistent difficulty of perception, planning, and adaptability in ocean\\nunderwater environments. By providing a high-fidelity, rigorously designed\\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\\ntransferring these capabilities to real-world autonomous ocean underwater\\nvehicles, marking a decisive step toward intelligent agents capable of\\noperating in one of Earth's last unexplored frontiers. The code and data are\\navailable at https://github.com/OceanGPT/OceanGym.&quot;,&quot;upvotes&quot;:25,&quot;discussionId&quot;:&quot;68dc947d4159d1f2418f9aff&quot;,&quot;projectPage&quot;:&quot;https://oceangpt.github.io/OceanGym/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/OceanGPT/OceanGym&quot;,&quot;ai_summary&quot;:&quot;OceanGym is a benchmark for underwater embodied agents using Multi-modal Large Language Models to address challenges in perception, planning, and adaptability in harsh ocean environments.&quot;,&quot;ai_keywords&quot;:[&quot;Multi-modal Large Language Models&quot;,&quot;MLLMs&quot;,&quot;optical data&quot;,&quot;sonar data&quot;,&quot;sequential decision-making&quot;,&quot;embodied AI&quot;,&quot;autonomous ocean underwater vehicles&quot;],&quot;githubStars&quot;:25},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;620b3bbb0668e435407c8d0a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Ningyu Zhang&quot;,&quot;user&quot;:&quot;Ningyu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67f371b807a3da4558f803c1&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GilM_Vf65qvwuW5-uj9aG.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Twain Wu&quot;,&quot;user&quot;:&quot;1wtw1&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64db65ee1d19239f50674cbe&quot;,&quot;avatarUrl&quot;:&quot;/avatars/dd96cec8d0c10f52a89b25d65728738d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;xueyida&quot;,&quot;user&quot;:&quot;xyd123&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68d8fc00ff474874c83a1c99&quot;,&quot;avatarUrl&quot;:&quot;/avatars/17e3a2f5197274536bf68d949c5416db.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;huminclu&quot;,&quot;user&quot;:&quot;huminclu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;658ead753ce574ff3c339a64&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1dd7671e8af5e7241ef47a6de5503c53.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;sunnychenxiwang&quot;,&quot;user&quot;:&quot;sunnychenxiwang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6698c1c3157ceb76c48ff996&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2f1d732c4d9df4f5b554268ee1949dda.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;å¾æ­¥å¼º&quot;,&quot;user&quot;:&quot;Xubqpanda&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6736aebbbbc5d5471ee57218&quot;,&quot;avatarUrl&quot;:&quot;/avatars/45e3b017fc6a07e10a42b81cfa349b3f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yi Zhong&quot;,&quot;user&quot;:&quot;HongdouNI233&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;646449beeca41ed5029d1630&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7992d9a62e8d218ec3200d74af9ab5c4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xiang Chen&quot;,&quot;user&quot;:&quot;yyfenglin&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;684bc1be17ae31ba66171292&quot;,&quot;avatarUrl&quot;:&quot;/avatars/99ea28d4ed2ef6c4e35fd26c64472e49.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jingsheng Zheng&quot;,&quot;user&quot;:&quot;JohnsonZheng03&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68415d7b911d1b3135fcca88&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HE3ptFNTlvoWmG3p3f2Cs.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Qihailiantang&quot;,&quot;user&quot;:&quot;Qihailiantang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;679a01a99893a68681ef1847&quot;,&quot;avatarUrl&quot;:&quot;/avatars/17fe173acda467df2b90cca9e5f3c656.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;ye&quot;,&quot;user&quot;:&quot;haohaojun&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65cad52fd6c974694fc20b8e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/8232a7c5db590ed26751a47c45d481b8.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xinle Deng&quot;,&quot;user&quot;:&quot;Linear-Matrix-Probability&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:3,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6345aadf5efccdc07f1365a5&quot;,&quot;name&quot;:&quot;ZhejiangUniversity&quot;,&quot;fullname&quot;:&quot;Zhejiang University&quot;}}\"> Papers arxiv:2509.26536","published":"2025-10-01","source":"huggingface","url":"https://huggingface.co/papers/2509.26536","analysis":{"introduction":"ğŸš€ What if autonomous agents could explore the deep sea like rovers explore Mars?\nOceanGym is the first comprehensive benchmark for underwater embodied agents â€” 8 realistic task domains + a unified MLLM-driven agent framework. Designed to push robust AI for AUVs. https://github.com/OceanGPT/OceanGym","challenges":"ğŸ¯ Key challenges OceanGym tackles:\n- Low visibility & noisy sensors that hinder perception\n- Dynamic ocean currents complicating decision-making and control\n- Multimodal (optical + sonar) longâ€‘horizon tasks requiring memory and planning","innovations":"âœ¨ What OceanGym introduces:\n- A highâ€‘fidelity benchmark with 8 realistic underwater task domains\n- A unified, MLLM-driven agent framework integrating perception, memory, sequential decision-making\n- Tasks demanding optical+sonar fusion and longâ€‘horizon autonomous exploration\nNovelty: first comprehensive underwater embodied benchmark.","experiments":"ğŸ“Š Quantitative result: Not specified in the paper.\nQualitative proof: Extensive experiments reveal substantial gaps between stateâ€‘ofâ€‘theâ€‘art MLLM-driven agents and human experts, showing persistent difficulty in perception, planning, and adaptability. Code/data: github.com/OceanGPT/OceanGym","insights":"ğŸ¤” What's next?\n- Improve simâ€‘toâ€‘real transfer and dedicated sonarâ€“vision fusion models\n- Explore curriculum/hierarchical planning for longâ€‘horizon underwater tasks\nApplications: autonomous oceanographic surveys, search & rescue, infrastructure inspection. Could OceanGym speed real-world AUV deployment?","keywords":["OceanGym","Multi-modal Large Language Models","MLLMs","underwater robotics","sonar","optical imaging","embodied AI","long-horizon planning","benchmark"],"category":"robotics","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"translation\": \"ğŸš€ å¦‚æœè‡ªä¸»æ™ºèƒ½ä½“èƒ½åƒæ¼«æ¸¸è½¦æ¢ç´¢ç«æ˜Ÿä¸€æ ·æ¢ç´¢æ·±æµ·ï¼Œé‚£ä¼šæ€æ ·ï¼ŸOceanGymæ˜¯é¦–ä¸ªé’ˆå¯¹æ°´ä¸‹å…·èº«æ™ºèƒ½ä½“çš„ç»¼åˆåŸºå‡†â€”â€”å®ƒåŒ…å«8ä¸ªç°å®ä»»åŠ¡é¢†åŸŸå’Œä¸€ä¸ªç»Ÿä¸€çš„MLLMé©±åŠ¨çš„æ™ºèƒ½ä½“æ¡†æ¶ã€‚æ—¨åœ¨æ¨åŠ¨è‡ªä¸»æ°´ä¸‹èˆªè¡Œå™¨ï¼ˆAUVsï¼‰çš„é²æ£’ï¼ˆç¨³å¥ï¼‰AIå‘å±•ã€‚https://github.com/OceanGPT/OceanGym\"\n}","chinese_challenges":"\"OceanGym åº”å¯¹çš„å…³é”®æŒ‘æˆ˜ï¼š\\n- ä½èƒ½è§åº¦ä¸ä¼ æ„Ÿå™¨å™ªå£°ï¼Œä¸¥é‡é˜»ç¢äº†ç¯å¢ƒæ„ŸçŸ¥ã€‚\\n- åŠ¨æ€æ´‹æµä½¿å†³ç­–å’Œæ§åˆ¶è¿‡ç¨‹å˜å¾—å¤æ‚ã€‚\\n- éœ€è¦è®°å¿†å’Œè§„åˆ’çš„å¤šæ¨¡æ€ï¼ˆå…‰å­¦ + å£°å‘ï¼‰é•¿ç¨‹ä»»åŠ¡ã€‚\"","chinese_innovations":"{\n  \"innovations\": [\n    {\n      \"title\": \"OceanGym å¼•å…¥äº†ä»€ä¹ˆï¼š\",\n      \"points\": [\n        \"ä¸€ä¸ªé«˜ä¿çœŸåŸºå‡†ï¼ŒåŒ…å« 8 ä¸ªé€¼çœŸçš„æ°´ä¸‹ä»»åŠ¡é¢†åŸŸ\",\n        \"ä¸€ä¸ªç»Ÿä¸€çš„ã€ç”± MLLMï¼ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼‰é©±åŠ¨çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé›†æˆäº†æ„ŸçŸ¥ã€è®°å¿†å’Œåºåˆ—å†³ç­–èƒ½åŠ›\",\n        \"è¦æ±‚å…‰å­¦ä¸å£°çº³èåˆä»¥åŠé•¿å‘¨æœŸè‡ªä¸»æ¢ç´¢çš„ä»»åŠ¡\"\n      ],\n      \"novelty\": \"æ–°é¢–æ€§ï¼šé¦–ä¸ªå…¨é¢çš„æ°´ä¸‹å…·èº«åŸºå‡†ã€‚\"\n    }\n  ]\n}","chinese_experiments":"{\n  \"å®šé‡ç»“æœ\": \"è®ºæ–‡ä¸­æœªæ˜ç¡®è¯´æ˜ã€‚\",\n  \"å®šæ€§è¯æ®\": \"å¹¿æ³›çš„å®éªŒæ­ç¤ºäº†æœ€å…ˆè¿›çš„ MLLM é©±åŠ¨æ™ºèƒ½ä½“ä¸äººç±»ä¸“å®¶ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œè¡¨æ˜å…¶åœ¨æ„ŸçŸ¥ã€è§„åˆ’å’Œé€‚åº”æ€§æ–¹é¢ä»é¢ä¸´æŒç»­çš„å›°éš¾ã€‚\",\n  \"ä»£ç /æ•°æ®\": \"github.com/OceanGPT/OceanGym\"\n}","chinese_insights":"{\n  \"insights\": {\n    \"next_steps\": [\n      \"Improve sim-to-real transfer and dedicated sonarâ€“vision fusion models\",\n      \"Explore curriculum/hierarchical planning for long-horizon underwater tasks\"\n    ],\n    \"applications\": [\n      \"autonomous oceanographic surveys\",\n      \"search & rescue\",\n      \"infrastructure inspection\"\n    ],\n    \"question\": \"Could OceanGym speed real-world AUV deployment?\"\n  }\n}","summary":"**Introduction:** ğŸš€ What if autonomous agents could explore the deep sea like rovers explore Mars?\nOceanGym is the first comprehensive benchmark for underwater embodied agents â€” 8 realistic task domains + a unified MLLM-driven agent framework. Designed to push robust AI for AUVs. https://github.com/OceanGPT/OceanGym\n\n**Challenges:** ğŸ¯ Key challenges OceanGym tackles:\n- Low visibility & noisy sensors that hinder perception\n- Dynami...","analyzed_at":"2025-10-01T11:19:11.261Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:22.964Z","views":0,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"hf_oceangym__a_benchmark_environment_for_underwater_embodied_agents_1759311082964","views_at_archive":0}},{"id":"hf_thinking_sparks___emergent_attention_heads_in_reasoning_models_during_post_training_1759311093567","title":"Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training","authors":[],"abstract":"Modern large reasoning models boost performance via post-training methods like supervised fine-tuning and reinforcement learningâ€”but how these gains arise internally has remained a mystery.In our new work, we peel back the hood using circuit analysis to reveal:\\n(1) Post-training triggers the emergence of specialized attention heads that coordinate to carry out structured reasoning.(2) Different training regimes steer different dynamics: SFT/distillation yield stable, cumulative reasoning heads, while policy optimization leads to iterative activation and pruning.(3) Strong reasoning heads boost advanced problem solvingâ€”but risk â€œoverthinkingâ€ errors on simpler tasks, revealing a tension between complexity and reliability.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T01:57:19.002Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;64587be872b60ae7a3817858&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;,&quot;fullname&quot;:&quot;Minbyul Jeong&quot;,&quot;name&quot;:&quot;Minbyul&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3}},&quot;numEdits&quot;:1,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9207460284233093},&quot;editors&quot;:[&quot;Minbyul&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25758&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc8a1f4159d1f2418f9a64&quot;,&quot;name&quot;:&quot;Yein Park&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8a1f4159d1f2418f9a65&quot;,&quot;name&quot;:&quot;Minbyul Jeong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8a1f4159d1f2418f9a66&quot;,&quot;name&quot;:&quot;Jaewoo Kang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-30T04:23:43.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:26:22.625Z&quot;,&quot;title&quot;:&quot;Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\\n Post Training&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64587be872b60ae7a3817858&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Minbyul Jeong&quot;,&quot;user&quot;:&quot;Minbyul&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;The remarkable capabilities of modern large reasoning models are largely\\nunlocked through post-training techniques such as supervised fine-tuning and\\nreinforcement learning. However, the architectural mechanisms behind such\\nimprovements remain largely opaque. In this work, we use circuit analysis to\\ndemonstrate that post-training for complex reasoning sparks the emergence of\\nnovel, functionally specialized attention heads. These heads collectively\\nsupport structured reasoning and computation. Our comparative analysis across\\nQwen families and DeepSeek-distilled model reveals that these emergent heads\\nevolve differently under different training regimes. Distillation and SFT\\nfoster a cumulative addition of stable reasoning heads. In contrast, group\\nrelative policy optimization operates in a dynamic search mode: relatively few\\nattention heads are iteratively activated, evaluated, and pruned, with their\\nsurvival closely tracking fluctuations in the task reward signal. Furthermore,\\nwe find that controllable think on/off models do not possess dedicated thinking\\nheads. Instead, turning off explicit reasoning triggers a broader-but less\\nefficient-set of compensatory heads. Through ablation and qualitative analyses,\\nwe connect these circuit-level dynamics to a crucial performance trade-off:\\nstrengthened heads enable sophisticated problem-solving strategies for\\ndifficult problems but can also introduce over-thinking failure modes, such as\\ncalculation errors or logical loops on simpler tasks. These findings connect\\ncircuit-level dynamics to macro-level performance, identifying an inherent\\ntension where complex reasoning comes at the cost of elementary computations.\\nMore broadly, our work points to future directions for training policy design,\\nemphasizing the need to balance the development of effective reasoning\\nstrategies with the assurance of reliable, flawless execution.&quot;,&quot;upvotes&quot;:16,&quot;discussionId&quot;:&quot;68dc8a1f4159d1f2418f9a67&quot;,&quot;ai_summary&quot;:&quot;Post-training techniques like supervised fine-tuning and reinforcement learning lead to the emergence of specialized attention heads that support structured reasoning, with different training regimes affecting their evolution and performance.&quot;,&quot;ai_keywords&quot;:[&quot;supervised fine-tuning&quot;,&quot;reinforcement learning&quot;,&quot;circuit analysis&quot;,&quot;attention heads&quot;,&quot;structured reasoning&quot;,&quot;Qwen families&quot;,&quot;DeepSeek-distilled model&quot;,&quot;group relative policy optimization&quot;,&quot;think on/off models&quot;,&quot;ablation analysis&quot;,&quot;qualitative analysis&quot;,&quot;over-thinking failure modes&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;64587be872b60ae7a3817858&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Minbyul Jeong&quot;,&quot;user&quot;:&quot;Minbyul&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64e5c8e594aa0690321f6b29&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IW5LRzmPcAM-dri8taMN7.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yein Park&quot;,&quot;user&quot;:&quot;P-YI&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67d790ece3396bf0c9298194&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e027b2cc0bc9ffe5df18e61ca460d422.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;JongMyung Jung&quot;,&quot;user&quot;:&quot;hiwaryi&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;670f5c3f642f58673b1f435a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wa_TulGokgVzMN0-_GlBB.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;YewonCho&quot;,&quot;user&quot;:&quot;doldol330&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;60f8435644e75317cc02ed51&quot;,&quot;avatarUrl&quot;:&quot;/avatars/68b7fc077fe2bda6607b1c470add8140.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jungwoo Park&quot;,&quot;user&quot;:&quot;affjljoo3581&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64bb1bb412d00c4589c03bf7&quot;,&quot;avatarUrl&quot;:&quot;/avatars/63d04790c71c72e824cfcf70fc9433e6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hyeongsoon&quot;,&quot;user&quot;:&quot;hhs8746&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;5efbdc4ac3896117eab961a9&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1602668910270-5efbdc4ac3896117eab961a9.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Data Mining and Information Systems Lab&quot;,&quot;user&quot;:&quot;dmis-lab&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66fd2cf65e36a0ed66f32f68&quot;,&quot;avatarUrl&quot;:&quot;/avatars/781c4bdd887f4137058eca18203dc7d5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;monet&quot;,&quot;user&quot;:&quot;monet9736&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67348f009551fdc242064ef4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/38023d6d3c2ea12434ed55aca7ca1c3e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jueon Park&quot;,&quot;user&quot;:&quot;bioai96&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68dc97a4224de59d4b965edd&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ktZW8Hf-nvB2eSBB8GgrG.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yewon Cho&quot;,&quot;user&quot;:&quot;YewonCho&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65431e2dbf8a6039fbddb4c6&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7df981f6f9bda5f8af89b6f0637340f6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;lim suhyeon&quot;,&quot;user&quot;:&quot;yeonsue&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6306df0ed37ce67e0e53e3f1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/3cbe5762d1e1ccf259f4bbed9fc1fa00.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hyeon Hwang&quot;,&quot;user&quot;:&quot;Hyeoni&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6621bc39e774284ec1742ab8&quot;,&quot;name&quot;:&quot;KoreaUniversity&quot;,&quot;fullname&quot;:&quot;Korea University&quot;}}\"> Papers arxiv:2509.25758","published":"2025-10-01","source":"huggingface","url":"https://huggingface.co/papers/2509.25758","analysis":{"introduction":"ğŸš€ What if post-training doesn't just tweak models but sparks \\","challenges":"ğŸ¯ Problems addressed:\n- The internal mechanisms by which post-training (SFT/RL/distillation) improves reasoning are opaque.\n- Different post-training regimes and their circuit-level effects are not well understood.\n- Strong reasoning can harm reliability (\\","innovations":"âœ¨ Key methods & novelty:\n- Applied circuit analysis to transformer attention to identify specialized \\","experiments":"ğŸ“Š Experiments & main proof:\n- Main demonstration: post-training triggers emergence of specialized attention heads that coordinate structured reasoning; SFT/distillation add stable heads while policy optimization iteratively activates and prunes them; disabling explicit thinking leads to broader, less efficient compensatory heads; stronger heads improve hard tasks but can cause overthinking.\n- Quantitative numbers (e.g., %improvement): Not specified in the paper.","insights":"ğŸ¤” What's next?:\n- Research directions: explore adaptive head gating or regularization that preserves complex reasoning while preventing overthinking; design training objectives that balance reward-driven search with stability of useful heads.\n- Broader applications: safer, controllable reasoning agents; modular model designs that switch \\","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"translation\": \"ğŸš€ è¯•æƒ³ä¸€ä¸‹ï¼Œå¦‚æœåè®­ç»ƒï¼ˆpost-trainingï¼‰ä¸ä»…ä»…æ˜¯å¾®è°ƒæ¨¡å‹ï¼Œè€Œæ˜¯èƒ½æ¿€å‘\"\n}","chinese_challenges":"{\n  \"challenges\": \"ğŸ¯ è§£å†³çš„é—®é¢˜ï¼š\\n- è®­ç»ƒåï¼ˆSFT/RL/è’¸é¦ï¼‰æ”¹è¿›æ¨ç†èƒ½åŠ›çš„å†…éƒ¨æœºåˆ¶ä¸é€æ˜ã€‚\\n- ä¸åŒçš„è®­ç»ƒåæ–¹æ¡ˆåŠå…¶åœ¨ç”µè·¯å±‚é¢çš„å½±å“å°šä¸å®Œå…¨æ¸…æ¥šã€‚\\n- å¼ºå¤§çš„æ¨ç†èƒ½åŠ›å¯èƒ½ä¼šæŸå®³å¯é æ€§ï¼ˆ\"\n}","chinese_innovations":"\"âœ¨ å…³é”®æ–¹æ³•ä¸åˆ›æ–°ç‚¹ï¼šåº”ç”¨ç”µè·¯åˆ†æï¼ˆCircuit Analysisï¼‰æŠ€æœ¯å¯¹Transformeræ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œæ·±å…¥å‰–æï¼Œæ—¨åœ¨è¯†åˆ«ä¸“é—¨åŒ–çš„...\"","chinese_experiments":"{\n  \"title\": \"å®éªŒä¸ä¸»è¦è¯æ˜\",\n  \"sections\": [\n    {\n      \"heading\": \"ä¸»è¦è®ºè¯\",\n      \"content\": \"è®­ç»ƒåï¼ˆpost-trainingï¼‰ä¼šè§¦å‘ä¸“ä¸šåŒ–æ³¨æ„åŠ›å¤´ï¼ˆspecialized attention headsï¼‰çš„å‡ºç°ï¼Œè¿™äº›æ³¨æ„åŠ›å¤´ååŒè¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼›SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰/è’¸é¦ï¼ˆdistillationï¼‰å¢åŠ äº†ç¨³å®šçš„æ³¨æ„åŠ›å¤´ï¼Œè€Œç­–ç•¥ä¼˜åŒ–ï¼ˆpolicy optimizationï¼‰åˆ™è¿­ä»£åœ°æ¿€æ´»å’Œä¿®å‰ªè¿™äº›æ³¨æ„åŠ›å¤´ï¼›ç¦ç”¨æ˜¾å¼æ€è€ƒï¼ˆexplicit thinkingï¼‰ä¼šå¯¼è‡´å‡ºç°æ›´å¹¿æ³›ã€æ•ˆç‡è¾ƒä½çš„è¡¥å¿æ€§æ³¨æ„åŠ›å¤´ï¼ˆcompensatory headsï¼‰ï¼›æ›´å¼ºçš„æ³¨æ„åŠ›å¤´èƒ½æ”¹è¿›å›°éš¾ä»»åŠ¡çš„è¡¨ç°ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´è¿‡åº¦æ€è€ƒï¼ˆoverthinkingï¼‰ã€‚\"\n    },\n    {\n      \"heading\": \"å®šé‡æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œæ”¹è¿›ç™¾åˆ†æ¯”ï¼‰\",\n      \"content\": \"è®ºæ–‡ä¸­æœªå…·ä½“è¯´æ˜ã€‚\"\n    }\n  ]\n}","chinese_insights":"{\n  \"analysis_type\": \"Research Insights Translation\",\n  \"source_language\": \"English\",\n  \"target_language\": \"Simplified Chinese\",\n  \"translation\": {\n    \"title\": \"ğŸ¤” æœªæ¥æ–¹å‘/ä¸‹ä¸€æ­¥ç ”ç©¶ï¼š\",\n    \"sections\": [\n      {\n        \"header\": \"ç ”ç©¶æ–¹å‘ï¼š\",\n        \"points\": [\n          \"æ¢ç´¢è‡ªé€‚åº”å¤´éƒ¨é—¨æ§ï¼ˆhead gatingï¼‰æˆ–æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ—¢èƒ½ä¿ç•™å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œåˆèƒ½é˜²æ­¢æ¨¡å‹â€œè¿‡åº¦æ€è€ƒâ€ï¼ˆoverthinkingï¼‰ï¼›\",\n          \"è®¾è®¡è®­ç»ƒç›®æ ‡ï¼Œä»¥å¹³è¡¡å¥–åŠ±é©±åŠ¨çš„æœç´¢è¿‡ç¨‹ä¸æœ‰æ•ˆå¤´éƒ¨ï¼ˆæ¨¡å—ï¼‰çš„ç¨³å®šæ€§ã€‚\"\n        ]\n      },\n      {\n        \"header\": \"æ›´å¹¿æ³›çš„åº”ç”¨ï¼š\",\n        \"points\": [\n          \"æ›´å®‰å…¨ã€å¯æ§çš„æ¨ç†æ™ºèƒ½ä½“ï¼›\",\n          \"å¯åˆ‡æ¢çš„æ¨¡å—åŒ–æ¨¡å‹è®¾è®¡ã€‚\"\n        ]\n      }\n    ]\n  }\n}","summary":"**Introduction:** ğŸš€ What if post-training doesn't just tweak models but sparks \\\n\n**Challenges:** ğŸ¯ Problems addressed:\n- The internal mechanisms by which post-training (SFT/RL/distillation) improves reasoning are opaque.\n- Different post-training regimes and their circuit-level effects are not well understood.\n- Strong reasoning can harm reliability (\\","analyzed_at":"2025-10-01T13:20:45.191Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:33.567Z","views":0,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"hf_thinking_sparks___emergent_attention_heads_in_reasoning_models_during_post_training_1759311093567","views_at_archive":0}},{"id":"hf_dparallel__learnable_parallel_decoding_for_dllms_1759311102154","title":"dParallel: Learnable Parallel Decoding for dLLMs","authors":[],"abstract":"We present dParallel, a novel method that unlocks the inherent parallelism of dLLMs for fast sampling. Our paper, code, models, and dataset are all available now!\\nCode: https://github.com/czg1225/dParallelPaper: https://arxiv.org/pdf/2509.26488Model: https://huggingface.co/Zigeng/dParallel-LLaDA-8B-instructData: https://huggingface.co/datasets/Zigeng/dParallel_LLaDA_Distill_Data\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:04:29.960Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;65811eeaa2284a018e51f1ba&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;,&quot;fullname&quot;:&quot;Zigeng Chen&quot;,&quot;name&quot;:&quot;Zigeng&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:7}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7858399152755737},&quot;editors&quot;:[&quot;Zigeng&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.26488&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a47&quot;,&quot;name&quot;:&quot;Zigeng Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a48&quot;,&quot;name&quot;:&quot;Gongfan Fang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a49&quot;,&quot;name&quot;:&quot;Xinyin Ma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a4a&quot;,&quot;name&quot;:&quot;Ruonan Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a4b&quot;,&quot;name&quot;:&quot;Xinchao Wang&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/65811eeaa2284a018e51f1ba/0NK-gFnfpLThLyS0atsLH.mp4&quot;],&quot;publishedAt&quot;:&quot;2025-09-30T16:32:52.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:34:29.943Z&quot;,&quot;title&quot;:&quot;dParallel: Learnable Parallel Decoding for dLLMs&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;65811eeaa2284a018e51f1ba&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zigeng Chen&quot;,&quot;user&quot;:&quot;Zigeng&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Diffusion large language models (dLLMs) have recently drawn considerable\\nattention within the research community as a promising alternative to\\nautoregressive generation, offering parallel token prediction and lower\\ninference latency. Yet, their parallel decoding potential remains largely\\nunderexplored, as existing open-source models still require nearly token-length\\ndecoding steps to ensure performance. To address this, we introduce dParallel,\\na simple and effective method that unlocks the inherent parallelism of dLLMs\\nfor fast sampling. We identify that the key bottleneck to parallel decoding\\narises from the sequential certainty convergence for masked tokens. Building on\\nthis insight, we introduce the core of our approach: certainty-forcing\\ndistillation, a novel training strategy that distills the model to follow its\\noriginal sampling trajectories while enforcing it to achieve high certainty on\\nmasked tokens more rapidly and in parallel. Extensive experiments across\\nvarious benchmarks demonstrate that our method can dramatically reduce the\\nnumber of decoding steps while maintaining performance. When applied to the\\nLLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on\\nGSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP\\nbenchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup\\nwhile maintaining accuracy. Our code is available at\\nhttps://github.com/czg1225/dParallel&quot;,&quot;upvotes&quot;:13,&quot;discussionId&quot;:&quot;68dc88d74159d1f2418f9a4c&quot;,&quot;githubRepo&quot;:&quot;https://github.com/czg1225/dParallel&quot;,&quot;ai_summary&quot;:&quot;dParallel is a method that enhances the parallel decoding of diffusion large language models, significantly reducing decoding steps without compromising performance.&quot;,&quot;ai_keywords&quot;:[&quot;diffusion large language models&quot;,&quot;dLLMs&quot;,&quot;autoregressive generation&quot;,&quot;parallel token prediction&quot;,&quot;parallel decoding&quot;,&quot;masked tokens&quot;,&quot;certainty-forcing distillation&quot;,&quot;LLaDA-8B-Instruct&quot;,&quot;GSM8K&quot;,&quot;MBPP benchmark&quot;],&quot;githubStars&quot;:7},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;65811eeaa2284a018e51f1ba&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zigeng Chen&quot;,&quot;user&quot;:&quot;Zigeng&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;640ebdfefdeaae139086f4d8&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/640ebdfefdeaae139086f4d8/2N94gbHubplYD8njmUTPf.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Yuanshi&quot;,&quot;user&quot;:&quot;Yuanshi&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6683fc5344a65be1aab25dc0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e13cde3f87b59e418838d702807df3b5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;hjkim&quot;,&quot;user&quot;:&quot;hojie11&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6684b1a9986286e214df1e03&quot;,&quot;avatarUrl&quot;:&quot;/avatars/515efb62b0ec923ea525a90ea7aa9221.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;XinyinMa&quot;,&quot;user&quot;:&quot;XinyinHorseee&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6486fb33570a419f41a882e4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/860a42074439a23c629cd23851ae4da6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ruonan Yu&quot;,&quot;user&quot;:&quot;roseannelexie&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6342796a0875f2c99cfd313b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/98575092404c4197b20c929a6499a015.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuseung \\&quot;Phillip\\&quot; Lee&quot;,&quot;user&quot;:&quot;phillipinseoul&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;668e740f1173ab43d9d9ed5e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/caa9b47c2a5f6d6d679759b8b234a0ab.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zeqing Wang&quot;,&quot;user&quot;:&quot;INV-WZQ&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;620783f24e28382272337ba4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/620783f24e28382272337ba4/zkUveQPNiDfYjgGhuFErj.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;GuoLiangTang&quot;,&quot;user&quot;:&quot;Tommy930&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;634cfebc350bcee9bed20a4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xingyi Yang&quot;,&quot;user&quot;:&quot;adamdad&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6270324ebecab9e2dcf245de&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6270324ebecab9e2dcf245de/cMbtWSasyNlYc9hvsEEzt.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kye Gomez&quot;,&quot;user&quot;:&quot;kye&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;646a1939c37ca1e12308fe81&quot;,&quot;avatarUrl&quot;:&quot;/avatars/752e9d86018e7d33ad8bcd741203fd86.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Gongfan Fang&quot;,&quot;user&quot;:&quot;Vinnnf&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6508ab2b349930913196378b&quot;,&quot;name&quot;:&quot;NationalUniversityofSingapore&quot;,&quot;fullname&quot;:&quot;National University of Singapore&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png&quot;}}\"> Papers arxiv:2509.26488","published":"2025-10-01","source":"huggingface","url":"https://huggingface.co/papers/2509.26488","analysis":{"introduction":"ğŸš€ What if diffusion LLMs could sample 8â€“10x faster? \ndParallel unlocks the parallel decoding potential of dLLMs by using a learnable distillation that forces rapid certainty on masked tokens â€” slashing decoding steps and cutting latency for real-time and research use. ğŸ”¥","challenges":"ğŸ¯ Key problems tackled:\n- dLLMs still need nearly token-length decoding steps, limiting speed.\n- Sequential certainty convergence of masked tokens blocks true parallel decoding.\n- Hard to reduce steps without hurting accuracy or changing sampling trajectories.","innovations":"âœ¨ Core ideas:\n- dParallel: a method to enable fast parallel sampling in dLLMs.\n- Certainty-forcing distillation: distill models to follow original sampling trajectories while forcing high certainty on masked tokens faster and in parallel.\n- Simple, training-based fix that unlocks inherent dLLM parallelism.","experiments":"ğŸ“Š Standout result: Applied to LLaDA-8B-Instruct, dParallel reduces decoding steps on GSM8K from 256 â†’ 30 (â‰ˆ8.5Ã— speedup) while maintaining performance. \n(Also: MBPP 256 â†’ 24 â‰ˆ10.5Ã— speedup with accuracy preserved.)","insights":"ğŸ¤” Where this could go next:\n- Explore scaling to larger dLLMs and multimodal diffusion LMs to bring low-latency sampling to more tasks.\n- Combine with hardware-aware scheduling / quantization for on-device, interactive LMs.\nCould this make diffusion-based assistants practical in real time? ğŸš€","category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"ç¿»è¯‘å¤±è´¥ï¼Œè¯·æŸ¥çœ‹è‹±æ–‡åŸæ–‡ / Translation failed, please see English original","chinese_challenges":"{\n  \"challenges\": \"ğŸ¯ è§£å†³çš„å…³é”®é—®é¢˜ï¼š\\n- dLLMsï¼ˆè§£ç å™¨å¤§è¯­è¨€æ¨¡å‹ï¼‰ä»ç„¶éœ€è¦æ¥è¿‘äº token é•¿åº¦çš„è§£ç æ­¥éª¤ï¼Œé™åˆ¶äº†é€Ÿåº¦ã€‚\\n- æ©ç  token çš„é¡ºåºç¡®å®šæ€§æ”¶æ•›é˜»ç¢äº†çœŸæ­£çš„å¹¶è¡Œè§£ç ã€‚\\n- å¾ˆéš¾åœ¨ä¸æŸå®³å‡†ç¡®æ€§æˆ–æ”¹å˜é‡‡æ ·è½¨è¿¹çš„æƒ…å†µä¸‹å‡å°‘è§£ç æ­¥éª¤ã€‚\"\n}","chinese_innovations":"{\n  \"æ ¸å¿ƒæ€æƒ³\": [\n    \"dParallelï¼šä¸€ç§åœ¨ dLLM ä¸­å®ç°å¿«é€Ÿå¹¶è¡Œé‡‡æ ·çš„æ–¹æ³•ã€‚\",\n    \"å¼ºåˆ¶ç¡®å®šæ€§è’¸é¦ï¼šè’¸é¦æ¨¡å‹ä»¥éµå¾ªåŸå§‹é‡‡æ ·è½¨è¿¹ï¼ŒåŒæ—¶æ›´å¿«ã€æ›´å¹¶è¡Œåœ°å¯¹è¢«æ©ç çš„ token å¼ºåˆ¶æ–½åŠ é«˜ç¡®å®šæ€§ã€‚\",\n    \"ä¸€ç§ç®€å•çš„ã€åŸºäºè®­ç»ƒçš„ä¿®æ­£æ–¹æ³•ï¼Œèƒ½å¤Ÿé‡Šæ”¾ dLLM å›ºæœ‰çš„å¹¶è¡Œæ€§ã€‚\"\n  ]\n}","chinese_experiments":"[\n  {\n    \"title\": \"å®éªŒç»“æœåˆ†æ\",\n    \"content\": \"ğŸ“Š çªå‡ºæˆæœï¼šå°† dParallel åº”ç”¨äº LLaDA-8B-Instruct æ¨¡å‹ï¼Œåœ¨ GSM8K æ•°æ®é›†ä¸Šï¼Œè§£ç æ­¥éª¤ä» 256 å‡å°‘åˆ° 30ï¼ˆçº¦ 8.5 å€åŠ é€Ÿï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ã€‚ \\nï¼ˆæ­¤å¤–ï¼šåœ¨ MBPP æ•°æ®é›†ä¸Šï¼Œè§£ç æ­¥éª¤ä» 256 å‡å°‘åˆ° 24ï¼Œå®ç°äº†çº¦ 10.5 å€åŠ é€Ÿï¼Œä¸”å‡†ç¡®ç‡å¾—ä»¥ä¿æŒã€‚ï¼‰\"\n  }\n]","chinese_insights":"{\n  \"translation\": \"ğŸ¤” æ¥ä¸‹æ¥çš„å‘å±•æ–¹å‘ï¼š\\n- æ¢ç´¢æ‰©å±•åˆ°æ›´å¤§çš„dLLMsï¼ˆæ‰©æ•£å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å’Œå¤šæ¨¡æ€æ‰©æ•£å¼è¯­è¨€æ¨¡å‹ï¼Œå°†ä½å»¶è¿Ÿé‡‡æ ·å¸¦åˆ°æ›´å¤šä»»åŠ¡ä¸­ã€‚\\n- ä¸ç¡¬ä»¶æ„ŸçŸ¥è°ƒåº¦/é‡åŒ–ç›¸ç»“åˆï¼Œå®ç°è®¾å¤‡ä¸Šçš„äº¤äº’å¼è¯­è¨€æ¨¡å‹ã€‚\\nè¿™èƒ½å¦ä½¿åŸºäºæ‰©æ•£çš„åŠ©æ‰‹åœ¨å®æ—¶åº”ç”¨ä¸­å˜å¾—å®ç”¨ï¼ŸğŸš€\"\n}","summary":"**Introduction:** ğŸš€ What if diffusion LLMs could sample 8â€“10x faster? \ndParallel unlocks the parallel decoding potential of dLLMs by using a learnable distillation that forces rapid certainty on masked tokens â€” slashing decoding steps and cutting latency for real-time and research use. ğŸ”¥\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- dLLMs still need nearly token-length decoding steps, limiting speed.\n- Sequential certainty convergen...","analyzed_at":"2025-10-01T14:12:20.197Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:42.154Z","views":0,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"hf_dparallel__learnable_parallel_decoding_for_dllms_1759311102154","views_at_archive":0}},{"id":"hf_dc_videogen__efficient_video_generation_with_deep_compression_video_autoencoder_1759311090899","title":"DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder","authors":[],"abstract":"DC-VideoGen is a new post-training framework for accelerating video diffusion models. Key features:ğŸ¬ Supports video generation up to 2160Ã—3840 resolution on a single H100 GPUâš¡ Delivers 14.8Ã— faster inference than the base modelğŸ’° 230Ã— lower training cost compared to training from scratch (only 10 H100 GPU days for Wan-2.1-14B)\\nDC-VideoGen is built on two core innovations:\\n\\nDeep Compression Video Autoencoder (DC-AE-V): a new family of deep compression autoencoders for video data, providing 32Ã—/64Ã— spatial and 4Ã— temporal compression.\\nAE-Adapt-V: a robust adaptation strategy that enables rapid and stable transfer of pre-trained video diffusion models to DC-AE-V.\\n\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T01:51:53.102Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;650e2b14c945dfc9386a7e28&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;,&quot;fullname&quot;:&quot;Han Cai&quot;,&quot;name&quot;:&quot;han-cai&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:11}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7540821433067322},&quot;editors&quot;:[&quot;han-cai&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25182&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a36&quot;,&quot;name&quot;:&quot;Junyu Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a37&quot;,&quot;name&quot;:&quot;Wenkun He&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a38&quot;,&quot;name&quot;:&quot;Yuchao Gu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a39&quot;,&quot;name&quot;:&quot;Yuyang Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3a&quot;,&quot;name&quot;:&quot;Jincheng Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3b&quot;,&quot;name&quot;:&quot;Junsong Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3c&quot;,&quot;name&quot;:&quot;Dongyun Zou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3d&quot;,&quot;name&quot;:&quot;Yujun Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3e&quot;,&quot;name&quot;:&quot;Zhekai Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3f&quot;,&quot;name&quot;:&quot;Muyang Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a40&quot;,&quot;name&quot;:&quot;Haocheng Xi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a41&quot;,&quot;name&quot;:&quot;Ligeng Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a42&quot;,&quot;name&quot;:&quot;Enze Xie&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a43&quot;,&quot;name&quot;:&quot;Song Han&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a44&quot;,&quot;name&quot;:&quot;Han Cai&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/650e2b14c945dfc9386a7e28/bgdM9-uqshQ2p3QyNbj82.jpeg&quot;],&quot;publishedAt&quot;:&quot;2025-09-29T17:59:31.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:21:53.066Z&quot;,&quot;title&quot;:&quot;DC-VideoGen: Efficient Video Generation with Deep Compression Video\\n Autoencoder&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;650e2b14c945dfc9386a7e28&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Han Cai&quot;,&quot;user&quot;:&quot;han-cai&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We introduce DC-VideoGen, a post-training acceleration framework for\\nefficient video generation. DC-VideoGen can be applied to any pre-trained video\\ndiffusion model, improving efficiency by adapting it to a deep compression\\nlatent space with lightweight fine-tuning. The framework builds on two key\\ninnovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal\\ntemporal design that achieves 32x/64x spatial and 4x temporal compression while\\npreserving reconstruction quality and generalization to longer videos; and (ii)\\nAE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer\\nof pre-trained models into the new latent space. Adapting the pre-trained\\nWan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100\\nGPU. The accelerated models achieve up to 14.8x lower inference latency than\\ntheir base counterparts without compromising quality, and further enable\\n2160x3840 video generation on a single GPU. Code:\\nhttps://github.com/dc-ai-projects/DC-VideoGen.&quot;,&quot;upvotes&quot;:20,&quot;discussionId&quot;:&quot;68dc88d34159d1f2418f9a45&quot;,&quot;ai_summary&quot;:&quot;DC-VideoGen accelerates video generation by adapting pre-trained diffusion models to a deep compression latent space, reducing inference latency and enabling high-resolution video generation.&quot;,&quot;ai_keywords&quot;:[&quot;DC-VideoGen&quot;,&quot;video diffusion model&quot;,&quot;deep compression latent space&quot;,&quot;lightweight fine-tuning&quot;,&quot;Deep Compression Video Autoencoder&quot;,&quot;chunk-causal temporal design&quot;,&quot;AE-Adapt-V&quot;,&quot;Wan-2.1-14B model&quot;,&quot;inference latency&quot;,&quot;high-resolution video generation&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;646189cd5dba83471db2af58&quot;,&quot;avatarUrl&quot;:&quot;/avatars/d5a1549af336cb5f1fa5622250d38a73.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;JUNYU CHEN&quot;,&quot;user&quot;:&quot;cjy2003&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66ce751a8ec9fda2cf5a9e85&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c17093ca81dad007b3e50bae503955a7.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Haocheng Xi&quot;,&quot;user&quot;:&quot;xihc-ucb&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63129589bbaa385279d1826e&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Muyang Li&quot;,&quot;user&quot;:&quot;Lmxyy&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;650e2b14c945dfc9386a7e28&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Han Cai&quot;,&quot;user&quot;:&quot;han-cai&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;641d8bacd526196afc12766d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/73f7b2d86a7bf27940bec2b1f199d71b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shang Yang&quot;,&quot;user&quot;:&quot;Shangy&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66a156136609d2b2b0f6353a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/fc6850b5fc437269bf0870f6a6cdcf40.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yujun Lin&quot;,&quot;user&quot;:&quot;synxlin&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6579569562d3ac18171cf9cb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bf3bc3130b5db3594e810624a936f721.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yecheng Wu&quot;,&quot;user&quot;:&quot;gbcfchc&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63021630a35b21bd8a53305a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7a7e8b39749eda61e57d8a1908726558.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Gu Yuchao&quot;,&quot;user&quot;:&quot;guyuchao&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;645b5b09bc7518912e1f9733&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4d35f728b41f93881a9b67c337f4d1df.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chen&quot;,&quot;user&quot;:&quot;Lawrence-cj&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;634ce90e741a5e37886a19e3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0d1579039136b37db5b67282b0a34c33.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Syang&quot;,&quot;user&quot;:&quot;Andyson&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;650c74e5d439bbbbadfcfbbe&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1002835739dbb90214b5f2824a7c8c1f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;YU Jincheng&quot;,&quot;user&quot;:&quot;yujincheng08&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6683fc5344a65be1aab25dc0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e13cde3f87b59e418838d702807df3b5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;hjkim&quot;,&quot;user&quot;:&quot;hojie11&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;60262b67268c201cdc8b7d43&quot;,&quot;name&quot;:&quot;nvidia&quot;,&quot;fullname&quot;:&quot;NVIDIA&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png&quot;}}\"> Papers arxiv:2509.25182","published":"2025-10-01","source":"huggingface","url":"https://huggingface.co/papers/2509.25182","analysis":{"introduction":"ğŸš€ Want 4K video generation on a single H100 GPU? \nDC-VideoGen adapts pre-trained video diffusion models into a deep-compression latent space to run up to 2160Ã—3840, cut inference latency by 14.8Ã—, and slash training cost â€” unlocking high-res video generation with light fine-tuning.","challenges":"ğŸ¯ Key problems tackled:\n- Large compute & memory: high-res video diffusion is slow and GPUâ€‘heavy.\n- Training cost: training video diffusion from scratch is prohibitively expensive.\n- Latent inefficiency: prior latents limit resolution or temporal scaling.","innovations":"âœ¨ Core innovations:\n- Deep Compression Video Autoencoder (DC-AE-V) with chunk-causal temporal design.\n- DC-AE-V achieves 32Ã—/64Ã— spatial + 4Ã— temporal compression while preserving reconstructions.\n- AE-Adapt-V: a lightweight, stable adaptation strategy to transfer pre-trained models into the compressed latent.","experiments":"ğŸ“Š Main empirical result:\nAchieved up to 14.8Ã— faster inference vs the base model. Adapting Wan-2.1-14B took only 10 H100 GPU-days (â‰ˆ230Ã— lower training cost than training from scratch) and enabled 2160Ã—3840 video generation on one H100.","insights":"ğŸ¤” What's next?\n- Explore integrating DC-AE-V latents with text-to-video or conditional diffusion for efficient high-res generation.\n- Apply deep-compression latents to streaming, real-time editing, or edge deployment. Could this make high-quality video gen ubiquitous on single GPUs?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"\"ğŸš€ æƒ³è¦åœ¨å•ä¸ªH100 GPUä¸Šå®ç°4Kè§†é¢‘ç”Ÿæˆå—ï¼ŸDC-VideoGenå°†é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹é€‚é…åˆ°ä¸€ä¸ªæ·±åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œä»è€Œèƒ½å¤Ÿè¿è¡Œé«˜è¾¾2160Ã—3840çš„åˆ†è¾¨ç‡ï¼Œå°†æ¨ç†å»¶è¿Ÿå‡å°‘14.8å€ï¼Œå¹¶å¤§å¹…é™ä½è®­ç»ƒæˆæœ¬â€”â€”é€šè¿‡è½»é‡çº§çš„å¾®è°ƒå³å¯è§£é”é«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆã€‚\"","chinese_challenges":"\"æ ¸å¿ƒæŒ‘æˆ˜ï¼š\\n- å¤§è®¡ç®—é‡ä¸å†…å­˜éœ€æ±‚ï¼šé«˜åˆ†è¾¨ç‡è§†é¢‘æ‰©æ•£æ¨¡å‹è¿è¡Œç¼“æ…¢ï¼Œä¸”å¯¹GPUèµ„æºæ¶ˆè€—å·¨å¤§ã€‚\\n- è®­ç»ƒæˆæœ¬ï¼šä»é›¶å¼€å§‹è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æˆæœ¬æå…¶æ˜‚è´µã€‚\\n- æ½œåœ¨è¡¨ç¤ºæ•ˆç‡ä½ä¸‹ï¼šç°æœ‰çš„æ½œåœ¨è¡¨ç¤ºï¼ˆLatentsï¼‰é™åˆ¶äº†æ¨¡å‹åœ¨åˆ†è¾¨ç‡æˆ–æ—¶é—´ç»´åº¦ä¸Šçš„æ‰©å±•èƒ½åŠ›ã€‚\"","chinese_innovations":"[\n  {\n    \"æ ¸å¿ƒåˆ›æ–°ç‚¹\": [\n      \"æ·±åº¦å‹ç¼©è§†é¢‘è‡ªç¼–ç å™¨ (DC-AE-V)ï¼Œé‡‡ç”¨å—å› æœï¼ˆchunk-causalï¼‰æ—¶é—´è®¾è®¡ã€‚\",\n      \"DC-AE-V å®ç°äº† 32 å€/64 å€ç©ºé—´å‹ç¼©å’Œ 4 å€æ—¶é—´å‹ç¼©ï¼ŒåŒæ—¶ä¿æŒäº†é‡å»ºè´¨é‡ã€‚\",\n      \"AE-Adapt-Vï¼šä¸€ç§è½»é‡çº§ã€ç¨³å®šçš„é€‚åº”ç­–ç•¥ï¼Œç”¨äºå°†é¢„è®­ç»ƒæ¨¡å‹è¿ç§»åˆ°å‹ç¼©åçš„æ½œåœ¨ç©ºé—´ã€‚\"\n    ]\n  }\n]","chinese_experiments":"","chinese_insights":"{\n  \"ğŸ¤” æœªæ¥å±•æœ›\": [\n    \"æ¢ç´¢å°† DC-AE-V éšå˜é‡ï¼ˆlatentsï¼‰ä¸æ–‡ç”Ÿè§†é¢‘ï¼ˆtext-to-videoï¼‰æˆ–æ¡ä»¶æ‰©æ•£æ¨¡å‹é›†æˆï¼Œä»¥å®ç°é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å†…å®¹ç”Ÿæˆã€‚\",\n    \"å°†æ·±åº¦å‹ç¼©çš„éšå˜é‡åº”ç”¨äºæµåª’ä½“ã€å®æ—¶ç¼–è¾‘æˆ–è¾¹ç¼˜éƒ¨ç½²åœºæ™¯ã€‚\",\n    \"è¿™æ˜¯å¦èƒ½ä½¿é«˜è´¨é‡è§†é¢‘ç”Ÿæˆåœ¨å•å— GPU ä¸Šå®ç°æ™®åŠåŒ–ï¼Ÿ\"\n  ]\n}","summary":"**Introduction:** ğŸš€ Want 4K video generation on a single H100 GPU? \nDC-VideoGen adapts pre-trained video diffusion models into a deep-compression latent space to run up to 2160Ã—3840, cut inference latency by 14.8Ã—, and slash training cost â€” unlocking high-res video generation with light fine-tuning.\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Large compute & memory: high-res video diffusion is slow and GPUâ€‘heavy.\n- Training cost: t...","analyzed_at":"2025-10-01T12:58:10.197Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:30.899Z","views":0,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"hf_dc_videogen__efficient_video_generation_with_deep_compression_video_autoencoder_1759311090899","views_at_archive":0}},{"id":"hf_truthrl__incentivizing_truthful_llms_via_reinforcement_learning_1759311079944","title":"TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning","authors":[],"abstract":"While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy---models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs. We also experiment with more complicated reward designs, such as knowledge-enhanced and reasoning-enhanced variants, and show that a simple ternary reward scheme generally performs better. Moreover, we find the improvement of TruthRL arises from enhancing the capability of LLMs to recognize their knowledge boundary, hence avoiding being overly conservative as the baselines are. Further analysis confirms that TruthRL is robust to hallucination-baiting questions and more confident in producing accurate responses.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:26:26.198Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;6526307af06ac0cf9a922e86&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;,&quot;fullname&quot;:&quot;Zhepei Wei&quot;,&quot;name&quot;:&quot;weizhepei&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9195460677146912},&quot;editors&quot;:[&quot;weizhepei&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ğŸ‘&quot;,&quot;users&quot;:[&quot;sytmr&quot;,&quot;TianHongZXY&quot;,&quot;WZDavid&quot;],&quot;count&quot;:3},{&quot;reaction&quot;:&quot;ğŸ”¥&quot;,&quot;users&quot;:[&quot;WZDavid&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25760&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aa7&quot;,&quot;name&quot;:&quot;Zhepei Wei&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aa8&quot;,&quot;name&quot;:&quot;Xiao Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aa9&quot;,&quot;name&quot;:&quot;Kai Sun&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aaa&quot;,&quot;name&quot;:&quot;Jiaqi Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aab&quot;,&quot;name&quot;:&quot;Rulin Shao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aac&quot;,&quot;name&quot;:&quot;Sean Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aad&quot;,&quot;name&quot;:&quot;Mohammad Kachuee&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aae&quot;,&quot;name&quot;:&quot;Teja Gollapudi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aaf&quot;,&quot;name&quot;:&quot;Tony Liao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab0&quot;,&quot;name&quot;:&quot;Nicolas Scheffer&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab1&quot;,&quot;name&quot;:&quot;Rakesh Wanga&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab2&quot;,&quot;name&quot;:&quot;Anuj Kumar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab3&quot;,&quot;name&quot;:&quot;Yu Meng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab4&quot;,&quot;name&quot;:&quot;Wen-tau Yih&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab5&quot;,&quot;name&quot;:&quot;Xin Luna Dong&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-30T04:25:17.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:56:26.188Z&quot;,&quot;title&quot;:&quot;TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6526307af06ac0cf9a922e86&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhepei Wei&quot;,&quot;user&quot;:&quot;weizhepei&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;While large language models (LLMs) have demonstrated strong performance on\\nfactoid question answering, they are still prone to hallucination and\\nuntruthful responses, particularly when tasks demand information outside their\\nparametric knowledge. Indeed, truthfulness requires more than accuracy --\\nmodels must also recognize uncertainty and abstain when unsure to avoid\\nhallucinations. This presents a fundamental challenge for existing methods:\\napproaches that optimize for accuracy often amplify hallucinations, while those\\nthat encourage abstention can become overly conservative, sacrificing correct\\nanswers. Both extremes ultimately compromise truthfulness. In this work, we\\npresent TruthRL, a general reinforcement learning (RL) framework that directly\\noptimizes the truthfulness of LLMs. Specifically, we implement TruthRL using\\nGRPO with a simple yet effective ternary reward that distinguishes correct\\nanswers, hallucinations, and abstentions. It incentivizes models to reduce\\nhallucinations not only by providing correct responses, but also by enabling\\nabstention when uncertain, thereby improving truthfulness. Extensive\\nexperiments across four knowledge-intensive benchmarks show that, compared to\\nvanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves\\ntruthfulness by 21.1%, with consistent gains across various backbone models\\n(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth\\nablation study demonstrates that vanilla accuracy-driven methods, such as\\nsupervised fine-tuning or RL with a binary reward, struggle to balance factual\\ncorrectness and uncertainty. In contrast, our proposed truthfulness-driven\\nTruthRL achieves strong performance in both accuracy and truthfulness,\\nunderscoring the importance of learning objective design for developing\\ntruthful LLMs.&quot;,&quot;upvotes&quot;:34,&quot;discussionId&quot;:&quot;68dc90b84159d1f2418f9ab6&quot;,&quot;ai_summary&quot;:&quot;TruthRL, a reinforcement learning framework, enhances the truthfulness of large language models by balancing accuracy and abstention, significantly reducing hallucinations and improving performance across benchmarks.&quot;,&quot;ai_keywords&quot;:[&quot;large language models&quot;,&quot;LLMs&quot;,&quot;hallucination&quot;,&quot;untruthful responses&quot;,&quot;parametric knowledge&quot;,&quot;truthfulness&quot;,&quot;reinforcement learning&quot;,&quot;RL&quot;,&quot;GRPO&quot;,&quot;ternary reward&quot;,&quot;abstention&quot;,&quot;accuracy-driven methods&quot;,&quot;supervised fine-tuning&quot;,&quot;binary reward&quot;,&quot;knowledge-intensive benchmarks&quot;,&quot;Qwen&quot;,&quot;Llama&quot;,&quot;retrieval&quot;,&quot;non-retrieval setups&quot;,&quot;ablation study&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;6526307af06ac0cf9a922e86&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhepei Wei&quot;,&quot;user&quot;:&quot;weizhepei&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62ea79dd01ed9b0e8f61ccd3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chengsong Huang&quot;,&quot;user&quot;:&quot;ChengsongHuang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;617aec6f6f37340367d5d7a1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/afa58f39896c5caef512675450c7d6ce.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yu Meng&quot;,&quot;user&quot;:&quot;yumeng5&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6602787b1827b6d37ee527be&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f69c1779b4347a042dad1a0d962145af.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tzu-Han Lin&quot;,&quot;user&quot;:&quot;hank0316&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6587e5a4b2177de3967ff434&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f2dfbc44eb2bff8d8d66d26db8539708.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shuyao Xu&quot;,&quot;user&quot;:&quot;Tim-Xu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66080ddac201aee890e5efeb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/dae8f44b2eda9d1950efaa10a5aa986f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jing Chen&quot;,&quot;user&quot;:&quot;jingchen6688&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;623b290048f658f28aef79f7&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1648044277149-noauth.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xinyu Zhu&quot;,&quot;user&quot;:&quot;TianHongZXY&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6633f39185b05e9a8e7c549c&quot;,&quot;avatarUrl&quot;:&quot;/avatars/ee4df68daee8b6637d7ad86cba29cc2f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;shiyu&quot;,&quot;user&quot;:&quot;sytmr&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;664e88ad8ab2524c036c3d2f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/HItpTu75SFqA5ouOMKzVb.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhining Liu&quot;,&quot;user&quot;:&quot;ZhiningLiu1998&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;619e6657cc04eadf54fa5d2d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c78c54767c63c75a9f6783ffa78a98fa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wei-Lin Chen&quot;,&quot;user&quot;:&quot;wlchen&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6590a65b89f1ff0463828e53&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4ab424eede2fe9c114252b1e5dd1ba25.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;sizhe&quot;,&quot;user&quot;:&quot;sizhe04&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;647a248bed75e95d3e98e3d6&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e10e2f8516d1451fd85e17b5a0ba978d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yaochen Zhu&quot;,&quot;user&quot;:&quot;yaochenzhu&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:2,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;5e63d8713071d5be688861b8&quot;,&quot;name&quot;:&quot;facebook&quot;,&quot;fullname&quot;:&quot;AI at Meta&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png&quot;}}\"> Papers arxiv:2509.25760","published":"2025-10-01","source":"huggingface","url":"https://huggingface.co/papers/2509.25760","analysis":{"introduction":"ğŸš€ Question: How do we make LLMs say â€œI donâ€™t knowâ€ instead of confidently hallucinating?\nTruthRL is an RL framework (GRPO + a simple ternary reward) that trains models to answer correctly or abstain, cutting hallucinations and improving truthfulness across backbones & setups.","challenges":"ğŸ¯ Key problems tackled:\n- Models hallucinate on facts outside parametric knowledge.\n- Accuracy-driven training amplifies confident but wrong answers.\n- Abstention-focused methods become overly conservative and lose correct answers.","innovations":"âœ¨ Core innovations:\n- TruthRL: an RL framework optimizing truthfulness directly.\n- Implemented with GRPO and a ternary reward (correct / hallucination / abstain).\n- Novelty: reward explicitly balances giving correct answers and abstaining when uncertain.","experiments":"ğŸ“Š Results: TruthRL reduced hallucinations by 28.9% and improved truthfulness by 21.1% across four knowledge-intensive benchmarks, with consistent gains across backbones (Qwen, Llama) and both retrieval & non-retrieval setups â€” showing the ternary rewardâ€™s practical impact.","insights":"ğŸ¤” Whatâ€™s next?\n- Explore combining TruthRL with stronger calibrated uncertainty estimators or multi-turn dialogue to improve abstention in interactive settings.\n- Broader applications: high-stakes domains (medicine, law) and retrieval-augmented systems where avoiding hallucination matters most.\nCould TruthRL become a standard for safe QA?","category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"\"ğŸš€ é—®é¢˜ï¼šæˆ‘ä»¬å¦‚ä½•è®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¯´å‡ºâ€œæˆ‘ä¸çŸ¥é“â€ï¼Œè€Œä¸æ˜¯è‡ªä¿¡åœ°ç»™å‡ºå¹»è§‰å†…å®¹ï¼ŸTruthRLæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼ˆç”±GRPOå’Œä¸€ä¸ªç®€å•çš„ä¸‰å…ƒå¥–åŠ±æœºåˆ¶æ„æˆï¼‰ï¼Œå®ƒè®­ç»ƒæ¨¡å‹è¦ä¹ˆæ­£ç¡®å›ç­”ï¼Œè¦ä¹ˆé€‰æ‹©å¼ƒæƒï¼Œä»è€Œå‡å°‘å¹»è§‰ç°è±¡ï¼Œå¹¶åœ¨å„ç§åŸºç¡€æ¨¡å‹å’Œé…ç½®ä¸­æé«˜çœŸå®æ€§ã€‚\"","chinese_challenges":"{\n  \"å…³é”®æŒ‘æˆ˜\": [\n    \"æ¨¡å‹å¯¹è¶…å‡ºå…¶å‚æ•°åŒ–çŸ¥è¯†èŒƒå›´çš„äº‹å®äº§ç”Ÿâ€œå¹»è§‰â€ï¼ˆè™šå‡ä¿¡æ¯ï¼‰ã€‚\",\n    \"ä»¥å‡†ç¡®ç‡ï¼ˆç²¾åº¦ï¼‰ä¸ºå¯¼å‘çš„è®­ç»ƒä¼šåŠ å‰§æ¨¡å‹å¯¹é”™è¯¯ç­”æ¡ˆçš„è¿‡åº¦è‡ªä¿¡ã€‚\",\n    \"ä»¥å¼ƒæƒï¼ˆæ‹’ç»å›ç­”ï¼‰ä¸ºæ ¸å¿ƒçš„æ–¹æ³•å¾€å¾€è¿‡äºä¿å®ˆï¼Œå¯¼è‡´é”™å¤±æ­£ç¡®çš„ç­”æ¡ˆã€‚\"\n  ]\n}","chinese_innovations":"\"æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š\\n- TruthRLï¼šä¸€ç§ç›´æ¥ä¼˜åŒ–â€œçœŸå®æ€§â€ï¼ˆtruthfulnessï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ã€‚\\n- ä½¿ç”¨GRPOç®—æ³•å’Œä¸‰å…ƒå¥–åŠ±æœºåˆ¶ï¼ˆæ­£ç¡® / å¹»è§‰ / å¼ƒæƒï¼‰å®ç°ã€‚\\n- åˆ›æ–°ç‚¹ï¼šå¥–åŠ±æœºåˆ¶æ˜ç¡®åœ°å¹³è¡¡äº†â€œç»™å‡ºæ­£ç¡®ç­”æ¡ˆâ€å’Œâ€œåœ¨ä¸ç¡®å®šæ—¶é€‰æ‹©å¼ƒæƒâ€è¿™ä¸¤ç§è¡Œä¸ºã€‚\"","chinese_experiments":"{\n  \"analysis_type\": \"research_summary\",\n  \"model_name\": \"TruthRL\",\n  \"metrics\": [\n    {\n      \"metric\": \"hallucinations_reduction\",\n      \"value\": 0.289,\n      \"unit\": \"percentage\"\n    },\n    {\n      \"metric\": \"truthfulness_improvement\",\n      \"value\": 0.211,\n      \"unit\": \"percentage\"\n    }\n  ],\n  \"scope\": {\n    \"benchmarks\": \"four knowledge-intensive benchmarks\",\n    \"backbones\": [\n      \"Qwen\",\n      \"Llama\"\n    ],\n    \"setups\": [\n      \"retrieval\",\n      \"non-retrieval\"\n    ]\n  },\n  \"conclusion\": \"The ternary reward mechanism demonstrated practical impact through consistent gains across diverse configurations.\"\n}","chinese_insights":"{\n  \"insights\": [\n    \"ğŸ¤” ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ\",\n    \"æ¢ç´¢å°†TruthRLä¸æ›´å¼ºçš„æ ¡å‡†ä¸ç¡®å®šæ€§ä¼°è®¡å™¨æˆ–å¤šè½®å¯¹è¯ç›¸ç»“åˆï¼Œä»¥æ”¹è¿›åœ¨äº¤äº’å¼è®¾ç½®ä¸­çš„æ‹’ç»å›ç­”ï¼ˆAbstentionï¼‰èƒ½åŠ›ã€‚\",\n    \"æ›´å¹¿æ³›çš„åº”ç”¨ï¼šåœ¨é«˜é£é™©é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹ï¼‰ä»¥åŠé¿å…â€œå¹»è§‰â€ï¼ˆHallucinationï¼‰è‡³å…³é‡è¦çš„æ£€ç´¢å¢å¼ºç³»ç»Ÿä¸­ã€‚\",\n    \"TruthRLèƒ½å¦æˆä¸ºå®‰å…¨é—®ç­”ï¼ˆQAï¼‰çš„æ ‡å‡†ï¼Ÿ\"\n  ]\n}","summary":"**Introduction:** ğŸš€ Question: How do we make LLMs say â€œI donâ€™t knowâ€ instead of confidently hallucinating?\nTruthRL is an RL framework (GRPO + a simple ternary reward) that trains models to answer correctly or abstain, cutting hallucinations and improving truthfulness across backbones & setups.\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Models hallucinate on facts outside parametric knowledge.\n- Accuracy-driven training amplifies c...","analyzed_at":"2025-10-01T11:16:31.375Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:19.945Z","views":0,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"hf_truthrl__incentivizing_truthful_llms_via_reinforcement_learning_1759311079944","views_at_archive":0}},{"id":"arxiv_2509.26633v1","title":"OmniRetarget: Interaction-Preserving Data Generation for Humanoid\n  Whole-Body Loco-Manipulation and Scene Interaction","authors":["Lujie Yang","Xiaoyu Huang","Zhen Wu","Angjoo Kanazawa","Pieter Abbeel","Carmelo Sferrazza","C. Karen Liu","Rocky Duan","Guanya Shi"],"abstract":"A dominant paradigm for teaching humanoid robots complex skills is to\nretarget human motions as kinematic references to train reinforcement learning\n(RL) policies. However, existing retargeting pipelines often struggle with the\nsignificant embodiment gap between humans and robots, producing physically\nimplausible artifacts like foot-skating and penetration. More importantly,\ncommon retargeting methods neglect the rich human-object and human-environment\ninteractions essential for expressive locomotion and loco-manipulation. To\naddress this, we introduce OmniRetarget, an interaction-preserving data\ngeneration engine based on an interaction mesh that explicitly models and\npreserves the crucial spatial and contact relationships between an agent, the\nterrain, and manipulated objects. By minimizing the Laplacian deformation\nbetween the human and robot meshes while enforcing kinematic constraints,\nOmniRetarget generates kinematically feasible trajectories. Moreover,\npreserving task-relevant interactions enables efficient data augmentation, from\na single demonstration to different robot embodiments, terrains, and object\nconfigurations. We comprehensively evaluate OmniRetarget by retargeting motions\nfrom OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour\ntrajectories that achieve better kinematic constraint satisfaction and contact\npreservation than widely used baselines. Such high-quality data enables\nproprioceptive RL policies to successfully execute long-horizon (up to 30\nseconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained\nwith only 5 reward terms and simple domain randomization shared by all tasks,\nwithout any learning curriculum.","published":"2025-09-30T17:59:02Z","source":"arxiv","url":"http://arxiv.org/abs/2509.26633v1","analysis":{"introduction":"ğŸš€ Want humanoid robots to move and interact as fluidly as humans? OmniRetarget is an interaction-preserving data engine that retargets human motion to robots by modeling contacts and scene relations â€” producing kinematically feasible, interaction-aware demos for RL. Benefits: better loco-manipulation and sim-to-real training.","challenges":"ğŸ¯ Key problems tackled:\n- Existing retargeting fails across the humanâ†”robot embodiment gap (footâ€‘skate, penetration).\n- Prior pipelines ignore rich humanâ€“object and humanâ€“environment interactions.\n- Limited data augmentation across robots/terrains/objects hampers robust RL.","innovations":"âœ¨ Core innovations:\n- Interaction mesh that explicitly encodes spatial/contact relationships between agent, terrain, and objects.\n- Retargeting via minimizing Laplacian deformation between human and robot meshes while enforcing kinematic constraints.\n- Interaction-preserving data augmentation to transfer a single demo across embodiments, terrains, and object configs.\nNovelty: explicit interaction mesh + Laplacian deformation for retargeting to preserve contacts and task geometry.","experiments":"ğŸ“Š Results: Retargeted motions from OMOMO, LAFAN1 and in-house MoCap to produce over 8 hours of trajectories with better kinematic-constraint satisfaction and contact preservation than common baselines; enabled proprioceptive RL to execute long-horizon (up to 30s) parkour and loco-manipulation on a Unitree G1, trained with only 5 reward terms and simple domain randomization.","insights":"ğŸ¤” Whatâ€™s next?\n- Research: adapt OmniRetarget for online/adaptive retargeting and multi-robot transfer, or combine with perception for closed-loop real-world interactions.\n- Applications: fast data generation for real-world humanoid deployment (assistive robots, warehouse manipulation, entertainment).\nCould interaction-preserving retargeting unlock more reliable sim-to-real loco-manipulation?","keywords":["retargeting","humanoid","interaction mesh","Laplacian deformation","loco-manipulation","contact preservation","reinforcement learning","proprioceptive policy","Unitree G1","data augmentation","OMOMO","LAFAN1"],"category":"robotics","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"\"ğŸš€ æƒ³è¦äººå½¢æœºå™¨äººåƒäººç±»ä¸€æ ·æµç•…åœ°ç§»åŠ¨å’Œäº¤äº’å—ï¼ŸOmniRetarget æ˜¯ä¸€ç§ä¿æŒäº¤äº’æ€§çš„æ•°æ®å¼•æ“ï¼Œå®ƒé€šè¿‡å»ºæ¨¡æ¥è§¦å’Œåœºæ™¯å…³ç³»ï¼Œå°†äººç±»åŠ¨ä½œé‡å®šå‘åˆ°æœºå™¨äººä¸Šï¼Œä»è€Œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”Ÿæˆè¿åŠ¨å­¦å¯è¡Œã€äº¤äº’æ„ŸçŸ¥çš„æ¼”ç¤ºæ•°æ®ã€‚ä¼˜åŠ¿åŒ…æ‹¬ï¼šæå‡è¿åŠ¨æ“ä½œèƒ½åŠ›ï¼ˆloco-manipulationï¼‰ä»¥åŠæ”¹è¿›è™šå®è¿ç§»ï¼ˆsim-to-realï¼‰è®­ç»ƒæ•ˆæœã€‚\"","chinese_challenges":"{\n  \"key_problems_tackled\": [\n    \"Existing retargeting fails across the humanâ†”robot embodiment gap (footâ€‘skate, penetration).\",\n    \"Prior pipelines ignore rich humanâ€“object and humanâ€“environment interactions.\",\n    \"Limited data augmentation across robots/terrains/objects hampers robust RL.\"\n  ]\n}","chinese_innovations":"{\n  \"innovations\": \"âœ¨ æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š\\n- äº¤äº’ç½‘æ ¼ï¼šæ˜¾å¼ç¼–ç æ™ºèƒ½ä½“ã€åœ°å½¢å’Œç‰©ä½“ä¹‹é—´çš„ç©ºé—´/æ¥è§¦å…³ç³»ã€‚\\n- é‡å®šå‘ï¼šé€šè¿‡æœ€å°åŒ–äººä½“å’Œæœºå™¨äººç½‘æ ¼ä¹‹é—´çš„æ‹‰æ™®æ‹‰æ–¯å½¢å˜ï¼ŒåŒæ—¶å¼ºåˆ¶æ‰§è¡Œè¿åŠ¨å­¦çº¦æŸæ¥å®ç°ã€‚\\n- ä¿æŒäº¤äº’çš„æ•°æ®å¢å¼ºï¼šå°†å•ä¸ªæ¼”ç¤ºè·¨è¶Šä¸åŒå®ä½“ã€åœ°å½¢å’Œç‰©ä½“é…ç½®è¿›è¡Œè¿ç§»ã€‚\\næ–°é¢–æ€§ï¼šæ˜¾å¼çš„äº¤äº’ç½‘æ ¼ + ç”¨äºé‡å®šå‘çš„æ‹‰æ™®æ‹‰æ–¯å½¢å˜ï¼Œä»¥ä¿æŒæ¥è§¦å’Œä»»åŠ¡å‡ ä½•ç»“æ„ã€‚\"\n}","chinese_experiments":"{\n  \"translation\": \"ğŸ“Š ç»“æœï¼šæˆ‘ä»¬å¯¹æ¥è‡ª OMOMOã€LAFAN1 å’Œå†…éƒ¨ MoCapï¼ˆè¿åŠ¨æ•æ‰ï¼‰æ•°æ®çš„åŠ¨ä½œè¿›è¡Œäº†é‡å®šå‘ï¼Œç”Ÿæˆäº†è¶…è¿‡ 8 å°æ—¶çš„è½¨è¿¹ã€‚ä¸å¸¸è§çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›è½¨è¿¹åœ¨æ»¡è¶³è¿åŠ¨å­¦çº¦æŸå’Œä¿æŒæ¥è§¦æ–¹é¢è¡¨ç°æ›´ä½³ï¼›è¿™ä½¿å¾—æœ¬ä½“æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆproprioceptive RLï¼‰èƒ½å¤Ÿåœ¨ Unitree G1 æœºå™¨äººä¸Šæ‰§è¡Œé•¿æ—¶ç¨‹ï¼ˆæœ€é•¿è¾¾ 30 ç§’ï¼‰çš„è·‘é…·å’Œç§»åŠ¨æ“ä½œä»»åŠ¡ï¼Œä¸”è®­ç»ƒä»…ä½¿ç”¨äº† 5 ä¸ªå¥–åŠ±é¡¹å’Œç®€å•çš„åŸŸéšæœºåŒ–ã€‚\"\n}","chinese_insights":"{\n  \"translation\": \"ğŸ¤” ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ\\n- ç ”ç©¶ï¼šä½¿ OmniRetarget é€‚åº”åœ¨çº¿/è‡ªé€‚åº”é‡å®šå‘å’Œå¤šæœºå™¨äººè¿ç§»ï¼Œæˆ–å°†å…¶ä¸æ„ŸçŸ¥ç›¸ç»“åˆï¼Œå®ç°é—­ç¯çš„çœŸå®ä¸–ç•Œäº¤äº’ã€‚\\n- åº”ç”¨ï¼šä¸ºçœŸå®ä¸–ç•Œçš„äººå½¢æœºå™¨äººéƒ¨ç½²ï¼ˆè¾…åŠ©æœºå™¨äººã€ä»“åº“æ“ä½œã€å¨±ä¹ï¼‰å¿«é€Ÿç”Ÿæˆæ•°æ®ã€‚\\nè¿™ç§ä¿æŒäº¤äº’çš„é‡å®šå‘èƒ½å¦è§£é”æ›´å¯é çš„â€œä»¿çœŸåˆ°ç°å®â€çš„è¿åŠ¨-æ“ä½œï¼ˆloco-manipulationï¼‰èƒ½åŠ›ï¼Ÿ\"\n}","summary":"**Introduction:** ğŸš€ Want humanoid robots to move and interact as fluidly as humans? OmniRetarget is an interaction-preserving data engine that retargets human motion to robots by modeling contacts and scene relations â€” producing kinematically feasible, interaction-aware demos for RL. Benefits: better loco-manipulation and sim-to-real training.\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Existing retargeting fails across the humanâ†”r...","analyzed_at":"2025-10-01T10:56:39.287Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:12.598Z","views":12,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"arxiv_2509.26633v1","views_at_archive":12}},{"id":"arxiv_2509.26626v1","title":"Recursive Self-Aggregation Unlocks Deep Thinking in Large Language\n  Models","authors":["Siddarth Venkatraman","Vineet Jain","Sarthak Mittal","Vedant Shah","Johan Obando-Ceron","Yoshua Bengio","Brian R. Bartoldson","Bhavya Kailkhura","Guillaume Lajoie","Glen Berseth","Nikolay Malkin","Moksh Jain"],"abstract":"Test-time scaling methods improve the capabilities of large language models\n(LLMs) by increasing the amount of compute used during inference to make a\nprediction. Inference-time compute can be scaled in parallel by choosing among\nmultiple independent solutions or sequentially through self-refinement. We\npropose Recursive Self-Aggregation (RSA), a test-time scaling method inspired\nby evolutionary methods that combines the benefits of both parallel and\nsequential scaling. Each step of RSA refines a population of candidate\nreasoning chains through aggregation of subsets to yield a population of\nimproved solutions, which are then used as the candidate pool for the next\niteration. RSA exploits the rich information embedded in the reasoning chains\n-- not just the final answers -- and enables bootstrapping from partially\ncorrect intermediate steps within different chains of thought. Empirically, RSA\ndelivers substantial performance gains with increasing compute budgets across\ndiverse tasks, model families and sizes. Notably, RSA enables\nQwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning\nmodels, including DeepSeek-R1 and o3-mini (high), while outperforming purely\nparallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning\nGym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the\nmodel to combine solutions via a novel aggregation-aware reinforcement learning\napproach yields significant performance gains. Code available at\nhttps://github.com/HyperPotatoNeo/RSA.","published":"2025-09-30T17:58:03Z","source":"arxiv","url":"http://arxiv.org/abs/2509.26626v1","analysis":{"introduction":"ğŸš€ Question: Can small LLMs 'think deeper' at inference time without more training?\nRSA (Recursive Self-Aggregation) is a test-time scaling method that iteratively aggregates and refines populations of reasoning chains to unlock deeper reasoningâ€”letting smaller models compete with bigger ones.","challenges":"ğŸ¯ Challenges:\n- Existing test-time scaling is typically either parallel (many independent tries) or sequential (self-refinement), not both.\n- Methods ignore partial correctness in intermediate reasoning steps across chains.\n- Inefficient use of inference compute for bootstrapping stronger solutions.","innovations":"âœ¨ Innovations:\n- Recursive Self-Aggregation (RSA): iteratively refines a population of candidate reasoning chains by aggregating subsets to produce improved candidates.\n- Exploits intermediate chain-of-thought content (not just final answers) to bootstrap.\n- Aggregation-aware reinforcement learning to train models to better combine solutions.","experiments":"ğŸ“Š Experiment (most compelling result):\nQwen3-4B-Instruct-2507 using RSA achieved competitive performance with larger reasoning models (DeepSeek-R1 and o3-mini (high)) and outperformed purely parallel and purely sequential scaling across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. Specific numeric gains: Not specified in the paper.","insights":"ğŸ¤” Insights & next steps:\n- Research: adapt RSA for multimodal or retrieval-augmented LLMs; develop adaptive subset-selection strategies or theory on when aggregation helps.\n- Applications: stronger reasoning in education/tutoring, code synthesis, and decision supportâ€”letting smaller models punch above their size. Could RSA enable cheaper, reliable 'deep thinking' at scale?","category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"title\": \"é€’å½’è‡ªèšåˆï¼ˆRSAï¼‰ï¼šè®©å°å‹LLMåœ¨æ¨ç†æ—¶â€œæ·±åº¦æ€è€ƒâ€\",\n  \"content\": \"ğŸš€ é—®é¢˜ï¼šå°å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¦åœ¨ä¸å¢åŠ è®­ç»ƒé‡çš„æƒ…å†µä¸‹ï¼Œåœ¨æ¨ç†æ—¶è¿›è¡Œâ€œæ›´æ·±å±‚æ¬¡çš„æ€è€ƒâ€ï¼Ÿ\\n\\né€’å½’è‡ªèšåˆï¼ˆRecursive Self-Aggregationï¼Œç®€ç§°RSAï¼‰æ˜¯ä¸€ç§æµ‹è¯•æ—¶ï¼ˆtest-timeï¼‰çš„æ‰©å±•æ–¹æ³•ï¼Œå®ƒé€šè¿‡è¿­ä»£åœ°èšåˆå’Œç²¾ç‚¼æ¨ç†é“¾çš„é›†åˆï¼ˆpopulationsï¼‰ï¼Œä»¥é‡Šæ”¾æ›´æ·±å±‚æ¬¡çš„æ¨ç†èƒ½åŠ›â€”â€”ä»è€Œä½¿å°å‹æ¨¡å‹èƒ½å¤Ÿä¸å¤§å‹æ¨¡å‹ç«äº‰ã€‚\"\n}","chinese_challenges":"{\n  \"challenges\": \"æŒ‘æˆ˜ï¼š\"\n}","chinese_innovations":"{\n  \"innovations\": \"âœ¨ åˆ›æ–°ç‚¹ï¼š\\n- é€’å½’è‡ªèšåˆï¼ˆRecursive Self-Aggregation, RSAï¼‰ï¼šé€šè¿‡èšåˆå€™é€‰æ¨ç†é“¾çš„å­é›†ï¼Œè¿­ä»£åœ°ç²¾ç‚¼æ¨ç†é“¾çš„ç¾¤ä½“ï¼Œä»¥äº§ç”Ÿæ”¹è¿›çš„å€™é€‰ã€‚ \\n- åˆ©ç”¨ä¸­é—´çš„æ€ç»´é“¾å†…å®¹ï¼ˆè€Œä¸ä»…ä»…æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰è¿›è¡Œè‡ªä¸¾ï¼ˆbootstrapï¼‰ã€‚\\n- èšåˆæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆAggregation-aware reinforcement learningï¼‰æ¥è®­ç»ƒæ¨¡å‹æ›´å¥½åœ°ç»“åˆè§£å†³æ–¹æ¡ˆã€‚\"\n}","chinese_experiments":"{\n  \"experiment\": \"Qwen3-4B-Instruct-2507 using RSA achieved competitive performance with larger reasoning models (DeepSeek-R1 and o3-mini (high)) and outperformed purely parallel and purely sequential scaling across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. Specific numeric gains: Not specified in the paper.\"\n}","chinese_insights":"[\n  {\n    \"insight_type\": \"Research\",\n    \"description\": \"adapt RSA for multimodal or retrieval-augmented LLMs; develop adaptive subset-selection strategies or theory on when aggregation helps.\"\n  },\n  {\n    \"insight_type\": \"Applications\",\n    \"description\": \"stronger reasoning in education/tutoring, code synthesis, and decision supportâ€”letting smaller models punch above their size. Could RSA enable cheaper, reliable 'deep thinking' at scale?\"\n  }\n]","summary":"**Introduction:** ğŸš€ Question: Can small LLMs 'think deeper' at inference time without more training?\nRSA (Recursive Self-Aggregation) is a test-time scaling method that iteratively aggregates and refines populations of reasoning chains to unlock deeper reasoningâ€”letting smaller models compete with bigger ones.\n\n**Challenges:** ğŸ¯ Challenges:\n- Existing test-time scaling is typically either parallel (many independent tries) or seque...","analyzed_at":"2025-10-01T11:01:47.590Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:12.598Z","views":6,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"arxiv_2509.26626v1","views_at_archive":6}},{"id":"arxiv_2509.26631v1","title":"Learning Generalizable Shape Completion with SIM(3) Equivariance","authors":["Yuqing Wang","Zhaiyu Chen","Xiao Xiang Zhu"],"abstract":"3D shape completion methods typically assume scans are pre-aligned to a\ncanonical frame. This leaks pose and scale cues that networks may exploit to\nmemorize absolute positions rather than inferring intrinsic geometry. When such\nalignment is absent in real data, performance collapses. We argue that robust\ngeneralization demands architectural equivariance to the similarity group,\nSIM(3), so the model remains agnostic to pose and scale. Following this\nprinciple, we introduce the first SIM(3)-equivariant shape completion network,\nwhose modular layers successively canonicalize features, reason over\nsimilarity-invariant geometry, and restore the original frame. Under a\nde-biased evaluation protocol that removes the hidden cues, our model\noutperforms both equivariant and augmentation baselines on the PCN benchmark.\nIt also sets new cross-domain records on real driving and indoor scans,\nlowering minimal matching distance on KITTI by 17% and Chamfer distance $\\ell1$\non OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol\nstill outperforms competitors under their biased settings. These results\nestablish full SIM(3) equivariance as an effective route to truly generalizable\nshape completion. Project page: https://sime-completion.github.io.","published":"2025-09-30T17:58:55Z","source":"arxiv","url":"http://arxiv.org/abs/2509.26631v1","analysis":{"introduction":"ğŸš€ What if 3D completion models are secretly cheating with pose & scale cues? This paper introduces the first SIM(3)-equivariant shape completion network that is agnostic to pose and scale â€” improving real-world generalization for robotics, AR, and autonomous driving.","challenges":"ğŸ¯ Challenges:\n- Existing methods assume scans are pre-aligned, leaking pose/scale cues.\n- Networks memorize absolute positions instead of intrinsic geometry.\n- Performance collapses on unaligned, real-world scans (poor cross-domain generalization).","innovations":"âœ¨ Innovations:\n- First SIM(3)-equivariant shape completion network.\n- Modular layers that: canonicalize features â†’ reason over similarity-invariant geometry â†’ restore original frame.\n- De-biased evaluation protocol to remove hidden pose/scale cues.\nNovelty: full SIM(3) equivariance to force pose/scale agnosticism.","experiments":"ğŸ“Š Experiment (most compelling): Lowered minimal matching distance on KITTI by 17% vs prior methods â€” proving SIM(3) equivariance substantially improves cross-domain shape completion generalization.","insights":"ğŸ¤” Insights / Next steps:\n- Explore combining SIM(3)-equivariant completion with learned pose priors or SLAM for online reconstruction.\n- Apply to autonomous driving, indoor mapping, AR/VR where scans are unaligned.\nCould SIM(3) equivariance become a standard inductive bias for 3D tasks?","keywords":["SIM(3) equivariance","shape completion","3D deep learning","point clouds","generalization","canonicalization"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"translation\": \"ğŸš€ å¦‚æœ3Dè¡¥å…¨æ¨¡å‹å·å·åˆ©ç”¨å§¿æ€å’Œå°ºåº¦çº¿ç´¢ä½œå¼Šæ€ä¹ˆåŠï¼Ÿæœ¬æ–‡ä»‹ç»äº†é¦–ä¸ªSIM(3)ç­‰å˜å½¢çŠ¶è¡¥å…¨ç½‘ç»œï¼Œè¯¥ç½‘ç»œå¯¹å§¿æ€å’Œå°ºåº¦ä¸æ•æ„Ÿâ€”â€”ä»è€Œæé«˜äº†å…¶åœ¨æœºå™¨äººæŠ€æœ¯ã€å¢å¼ºç°å®ï¼ˆARï¼‰å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸçš„çœŸå®ä¸–ç•Œæ³›åŒ–èƒ½åŠ›ã€‚\"\n}","chinese_challenges":"{\n  \"challenges\": \"ğŸ¯ æŒ‘æˆ˜ï¼š\\n- ç°æœ‰æ–¹æ³•å‡è®¾æ‰«ææ•°æ®å·²é¢„å…ˆå¯¹é½ï¼Œè¿™ä¼šæ³„éœ²å§¿æ€/å°ºåº¦çº¿ç´¢ã€‚\\n- ç½‘ç»œå€¾å‘äºè®°å¿†ç»å¯¹ä½ç½®ï¼Œè€Œéå†…åœ¨å‡ ä½•ç»“æ„ã€‚\\n- åœ¨æœªå¯¹é½çš„çœŸå®ä¸–ç•Œæ‰«ææ•°æ®ä¸Šï¼Œæ€§èƒ½ä¼šæ€¥å‰§ä¸‹é™ï¼ˆè·¨åŸŸæ³›åŒ–èƒ½åŠ›å·®ï¼‰ã€‚\"\n}","chinese_innovations":"{\n  \"innovations\": [\n    \"é¦–ä¸ªSIM(3)ç­‰å˜å½¢çŠ¶è¡¥å…¨ç½‘ç»œã€‚\",\n    \"æ¨¡å—åŒ–å±‚ï¼šå°†ç‰¹å¾è§„èŒƒåŒ– $\\\\rightarrow$ åŸºäºç›¸ä¼¼æ€§ä¸å˜å‡ ä½•è¿›è¡Œæ¨ç† $\\\\rightarrow$ æ¢å¤åŸå§‹åæ ‡ç³»ã€‚\",\n    \"å»åå€šè¯„ä¼°åè®®ï¼Œä»¥æ¶ˆé™¤éšè—çš„ä½å§¿/å°ºåº¦çº¿ç´¢ã€‚\",\n    \"æ–°é¢–æ€§ï¼šå®Œå…¨SIM(3)ç­‰å˜æ€§ï¼Œä»¥å¼ºåˆ¶å®ç°ä½å§¿/å°ºåº¦æ— å…³æ€§ã€‚\"\n  ]\n}","chinese_experiments":"\"å®éªŒï¼ˆæœ€å…·è¯´æœåŠ›çš„ï¼‰ï¼šåœ¨KITTIæ•°æ®é›†ä¸Šï¼Œå°†æœ€å°åŒ¹é…è·ç¦»æ¯”ç°æœ‰æ–¹æ³•é™ä½äº†17%ï¼Œè¯æ˜äº†SIM(3)ç­‰å˜æ€§æ˜¾è‘—æé«˜äº†è·¨åŸŸå½¢çŠ¶è¡¥å…¨çš„æ³›åŒ–èƒ½åŠ›ã€‚\"","chinese_insights":"\"ğŸ¤” æ´å¯Ÿ/ä¸‹ä¸€æ­¥ï¼š\\n- æ¢ç´¢å°†SIM(3)ç­‰å˜è¡¥å…¨ä¸å­¦ä¹ åˆ°çš„å§¿æ€å…ˆéªŒæˆ–SLAMç›¸ç»“åˆï¼Œç”¨äºåœ¨çº¿é‡å»ºã€‚\\n- åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€å®¤å†…å»ºå›¾ã€AR/VRç­‰æ‰«ææ•°æ®æœªå¯¹é½çš„åœºæ™¯ã€‚\\n- SIM(3)ç­‰å˜æ€§æ˜¯å¦èƒ½æˆä¸º3Dä»»åŠ¡çš„æ ‡å‡†å½’çº³åç½®ï¼Ÿ\"","summary":"**Introduction:** ğŸš€ What if 3D completion models are secretly cheating with pose & scale cues? This paper introduces the first SIM(3)-equivariant shape completion network that is agnostic to pose and scale â€” improving real-world generalization for robotics, AR, and autonomous driving.\n\n**Challenges:** ğŸ¯ Challenges:\n- Existing methods assume scans are pre-aligned, leaking pose/scale cues.\n- Networks memorize absolute positions inst...","analyzed_at":"2025-10-01T10:58:18.765Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:12.598Z","views":0,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"arxiv_2509.26631v1","views_at_archive":0}},{"id":"arxiv_2509.26641v1","title":"Query-Kontext: An Unified Multimodal Model for Image Generation and\n  Editing","authors":["Yuxin Song","Wenkai Dong","Shizun Wang","Qi Zhang","Song Xue","Tao Yuan","Hu Yang","Haocheng Feng","Hang Zhou","Xinyan Xiao","Jingdong Wang"],"abstract":"Unified Multimodal Models (UMMs) have demonstrated remarkable performance in\ntext-to-image generation (T2I) and editing (TI2I), whether instantiated as\nassembled unified frameworks which couple powerful vision-language model (VLM)\nwith diffusion-based generator, or as naive Unified Multimodal Models with an\nearly fusion of understanding and generation modalities. We contend that in\ncurrent unified frameworks, the crucial capability of multimodal generative\nreasoning which encompasses instruction understanding, grounding, and image\nreferring for identity preservation and faithful reconstruction, is\nintrinsically entangled with high-fidelity synthesis. In this work, we\nintroduce Query-Kontext, a novel approach that bridges the VLM and diffusion\nmodel via a multimodal ``kontext'' composed of semantic cues and coarse-grained\nimage conditions encoded from multimodal inputs. This design delegates the\ncomplex ability of multimodal generative reasoning to powerful VLM while\nreserving diffusion model's role for high-quality visual synthesis. To achieve\nthis, we propose a three-stage progressive training strategy. First, we connect\nthe VLM to a lightweight diffusion head via multimodal kontext tokens to\nunleash the VLM's generative reasoning ability. Second, we scale this head to a\nlarge, pre-trained diffusion model to enhance visual detail and realism.\nFinally, we introduce a low-level image encoder to improve image fidelity and\nperform instruction tuning on downstream tasks. Furthermore, we build a\ncomprehensive data pipeline integrating real, synthetic, and open-source\ndatasets, covering diverse multimodal reference-to-image scenarios, including\nimage generation, instruction-driven editing, customized generation, and\nmulti-subject composition. Experiments show that our approach matches strong\nunified baselines and even outperforms task-specific state-of-the-art methods\nin several cases.","published":"2025-09-30T17:59:46Z","source":"arxiv","url":"http://arxiv.org/abs/2509.26641v1","analysis":{"introduction":"ğŸš€ Can one model both understand complex multimodal instructions and produce highâ€‘fidelity edits? \nQueryâ€‘Kontext bridges a visionâ€‘language model (VLM) and a diffusion generator via multimodal â€œkontextâ€ tokens, offloading reasoning to the VLM and reserving synthesis for diffusion â€” improving T2I and instructionâ€‘driven editing.","challenges":"ğŸ¯ Key problems tackled:\n- Existing unified frameworks entangle multimodal generative reasoning with highâ€‘fidelity synthesis, hurting modularity.\n- Preserving identity and faithful reconstruction during instructionâ€‘driven editing is hard.\n- Lack of a unified, diverse data pipeline for varied referenceâ†’image scenarios.","innovations":"âœ¨ Core innovations:\n- Multimodal kontext: semantic cues + coarse image condition tokens that bridge VLM â†” diffusion.\n- Threeâ€‘stage progressive training: lightweight diffusion head â†’ scale to pretrained diffusion â†’ add lowâ€‘level image encoder + instruction tuning.\n- Comprehensive data pipeline combining real, synthetic, and open datasets.\nNovel twist: explicit decoupling of generative reasoning (VLM) from highâ€‘quality synthesis (diffusion) via kontext tokens.","experiments":"ğŸ“Š Results: The approach matches strong unified baselines and even outperforms taskâ€‘specific stateâ€‘ofâ€‘theâ€‘art methods in several cases. Exact numeric improvements or benchmark scores are not specified in the paper. \nMain proof: decoupling reasoning and synthesis via kontext tokens yields competitive or superior empirical performance.","insights":"ğŸ¤” Future directions & applications:\n- Explore spatially explicit kontext (perâ€‘region tokens) or sceneâ€‘graph conditioning for finer control and compositionality.\n- Integrate multimodal chainâ€‘ofâ€‘thought or stronger grounding for complex multiâ€‘subject edits.\nPotential apps: personalized image editing, multiâ€‘subject composition for content creation and AR/VR. Could this modular split speed up customization and safety checks?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"translation\": \"ğŸš€ æ˜¯å¦å­˜åœ¨ä¸€ä¸ªæ¨¡å‹èƒ½å¤ŸåŒæ—¶ç†è§£å¤æ‚çš„å¤šæ¨¡æ€æŒ‡ä»¤å¹¶ç”Ÿæˆé«˜ä¿çœŸåº¦çš„ç¼–è¾‘ï¼ŸQuery-Kontext é€šè¿‡å¤šæ¨¡æ€â€œkontextâ€ä»¤ç‰Œæ¡¥æ¥äº†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œæ‰©æ•£ç”Ÿæˆå™¨ï¼Œå°†æ¨ç†ä»»åŠ¡å¸è½½ç»™VLMï¼Œå¹¶å°†åˆæˆä»»åŠ¡ä¿ç•™ç»™æ‰©æ•£æ¨¡å‹â€”â€”ä»è€Œæ”¹è¿›äº†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„ç”Ÿæˆå’ŒæŒ‡ä»¤é©±åŠ¨çš„ç¼–è¾‘ã€‚\"\n}","chinese_challenges":"[\n  {\n    \"name\": \"AlexNet\",\n    \"year\": 2012,\n    \"task\": \"Image Classification\",\n    \"key_innovation\": \"Deep CNN, ReLU activation, Dropout\",\n    \"impact_score\": 9.5\n  },\n  {\n    \"name\": \"VGGNet\",\n    \"year\": 2014,\n    \"task\": \"Image Classification, Localization\",\n    \"key_innovation\": \"3x3 convolution kernels, deep architecture\",\n    \"impact_score\": 8.8\n  },\n  {\n    \"name\": \"ResNet\",\n    \"year\": 2015,\n    \"task\": \"Image Classification, Object Detection\",\n    \"key_innovation\": \"Residual connections (skip connections), solving vanishing gradient\",\n    \"impact_score\": 9.8\n  },\n  {\n    \"name\": \"Transformer (Attention Is All You Need)\",\n    \"year\": 2017,\n    \"task\": \"Sequence Modeling (NLP, later Vision)\",\n    \"key_innovation\": \"Self-attention mechanism, eliminating recurrence\",\n    \"impact_score\": 10.0\n  },\n  {\n    \"name\": \"ViT (Vision Transformer)\",\n    \"year\": 2020,\n    \"task\": \"Image Classification\",\n    \"key_innovation\": \"Applying pure Transformer architecture directly to image patches\",\n    \"impact_score\": 9.2\n  },\n  {\n    \"name\": \"Diffusion Models (DDPM)\",\n    \"year\": 2020,\n    \"task\": \"Generative Modeling\",\n    \"key_innovation\": \"Iterative denoising process for high-quality image synthesis\",\n    \"impact_score\": 9.7\n  }\n]","chinese_innovations":"{\n  \"æ ¸å¿ƒåˆ›æ–°\": [\n    \"å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼ˆMultimodal kontextï¼‰ï¼šè¯­ä¹‰çº¿ç´¢ + ç²—ç²’åº¦å›¾åƒæ¡ä»¶ä»¤ç‰Œï¼Œç”¨äºè¿æ¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸æ‰©æ•£æ¨¡å‹ï¼ˆdiffusionï¼‰ã€‚\",\n    \"ä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒï¼šè½»é‡çº§æ‰©æ•£å¤´éƒ¨ â†’ æ‰©å±•åˆ°é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ â†’ æ·»åŠ ä½çº§å›¾åƒç¼–ç å™¨ + æŒ‡ä»¤å¾®è°ƒã€‚\",\n    \"ç»“åˆçœŸå®ã€åˆæˆå’Œå¼€æ”¾æ•°æ®é›†çš„ç»¼åˆæ•°æ®ç®¡çº¿ã€‚\",\n    \"æ–°é¢–çš„è½¬æŠ˜ç‚¹ï¼šé€šè¿‡ä¸Šä¸‹æ–‡ä»¤ç‰Œï¼Œå°†ç”Ÿæˆæ¨ç†ï¼ˆVLMï¼‰ä¸é«˜è´¨é‡åˆæˆï¼ˆæ‰©æ•£æ¨¡å‹ï¼‰æ˜ç¡®è§£è€¦ã€‚\"\n  ]\n}","chinese_experiments":"{\n  \"chinese_translation\": \"ğŸ“Š ç»“æœï¼šè¯¥æ–¹æ³•ä¸å¼ºå¤§çš„ç»Ÿä¸€åŸºçº¿ï¼ˆunified baselinesï¼‰ç›¸å½“ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¶Šäº†é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æœ€å…ˆè¿›ï¼ˆstate-of-the-artï¼‰æ–¹æ³•ã€‚è®ºæ–‡ä¸­æ²¡æœ‰å…·ä½“è¯´æ˜ç²¾ç¡®çš„æ•°å­—æ”¹è¿›æˆ–åŸºå‡†åˆ†æ•°ã€‚\\nä¸»è¦è¯æ˜ï¼šé€šè¿‡â€œkontext tokensâ€è§£è€¦æ¨ç†ï¼ˆreasoningï¼‰å’Œåˆæˆï¼ˆsynthesisï¼‰ï¼Œå¯ä»¥è·å¾—å…·æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜è¶Šçš„ç»éªŒæ€§èƒ½ã€‚\"\n}","chinese_insights":"{\n  \"insights\": [\n    {\n      \"key\": \"Future directions & applications\",\n      \"value\": \"æ¢ç´¢ç©ºé—´æ˜¾å¼ä¸Šä¸‹æ–‡ï¼ˆæ¯åŒºåŸŸæ ‡è®°ï¼‰æˆ–åœºæ™¯å›¾æ¡ä»¶ï¼Œä»¥å®ç°æ›´ç²¾ç»†çš„æ§åˆ¶å’Œç»„åˆæ€§ã€‚\"\n    },\n    {\n      \"key\": \"Future directions & applications\",\n      \"value\": \"æ•´åˆå¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆchain-of-thoughtï¼‰æˆ–æ›´å¼ºçš„åŸºç¡€ï¼ˆgroundingï¼‰ï¼Œä»¥åº”å¯¹å¤æ‚çš„å¤šä¸»ä½“ç¼–è¾‘ã€‚\"\n    },\n    {\n      \"key\": \"Potential apps\",\n      \"value\": \"ä¸ªæ€§åŒ–å›¾åƒç¼–è¾‘ã€ç”¨äºå†…å®¹åˆ›å»ºå’Œå¢å¼ºç°å®/è™šæ‹Ÿç°å®ï¼ˆAR/VRï¼‰çš„å¤šä¸»ä½“ç»„åˆã€‚\"\n    },\n    {\n      \"key\": \"Modular split analysis\",\n      \"value\": \"è¿™ç§æ¨¡å—åŒ–æ‹†åˆ†èƒ½å¦åŠ å¿«å®šåˆ¶åŒ–å’Œå®‰å…¨æ£€æŸ¥çš„é€Ÿåº¦ï¼Ÿ\"\n    }\n  ]\n}","summary":"**Introduction:** ğŸš€ Can one model both understand complex multimodal instructions and produce highâ€‘fidelity edits? \nQueryâ€‘Kontext bridges a visionâ€‘language model (VLM) and a diffusion generator via multimodal â€œkontextâ€ tokens, offloading reasoning to the VLM and reserving synthesis for diffusion â€” improving T2I and instructionâ€‘driven editing.\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Existing unified frameworks entangle multimoda...","analyzed_at":"2025-10-01T10:50:16.965Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:12.598Z","views":23,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"arxiv_2509.26641v1","views_at_archive":23}},{"id":"arxiv_2509.26642v1","title":"MLA: A Multisensory Language-Action Model for Multimodal Understanding\n  and Forecasting in Robotic Manipulation","authors":["Zhuoyang Liu","Jiaming Liu","Jiadong Xu","Nuowei Han","Chenyang Gu","Hao Chen","Kaichen Zhou","Renrui Zhang","Kai Chin Hsieh","Kun Wu","Zhengping Che","Jian Tang","Shanghang Zhang"],"abstract":"Vision-language-action models (VLAs) have shown generalization capabilities\nin robotic manipulation tasks by inheriting from vision-language models (VLMs)\nand learning action generation. Most VLA models focus on interpreting vision\nand language to generate actions, whereas robots must perceive and interact\nwithin the spatial-physical world. This gap highlights the need for a\ncomprehensive understanding of robotic-specific multisensory information, which\nis crucial for achieving complex and contact-rich control. To this end, we\nintroduce a multisensory language-action (MLA) model that collaboratively\nperceives heterogeneous sensory modalities and predicts future multisensory\nobjectives to facilitate physical world modeling. Specifically, to enhance\nperceptual representations, we propose an encoder-free multimodal alignment\nscheme that innovatively repurposes the large language model itself as a\nperception module, directly interpreting multimodal cues by aligning 2D images,\n3D point clouds, and tactile tokens through positional correspondence. To\nfurther enhance MLA's understanding of physical dynamics, we design a future\nmultisensory generation post-training strategy that enables MLA to reason about\nsemantic, geometric, and interaction information, providing more robust\nconditions for action generation. For evaluation, the MLA model outperforms the\nprevious state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex,\ncontact-rich real-world tasks, respectively, while also demonstrating improved\ngeneralization to unseen configurations. Project website:\nhttps://sites.google.com/view/open-mla","published":"2025-09-30T17:59:50Z","source":"arxiv","url":"http://arxiv.org/abs/2509.26642v1","analysis":{"introduction":"ğŸš€ Question: Can robots truly feel and predict the physical world, not just see and read it?\nMLA introduces a multisensory language-action model that fuses vision, 3D, and tactile cues and predicts future multisensory objectives to unlock contact-rich robotic manipulation.","challenges":"ğŸ¯ Challenges:\n- Existing VLAs mainly interpret vision+language and ignore rich physical sensory signals.\n- Difficulty modeling contact-rich dynamics and multisensory temporal objectives.\n- Poor generalization to unseen spatial configurations in real-world manipulation.","innovations":"âœ¨ Innovations:\n- Encoder-free multimodal alignment: repurposes a large language model as a perception module to align 2D images, 3D point clouds, and tactile tokens via positional correspondence.\n- Future multisensory generation post-training: trains the model to predict future multisensory objectives (semantic, geometric, interaction) to support action generation.","experiments":"ğŸ“Š Experiment:\nMLA outperforms previous SOTA: +12% over 2D VLA and +24% over 3D VLA on complex, contact-rich real-world tasks â€” showing stronger task performance and better generalization to unseen configurations.","insights":"ğŸ¤” Insights (what's next?):\n- Research directions: integrate proprioception/force sensing and closed-loop real-time control; explore online adaptation and sim-to-real fine-tuning for safety.\n- Applications: dexterous assembly, surgical robotics, assistive manipulation. Could multisensory LLM perception become a new standard for embodied agents?","category":"robotics","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"\"ğŸš€ é—®é¢˜ï¼šæœºå™¨äººèƒ½å¦çœŸæ­£æ„ŸçŸ¥å¹¶é¢„æµ‹ç‰©ç†ä¸–ç•Œï¼Œè€Œä¸ä»…ä»…æ˜¯è§‚å¯Ÿå’Œè¯†åˆ«å®ƒï¼ŸMLA å¼•å…¥äº†ä¸€ä¸ªå¤šæ„Ÿå®˜è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œå®ƒèåˆäº†è§†è§‰ã€3D å’Œè§¦è§‰çº¿ç´¢ï¼Œå¹¶é¢„æµ‹æœªæ¥çš„å¤šæ„Ÿå®˜ç›®æ ‡ï¼Œä»è€Œå®ç°æ¥è§¦å¯†é›†å‹æœºå™¨äººæ“ä½œã€‚\"","chinese_challenges":"[\n  {\n    \"æŒ‘æˆ˜\": [\n      \"ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAsï¼‰ä¸»è¦ä¾§é‡äºè§£é‡Šè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†ä¸°å¯Œçš„ç‰©ç†æ„ŸçŸ¥ä¿¡å·ã€‚\",\n      \"éš¾ä»¥å¯¹æ¥è§¦å¯†é›†å‹åŠ¨åŠ›å­¦å’Œå¤šæ„Ÿå®˜æ—¶é—´ç›®æ ‡è¿›è¡Œæœ‰æ•ˆå»ºæ¨¡ã€‚\",\n      \"åœ¨ç°å®ä¸–ç•Œæ“ä½œä¸­ï¼Œå¯¹æœªè§è¿‡çš„ç©ºé—´é…ç½®æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚\"\n    ]\n  }\n]","chinese_innovations":"[\n  {\n    \"innovations\": \"Encoder-free multimodal alignment: repurposes a large language model as a perception module to align 2D images, 3D point clouds, and tactile tokens via positional correspondence.\"\n  },\n  {\n    \"innovations\": \"Future multisensory generation post-training: trains the model to predict future multisensory objectives (semantic, geometric, interaction) to support action generation.\"\n  }\n]","chinese_experiments":"{\n\"translation\": \"ğŸ“Š å®éªŒï¼šMLAåœ¨å¤æ‚çš„ã€å¯Œæ¥è§¦çš„çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­ï¼Œç›¸æ¯”2D VLAæ€§èƒ½æå‡äº†12%ï¼Œç›¸æ¯”3D VLAæ€§èƒ½æå‡äº†24%â€”â€”è¿™è¡¨æ˜å…¶ä»»åŠ¡æ€§èƒ½æ›´å¼ºï¼Œå¹¶ä¸”å¯¹æœªè§è¿‡çš„é…ç½®å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚\"\n}","chinese_insights":"{\n  \"translation\": \"ğŸ¤” æ´å¯Ÿï¼ˆä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿï¼‰ï¼š\\n- ç ”ç©¶æ–¹å‘ï¼šæ•´åˆæœ¬ä½“æ„Ÿè§‰/åŠ›ä¼ æ„Ÿå’Œé—­ç¯å®æ—¶æ§åˆ¶ï¼›æ¢ç´¢åœ¨çº¿é€‚åº”å’Œç”¨äºå®‰å…¨çš„ä»æ¨¡æ‹Ÿåˆ°ç°å®çš„å¾®è°ƒï¼ˆsim-to-real fine-tuningï¼‰ã€‚\\n- åº”ç”¨ï¼šçµå·§è£…é…ã€å¤–ç§‘æ‰‹æœ¯æœºå™¨äººã€è¾…åŠ©æ“ä½œã€‚å¤šæ„Ÿå®˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„ŸçŸ¥èƒ½å¦æˆä¸ºå…·èº«æ™ºèƒ½ä½“ï¼ˆembodied agentsï¼‰çš„æ–°æ ‡å‡†ï¼Ÿ\"\n}","summary":"**Introduction:** ğŸš€ Question: Can robots truly feel and predict the physical world, not just see and read it?\nMLA introduces a multisensory language-action model that fuses vision, 3D, and tactile cues and predicts future multisensory objectives to unlock contact-rich robotic manipulation.\n\n**Challenges:** ğŸ¯ Challenges:\n- Existing VLAs mainly interpret vision+language and ignore rich physical sensory signals.\n- Difficulty modeling...","analyzed_at":"2025-10-01T10:08:50.959Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-01T09:31:12.598Z","views":1,"archive_metadata":{"archived_at":"2025-10-01T14:12:20.273Z","original_id":"arxiv_2509.26642v1","views_at_archive":1}}],"metadata":{"total_papers":10,"categories":{"robotics":3,"machine_learning":4,"natural_language_processing":3},"sources":{"huggingface":5,"arxiv":5},"average_score":9,"unique_keywords":["OceanGym","Multi-modal Large Language Models","MLLMs","underwater robotics","sonar","optical imaging","embodied AI","long-horizon planning","benchmark","retargeting","humanoid","interaction mesh","Laplacian deformation","loco-manipulation","contact preservation","reinforcement learning","proprioceptive policy","Unitree G1","data augmentation","OMOMO","LAFAN1","SIM(3) equivariance","shape completion","3D deep learning","point clouds","generalization","canonicalization"],"total_views":42,"created_at":"2025-10-01T14:12:20.274Z","source":"daily_update","auto_archived":true,"papers_archived":10,"total_papers_analyzed":25}}