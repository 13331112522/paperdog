[{"id":"arxiv_2510.27692v1","arxiv_id":"2510.27692v1","title":"LifWavNet: Lifting Wavelet-based Network for Non-contact ECG\n  Reconstruction from Radar","abstract":"Non-contact electrocardiogram (ECG) reconstruction from radar signals offers\na promising approach for unobtrusive cardiac monitoring. We present LifWavNet,\na lifting wavelet network based on a multi-resolution analysis and synthesis\n(MRAS) model for radar-to-ECG reconstruction. Unlike prior models that use\nfixed wavelet approaches, LifWavNet employs learnable lifting wavelets with\nlifting and inverse lifting units to adaptively capture radar signal features\nand synthesize physiologically meaningful ECG waveforms. To improve\nreconstruction fidelity, we introduce a multi-resolution short-time Fourier\ntransform (STFT) loss, that enforces consistency with the ground-truth ECG in\nboth temporal and spectral domains. Evaluations on two public datasets\ndemonstrate that LifWavNet outperforms state-of-the-art methods in ECG\nreconstruction and downstream vital sign estimation (heart rate and heart rate\nvariability). Furthermore, intermediate feature visualization highlights the\ninterpretability of multi-resolution decomposition and synthesis in\nradar-to-ECG reconstruction. These results establish LifWavNet as a robust\nframework for radar-based non-contact ECG measurement.","authors":["Soumitra Kundu","Gargi Panda","Saumik Bhattacharya","Aurobinda Routray","Rajlakshmi Guha"],"published":"2025-10-31T17:59:58Z","updated":"2025-10-31T17:59:58Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27692v1","pdf_url":"http://arxiv.org/pdf/2510.27692v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ Want contactless ECG from radar? LifWavNet learns lifting wavelets to reconstruct physiologically meaningful ECGs from radar signals using a multi-resolution analysis/synthesis framework and a multi-resolution STFT loss ‚Äî enabling unobtrusive cardiac monitoring for home, ICU, elder care.","challenges":"üéØ Problems solved: - Prior radar-to-ECG models used fixed wavelets that lack adaptivity to subject/sensor variations. - Reconstructing physiologically plausible ECG morphology from radar is hard (temporal + spectral fidelity). - Downstream vital-sign (HR/HRV) accuracy often degrades with prior methods.","innovations":"‚ú® Novel contributions: - LifWavNet: a lifting-wavelet network built on a multi-resolution analysis & synthesis (MRAS) model. - Learnable lifting and inverse-lifting units (adaptive wavelets vs fixed ones). - Multi-resolution STFT loss to enforce temporal and spectral consistency. - Intermediate feature visualization for interpretability of decomposition/synthesis.","experiments":"üìä Results: LifWavNet outperformed state-of-the-art methods on two public radar-to-ECG datasets and improved downstream heart-rate and heart-rate-variability estimation, demonstrating better ECG reconstruction fidelity. Numerical improvement details: Not specified in the paper.","insights":"ü§î What's next? - Research: adapt LifWavNet to other contactless biosignals (e.g., PPG from camera/radar) and to robustify against real-world noise/motion; explore multimodal fusion (radar+camera/PPG) to boost fidelity. - Applications: remote patient monitoring, sleep studies, non-contact ICU monitoring. Could adaptive lifting wavelets enable broader non-contact biosignal reconstruction?","keywords":["lifting_wavelet","learnable_wavelets","radar_to_ecg","non_contact_ecg","multi_resolution_STFT","biosignal_reconstruction","vital_sign_estimation","interpretable_ml"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Want contactless ECG from radar? LifWavNet learns lifting wavelets to reconstruct physiologically meaningful ECGs from radar signals using a multi-resolution analysis/synthesis framework and a multi-resolution STFT loss ‚Äî enabling unobtrusive cardiac monitoring for home, ICU, elder care.\n\n**Challenges:** üéØ Problems solved: - Prior radar-to-ECG models used fixed wavelets that lack adaptivity to subject/sensor va...","analyzed_at":"2025-11-04T15:01:12.591Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27688v1","arxiv_id":"2510.27688v1","title":"Continuous Autoregressive Language Models","abstract":"The efficiency of large language models (LLMs) is fundamentally limited by\ntheir sequential, token-by-token generation process. We argue that overcoming\nthis bottleneck requires a new design axis for LLM scaling: increasing the\nsemantic bandwidth of each generative step. To this end, we introduce\nContinuous Autoregressive Language Models (CALM), a paradigm shift from\ndiscrete next-token prediction to continuous next-vector prediction. CALM uses\na high-fidelity autoencoder to compress a chunk of K tokens into a single\ncontinuous vector, from which the original tokens can be reconstructed with\nover 99.9\\% accuracy. This allows us to model language as a sequence of\ncontinuous vectors instead of discrete tokens, which reduces the number of\ngenerative steps by a factor of K. The paradigm shift necessitates a new\nmodeling toolkit; therefore, we develop a comprehensive likelihood-free\nframework that enables robust training, evaluation, and controllable sampling\nin the continuous domain. Experiments show that CALM significantly improves the\nperformance-compute trade-off, achieving the performance of strong discrete\nbaselines at a significantly lower computational cost. More importantly, these\nfindings establish next-vector prediction as a powerful and scalable pathway\ntowards ultra-efficient language models. Code:\nhttps://github.com/shaochenze/calm. Project:\nhttps://shaochenze.github.io/blog/2025/CALM.","authors":["Chenze Shao","Darren Li","Fandong Meng","Jie Zhou"],"published":"2025-10-31T17:58:11Z","updated":"2025-10-31T17:58:11Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27688v1","pdf_url":"http://arxiv.org/pdf/2510.27688v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ Ever frustrated by slow token-by-token LLM outputs? CALM flips the script: it predicts continuous next-vectors instead of tokens, using an autoencoder to compress K tokens into one vector (reconstructed >99.9%). Fewer steps ‚Üí big efficiency gains for inference-heavy apps.","challenges":"üéØ Problems tackled: - Sequential token-by-token generation bottlenecks LLM throughput. - Low semantic bandwidth per generation step makes scaling inefficient. - Existing models/tools don‚Äôt support robust training/evaluation in a continuous generative domain.","innovations":"‚ú® Innovations: - High-fidelity autoencoder that compresses K tokens into a single continuous vector (>99.9% reconstruction). - Continuous autoregressive modeling: next-vector prediction replaces next-token prediction. - A likelihood-free framework for training, evaluation, and controllable sampling in continuous space. Novelty: a paradigm shift from discrete tokens to continuous vectors, reducing steps by factor K.","experiments":"üìä Experiment highlight: The autoencoder reconstructs K-token chunks with >99.9% accuracy. CALM models sequences of continuous vectors, cutting generative steps by a factor of K, and achieves the performance of strong discrete baselines at significantly lower computational cost. Exact benchmark numbers: Not specified in the paper.","insights":"ü§î What‚Äôs next? - Research directions: adaptive/variable chunk sizes (dynamic K) and hybrid discrete‚Äìcontinuous decoding strategies. - Applications: faster, lower-cost LLM inference for chatbots and on-device models; potential energy/cost savings in deployment. Could next-vector prediction be the key to ultra-efficient LLMs?","keywords":["CALM","continuous autoregressive","next-vector prediction","autoencoder","likelihood-free","LLM efficiency","semantic bandwidth"],"category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Ever frustrated by slow token-by-token LLM outputs? CALM flips the script: it predicts continuous next-vectors instead of tokens, using an autoencoder to compress K tokens into one vector (reconstructed >99.9%). Fewer steps ‚Üí big efficiency gains for inference-heavy apps.\n\n**Challenges:** üéØ Problems tackled: - Sequential token-by-token generation bottlenecks LLM throughput. - Low semantic bandwidth per generati...","analyzed_at":"2025-11-04T15:01:25.510Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27684v1","arxiv_id":"2510.27684v1","title":"Phased DMD: Few-step Distribution Matching Distillation via Score\n  Matching within Subintervals","abstract":"Distribution Matching Distillation (DMD) distills score-based generative\nmodels into efficient one-step generators, without requiring a one-to-one\ncorrespondence with the sampling trajectories of their teachers. However,\nlimited model capacity causes one-step distilled models underperform on complex\ngenerative tasks, e.g., synthesizing intricate object motions in text-to-video\ngeneration. Directly extending DMD to multi-step distillation increases memory\nusage and computational depth, leading to instability and reduced efficiency.\nWhile prior works propose stochastic gradient truncation as a potential\nsolution, we observe that it substantially reduces the generation diversity of\nmulti-step distilled models, bringing it down to the level of their one-step\ncounterparts. To address these limitations, we propose Phased DMD, a multi-step\ndistillation framework that bridges the idea of phase-wise distillation with\nMixture-of-Experts (MoE), reducing learning difficulty while enhancing model\ncapacity. Phased DMD is built upon two key ideas: progressive distribution\nmatching and score matching within subintervals. First, our model divides the\nSNR range into subintervals, progressively refining the model to higher SNR\nlevels, to better capture complex distributions. Next, to ensure the training\nobjective within each subinterval is accurate, we have conducted rigorous\nmathematical derivations. We validate Phased DMD by distilling state-of-the-art\nimage and video generation models, including Qwen-Image (20B parameters) and\nWan2.2 (28B parameters). Experimental results demonstrate that Phased DMD\npreserves output diversity better than DMD while retaining key generative\ncapabilities. We will release our code and models.","authors":["Xiangyu Fan","Zesong Qiu","Zhuguanyu Wu","Fanzhou Wang","Zhiqian Lin","Tianxiang Ren","Dahua Lin","Ruihao Gong","Lei Yang"],"published":"2025-10-31T17:55:10Z","updated":"2025-10-31T17:55:10Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27684v1","pdf_url":"http://arxiv.org/pdf/2510.27684v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ Want fast generative models that still capture complex scenes and motions? Phased DMD introduces a few-step distillation that matches distributions via score matching inside SNR subintervals ‚Äî boosting capacity and preserving diversity for image & video generators.","challenges":"üéØ Problems tackled: - One-step DMD models lack capacity for complex generative tasks (e.g., intricate object motion). - Direct multi-step distillation raises memory/compute and instability. - Stochastic gradient truncation reduces diversity in multi-step distilled models.","innovations":"‚ú® Novel methods: - Phased DMD: phase-wise (progressive) distribution matching across SNR subintervals. - Score matching restricted to subintervals to keep objectives accurate. - Integrates Mixture-of-Experts to raise model capacity while easing learning difficulty. - Rigorous mathematical derivations supporting the training objective.","experiments":"üìä Key experimental claim: Phased DMD preserves output diversity better than standard DMD while retaining core generative capabilities when distilling large image/video teachers (Qwen-Image 20B, Wan2.2 28B). Exact numeric improvements are not specified in the paper.","insights":"ü§î What's next? - Research: adaptive subinterval scheduling or learned SNR partitioning; explore routing/efficiency of MoE for few-step generators. - Applications: higher-fidelity text-to-video and faster deployed generative models for creative tools. Could phased, adaptive phasing enable real-time conditional generation?","keywords":["Phased DMD","Distribution Matching Distillation","score matching","SNR subintervals","Mixture-of-Experts","few-step distillation","Qwen-Image","Wan2.2","generative models","diffusion"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Want fast generative models that still capture complex scenes and motions? Phased DMD introduces a few-step distillation that matches distributions via score matching inside SNR subintervals ‚Äî boosting capacity and preserving diversity for image & video generators.\n\n**Challenges:** üéØ Problems tackled: - One-step DMD models lack capacity for complex generative tasks (e.g., intricate object motion). - Direct mult...","analyzed_at":"2025-11-04T15:01:18.059Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27680v1","arxiv_id":"2510.27680v1","title":"PETAR: Localized Findings Generation with Mask-Aware Vision-Language\n  Modeling for PET Automated Reporting","abstract":"Recent advances in vision-language models (VLMs) have enabled impressive\nmultimodal reasoning, yet most medical applications remain limited to 2D\nimaging. In this work, we extend VLMs to 3D positron emission tomography and\ncomputed tomography (PET/CT), a domain characterized by large volumetric data,\nsmall and dispersed lesions, and lengthy radiology reports. We introduce a\nlarge-scale dataset comprising over 11,000 lesion-level descriptions paired\nwith 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid\nrule-based and large language model (LLM) pipeline. Building upon this dataset,\nwe propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET,\nCT, and lesion contours for spatially grounded report generation. PETAR bridges\nglobal contextual reasoning with fine-grained lesion awareness, producing\nclinically coherent and localized findings. Comprehensive automated and human\nevaluations demonstrate that PETAR substantially improves PET/CT report\ngeneration quality, advancing 3D medical vision-language understanding.","authors":["Danyal Maqbool","Changhee Lee","Zachary Huemann","Samuel D. Church","Matthew E. Larson","Scott B. Perlman","Tomas A. Romero","Joshua D. Warner","Meghan Lubner","Xin Tie","Jameson Merkow","Junjie Hu","Steve Y. Cho","Tyler J. Bradshaw"],"published":"2025-10-31T17:49:01Z","updated":"2025-10-31T17:49:01Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27680v1","pdf_url":"http://arxiv.org/pdf/2510.27680v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ Ever wished PET/CT reports could be auto-generated from full 3D scans with lesion-level detail?  PETAR presents PETAR-4B: a 3D, mask-aware vision-language model that generates spatially grounded PET/CT findings.  Why it matters: speeds and improves radiology reporting for oncology and nuclear medicine.","challenges":"üéØ Key problems addressed: - Most VLMs focus on 2D images, not large 3D PET/CT volumes. - Lesions are small, dispersed, and need precise spatial grounding. - Radiology reports are long and require lesion-level, clinically coherent language.","innovations":"‚ú® Core contributions: - Built a large dataset: >11,000 lesion-level descriptions paired with 3D segmentations from >5,000 PET/CT exams via a hybrid rule-based + LLM pipeline. - PETAR-4B: a 3D mask-aware vision-language model that ingests PET, CT, and lesion contours. - Novelty: extends VLMs to volumetric PET/CT and explicitly uses lesion masks for spatially grounded report generation, bridging global context with fine-grained lesion awareness.","experiments":"üìä Results: Not specified in the paper.  What the paper reports: comprehensive automated and human evaluations showing PETAR \"substantially improves PET/CT report generation quality,\" demonstrating improved clinical coherence and localized findings over prior approaches.","insights":"ü§î What's next (inspired directions & applications): - Research: pretraining large 3D multimodal VLMs across PET/CT/MRI and exploring lesion-level outcome prediction and uncertainty quantification. - Applications: interactive radiology assistants, triage tools for oncology imaging, and cross-center adaptation for clinical deployment.  Could mask-aware 3D VLMs become routine in clinical reporting?","keywords":["PETAR-4B","PET/CT","3D vision-language","mask-aware","lesion segmentation","radiology report generation","dataset","VLM","LLM pipeline"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Ever wished PET/CT reports could be auto-generated from full 3D scans with lesion-level detail?  PETAR presents PETAR-4B: a 3D, mask-aware vision-language model that generates spatially grounded PET/CT findings.  Why it matters: speeds and improves radiology reporting for oncology and nuclear medicine.\n\n**Challenges:** üéØ Key problems addressed: - Most VLMs focus on 2D images, not large 3D PET/CT volumes. - Lesi...","analyzed_at":"2025-11-04T15:01:56.685Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27679v1","arxiv_id":"2510.27679v1","title":"Dark-Field X-Ray Imaging Significantly Improves Deep-Learning based\n  Detection of Synthetic Early-Stage Lung Tumors in Preclinical Models","abstract":"Low-dose computed tomography (LDCT) is the current standard for lung cancer\nscreening, yet its adoption and accessibility remain limited. Many regions lack\nLDCT infrastructure, and even among those screened, early-stage cancer\ndetection often yield false positives, as shown in the National Lung Screening\nTrial (NLST) with a sensitivity of 93.8 percent and a false-positive rate of\n26.6 percent. We aim to investigate whether X-ray dark-field imaging (DFI)\nradiograph, a technique sensitive to small-angle scatter from alveolar\nmicrostructure and less susceptible to organ shadowing, can significantly\nimprove early-stage lung tumor detection when coupled with deep-learning\nsegmentation. Using paired attenuation (ATTN) and DFI radiograph images of\neuthanized mouse lungs, we generated realistic synthetic tumors with irregular\nboundaries and intensity profiles consistent with physical lung contrast. A\nU-Net segmentation network was trained on small patches using either ATTN, DFI,\nor a combination of ATTN and DFI channels. Results show that the DFI-only model\nachieved a true-positive detection rate of 83.7 percent, compared with 51\npercent for ATTN-only, while maintaining comparable specificity (90.5 versus\n92.9 percent). The combined ATTN and DFI input achieved 79.6 percent\nsensitivity and 97.6 percent specificity. In conclusion, DFI substantially\nimproves early-tumor detectability in comparison to standard attenuation\nradiography and shows potential as an accessible, low-cost, low-dose\nalternative for pre-clinical or limited-resource screening where LDCT is\nunavailable.","authors":["Joyoni Dey","Hunter C. Meyer","Murtuza S. Taqi"],"published":"2025-10-31T17:47:40Z","updated":"2025-10-31T17:47:40Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27679v1","pdf_url":"http://arxiv.org/pdf/2510.27679v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ What if a single X‚Äëray could spot early lung tumors more reliably than standard radiographs?   This paper shows X‚Äëray dark‚Äëfield imaging (DFI) + deep‚Äëlearning segmentation dramatically boosts early‚Äëstage tumor detectability in preclinical mouse lungs ‚Äî a low‚Äëdose, low‚Äëcost alternative for settings without LDCT.","challenges":"üéØ Key problems tackled: - Limited LDCT access and infrastructure in many regions (screening gaps). - High false‚Äëpositive rates in screening (NLST: sensitivity 93.8%, false‚Äëpositive 26.6%). - Conventional attenuation radiographs miss microstructure changes and suffer organ shadowing.","innovations":"‚ú® Novel methods: - Use of X‚Äëray dark‚Äëfield imaging (DFI) sensitive to small‚Äëangle scatter from alveolar microstructure. - Generation of realistic synthetic early tumors in paired attenuation (ATTN) + DFI mouse radiographs. - Patchwise U‚ÄëNet segmentation trained on ATTN, DFI, or combined channels. - Novelty: demonstrating DFI‚Äëonly inputs markedly improve detection vs standard ATTN and combining channels raises specificity.","experiments":"üìä Most compelling result: DFI‚Äëonly model sensitivity: 83.7% vs ATTN‚Äëonly 51.0% (specificity comparable: 90.5% vs 92.9%). Combined ATTN+DFI: 79.6% sensitivity and 97.6% specificity. This proves DFI substantially improves early‚Äëtumor detectability over attenuation radiography in this preclinical setup.","insights":"ü§î What's next? - Validate on in vivo and real tumor datasets to confirm gains beyond synthetic lesions. - Explore DFI + DL in low‚Äëresource clinical screening workflows or portable devices. - Investigate fusion with 3D CT or temporal monitoring for longitudinal detection. Could DFI democratize early lung‚Äëcancer screening where LDCT is unavailable?","keywords":["dark-field imaging","x-ray","lung tumor","deep learning","U-Net","attenuation radiography","synthetic tumors","small-angle scatter","preclinical","screening"],"category":"machine_learning","relevance_score":9,"technical_depth":"intermediate","summary":"**Introduction:** üöÄ What if a single X‚Äëray could spot early lung tumors more reliably than standard radiographs?   This paper shows X‚Äëray dark‚Äëfield imaging (DFI) + deep‚Äëlearning segmentation dramatically boosts early‚Äëstage tumor detectability in preclinical mouse lungs ‚Äî a low‚Äëdose, low‚Äëcost alternative for settings without LDCT.\n\n**Challenges:** üéØ Key problems tackled: - Limited LDCT access and infrastructure in many regions (sc...","analyzed_at":"2025-11-04T15:01:52.325Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27677v1","arxiv_id":"2510.27677v1","title":"Vision Transformer for Robust Occluded Person Reidentification in\n  Complex Surveillance Scenes","abstract":"Person re-identification (ReID) in surveillance is challenged by occlusion,\nviewpoint distortion, and poor image quality. Most existing methods rely on\ncomplex modules or perform well only on clear frontal images. We propose Sh-ViT\n(Shuffling Vision Transformer), a lightweight and robust model for occluded\nperson ReID. Built on ViT-Base, Sh-ViT introduces three components: First, a\nShuffle module in the final Transformer layer to break spatial correlations and\nenhance robustness to occlusion and blur; Second, scenario-adapted augmentation\n(geometric transforms, erasing, blur, and color adjustment) to simulate\nsurveillance conditions; Third, DeiT-based knowledge distillation to improve\nlearning with limited labels.To support real-world evaluation, we construct the\nMyTT dataset, containing over 10,000 pedestrians and 30,000+ images from base\nstation inspections, with frequent equipment occlusion and camera variations.\nExperiments show that Sh-ViT achieves 83.2% Rank-1 and 80.1% mAP on MyTT,\noutperforming CNN and ViT baselines, and 94.6% Rank-1 and 87.5% mAP on\nMarket1501, surpassing state-of-the-art methods.In summary, Sh-ViT improves\nrobustness to occlusion and blur without external modules, offering a practical\nsolution for surveillance-based personnel monitoring.","authors":["Bo Li","Duyuan Zheng","Xinyang Liu","Qingwen Li","Hong Li","Hongyan Cui","Ge Gao","Chen Liu"],"published":"2025-10-31T17:43:50Z","updated":"2025-10-31T17:43:50Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27677v1","pdf_url":"http://arxiv.org/pdf/2510.27677v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ Frustrated with ReID breaking under occlusion or blur? Sh-ViT (Shuffling Vision Transformer) is a lightweight ViT-Base model that boosts robustness to occlusion and poor-quality surveillance images ‚Äî no external modules required ‚Äî built for real-world monitoring.","challenges":"üéØ Key problems tackled: - Occlusion: people partially blocked by equipment or objects. - Poor image quality & viewpoint distortion: blur and camera variation in surveillance. - Limited labeled data for robust training (paper uses distillation to help).","innovations":"‚ú® What they introduce: - Shuffle module in the final Transformer layer: breaks spatial correlations to improve robustness to occlusion/blur. - Scenario-adapted augmentation: geometric transforms, erasing, blur, color tweaks to simulate surveillance. - DeiT-based knowledge distillation to improve learning with limited labels. Novelty: robustness gain via internal token shuffling instead of external occlusion modules.","experiments":"üìä Results & proof: Sh-ViT achieves 83.2% Rank-1 & 80.1% mAP on the new MyTT dataset (real base-station surveillance), and 94.6% Rank-1 & 87.5% mAP on Market1501 ‚Äî outperforming CNN/ViT baselines and reported SOTA, showing improved occlusion/blur robustness.","insights":"ü§î Where this can lead: - Research: combine token shuffling with explicit occlusion masks or temporal fusion for multi-camera tracking; explore domain adaptation for diverse surveillance sites. - Applications: more robust crowd/security monitoring, retail analytics, edge deployment for station cameras. Could this replace heavy occlusion modules in practical deployments?","keywords":["Vision Transformer","Occluded ReID","Shuffle module","Data augmentation","Knowledge distillation","MyTT dataset","Surveillance"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Frustrated with ReID breaking under occlusion or blur? Sh-ViT (Shuffling Vision Transformer) is a lightweight ViT-Base model that boosts robustness to occlusion and poor-quality surveillance images ‚Äî no external modules required ‚Äî built for real-world monitoring.\n\n**Challenges:** üéØ Key problems tackled: - Occlusion: people partially blocked by equipment or objects. - Poor image quality & viewpoint distortion: b...","analyzed_at":"2025-11-04T15:01:53.123Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27675v1","arxiv_id":"2510.27675v1","title":"On Selecting Few-Shot Examples for LLM-based Code Vulnerability\n  Detection","abstract":"Large language models (LLMs) have demonstrated impressive capabilities for\nmany coding tasks, including summarization, translation, completion, and code\ngeneration. However, detecting code vulnerabilities remains a challenging task\nfor LLMs. An effective way to improve LLM performance is in-context learning\n(ICL) - providing few-shot examples similar to the query, along with correct\nanswers, can improve an LLM's ability to generate correct solutions. However,\nchoosing the few-shot examples appropriately is crucial to improving model\nperformance. In this paper, we explore two criteria for choosing few-shot\nexamples for ICL used in the code vulnerability detection task. The first\ncriterion considers if the LLM (consistently) makes a mistake or not on a\nsample with the intuition that LLM performance on a sample is informative about\nits usefulness as a few-shot example. The other criterion considers similarity\nof the examples with the program under query and chooses few-shot examples\nbased on the $k$-nearest neighbors to the given sample. We perform evaluations\nto determine the benefits of these criteria individually as well as under\nvarious combinations, using open-source models on multiple datasets.","authors":["Md Abdul Hannan","Ronghao Ni","Chi Zhang","Limin Jia","Ravi Mangal","Corina S. Pasareanu"],"published":"2025-10-31T17:41:58Z","updated":"2025-10-31T17:41:58Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27675v1","pdf_url":"http://arxiv.org/pdf/2510.27675v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ Want LLMs to spot security bugs better in code? This paper studies how to pick the best few-shot examples for in-context learning to boost LLM-based code vulnerability detection. It tests two selection strategies to make LLMs more reliable for security engineers and ML researchers.","challenges":"üéØ Key problems tackled: - LLMs often struggle to detect code vulnerabilities reliably. - Choosing helpful few-shot examples for in-context learning is unclear and ad-hoc. - No clear guidance on whether similarity or model-behavior signals make better examples.","innovations":"‚ú® Core ideas and novelty: - Two selection criteria: (1) LLM mistake-consistency (pick samples the model tends to err on)  - (2) k-nearest-neighbor similarity (pick samples most similar to the query) - Evaluate combinations of these criteria using open-source LLMs on multiple datasets - Novel twist: use the LLM's own error patterns as an informative signal for example selection.","experiments":"üìä Results & what they prove: - Single most compelling quantitative result: Not specified in the paper. - Main breakthrough demonstrated: a systematic empirical study that evaluates the benefits of LLM mistake-consistency and kNN-similarity (alone and combined) for few-shot ICL in code vulnerability detection across open-source models and multiple datasets. Quantitative magnitudes: Not specified in the paper.","insights":"ü§î What's next (inspired ideas & apps): - Explore adaptive, query-specific selection that updates examples online as model behavior changes. - Combine static code-analysis features with LLM-behavior signals for hybrid selection policies. - Broader apps: improved code-review assistants, automated vulnerability triage, and CI integration for security checks. Could dynamic example curricula make LLMs robust to adversarial or rare vulnerabilities?","keywords":["few-shot learning","in-context learning","LLMs","code vulnerability detection","example selection","k-nearest neighbors","model-behavior signal","software security","open-source models"],"category":"machine_learning","relevance_score":9,"technical_depth":"intermediate","summary":"**Introduction:** üöÄ Want LLMs to spot security bugs better in code? This paper studies how to pick the best few-shot examples for in-context learning to boost LLM-based code vulnerability detection. It tests two selection strategies to make LLMs more reliable for security engineers and ML researchers.\n\n**Challenges:** üéØ Key problems tackled: - LLMs often struggle to detect code vulnerabilities reliably. - Choosing helpful few-shot...","analyzed_at":"2025-11-04T15:02:25.186Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27672v1","arxiv_id":"2510.27672v1","title":"Culture Cartography: Mapping the Landscape of Cultural Knowledge","abstract":"To serve global users safely and productively, LLMs need culture-specific\nknowledge that might not be learned during pre-training. How do we find such\nknowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The\nmost common solutions are single-initiative: either researchers define\nchallenging questions that users passively answer (traditional annotation), or\nusers actively produce data that researchers structure as benchmarks (knowledge\nextraction). The process would benefit from mixed-initiative collaboration,\nwhere users guide the process to meaningfully reflect their cultures, and LLMs\nsteer the process towards more challenging questions that meet the researcher's\ngoals. We propose a mixed-initiative methodology called CultureCartography.\nHere, an LLM initializes annotation with questions for which it has\nlow-confidence answers, making explicit both its prior knowledge and the gaps\ntherein. This allows a human respondent to fill these gaps and steer the model\ntowards salient topics through direct edits. We implement this methodology as a\ntool called CultureExplorer. Compared to a baseline where humans answer\nLLM-proposed questions, we find that CultureExplorer more effectively produces\nknowledge that leading models like DeepSeek R1 and GPT-4o are missing, even\nwith web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B\nby up to 19.2% on related culture benchmarks.","authors":["Caleb Ziems","William Held","Jane Yu","Amir Goldberg","David Grusky","Diyi Yang"],"published":"2025-10-31T17:37:34Z","updated":"2025-10-31T17:37:34Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27672v1","pdf_url":"http://arxiv.org/pdf/2510.27672v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ Can LLMs truly understand local culture?  CultureCartography introduces a mixed-initiative approach where LLMs expose their knowledge gaps and humans fill them via CultureExplorer ‚Äî a practical way to build culture-specific knowledge for safer, more useful AI for global users. üåç","challenges":"üéØ Problems tackled: - Culture-specific knowledge often missing from pretraining, hurting global usability. - Single-initiative workflows (only annotation or only extraction) fail to reflect what communities find salient. - LLMs don't explicitly reveal which cultural gaps they have.","innovations":"‚ú® Key ideas: - Mixed-initiative methodology: CultureCartography. - Tool: CultureExplorer ‚Äî LLMs propose low-confidence questions, humans answer/edit to steer topics. - Novel twist: models explicitly expose gaps and users guide content, creating targeted cultural knowledge.","experiments":"üìä Results: - Fine-tuning on collected data boosted Llama-3.1-8B accuracy by up to 19.2% on related culture benchmarks. - CultureExplorer produced knowledge that leading models (DeepSeek R1, GPT-4o) were missing, even vs. web search ‚Äî showing mixed-initiative data fills real gaps.","insights":"ü§î What's next? - Extend mixed-initiative mapping to marginalized or domain-specific knowledge (e.g., local laws, dialects). - Build community-curated, updateable cultural KBs to keep models current. Could this reshape localization and culturally-aware assistants? üåê","keywords":["CultureCartography","CultureExplorer","mixed-initiative","cultural knowledge","LLMs","fine-tuning","Llama-3.1-8B","GPT-4o","DeepSeek R1"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Can LLMs truly understand local culture?  CultureCartography introduces a mixed-initiative approach where LLMs expose their knowledge gaps and humans fill them via CultureExplorer ‚Äî a practical way to build culture-specific knowledge for safer, more useful AI for global users. üåç\n\n**Challenges:** üéØ Problems tackled: - Culture-specific knowledge often missing from pretraining, hurting global usability. - Single-...","analyzed_at":"2025-11-04T15:02:22.360Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27671v1","arxiv_id":"2510.27671v1","title":"MolChord: Structure-Sequence Alignment for Protein-Guided Drug Design","abstract":"Structure-based drug design (SBDD), which maps target proteins to candidate\nmolecular ligands, is a fundamental task in drug discovery. Effectively\naligning protein structural representations with molecular representations, and\nensuring alignment between generated drugs and their pharmacological\nproperties, remains a critical challenge. To address these challenges, we\npropose MolChord, which integrates two key techniques: (1) to align protein and\nmolecule structures with their textual descriptions and sequential\nrepresentations (e.g., FASTA for proteins and SMILES for molecules), we\nleverage NatureLM, an autoregressive model unifying text, small molecules, and\nproteins, as the molecule generator, alongside a diffusion-based structure\nencoder; and (2) to guide molecules toward desired properties, we curate a\nproperty-aware dataset by integrating preference data and refine the alignment\nprocess using Direct Preference Optimization (DPO). Experimental results on\nCrossDocked2020 demonstrate that our approach achieves state-of-the-art\nperformance on key evaluation metrics, highlighting its potential as a\npractical tool for SBDD.","authors":["Wei Zhang","Zekun Guo","Yingce Xia","Peiran Jin","Shufang Xie","Tao Qin","Xiang-Yang Li"],"published":"2025-10-31T17:35:53Z","updated":"2025-10-31T17:35:53Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27671v1","pdf_url":"http://arxiv.org/pdf/2510.27671v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ Can we design drugs straight from protein structures?  MolChord tackles structure-based drug design by aligning protein structures with molecular sequences and texts to generate ligands.  Why it matters: helps SBDD & drug discovery teams produce better-targeted candidates.","challenges":"üéØ Problems MolChord addresses: - Aligning protein structural representations with molecular representations (structure ‚Üî sequence/text). - Ensuring generated molecules match desired pharmacological properties. - Limited property-aware supervision for guiding molecule generation.","innovations":"‚ú® Core ideas in MolChord: - Uses NatureLM (an autoregressive model unifying text, small molecules, proteins) as the molecule generator. - Introduces a diffusion-based structure encoder to map protein structures. - Curates a property-aware dataset with preference data and refines alignment via Direct Preference Optimization (DPO). Novel twist: explicitly aligns structure, sequence/text, and property preferences in one pipeline.","experiments":"üìä Main empirical claim: - MolChord achieves state-of-the-art performance on key evaluation metrics on CrossDocked2020. - Note: the paper states SOTA on CrossDocked2020 but does not provide numeric improvement details in the abstract. ","insights":"ü§î Where this could lead: - Research: extend joint structure-sequence alignment to integrate experimental binding assay data or multi-target optimization. - Applications: improves lead optimization and virtual screening workflows; could accelerate early-stage hit discovery. Could MolChord bridge computational design and wet-lab validation more tightly?","keywords":["MolChord","Structure-based drug design","SBDD","NatureLM","autoregressive model","diffusion-based encoder","Direct Preference Optimization","DPO","CrossDocked2020","FASTA","SMILES","property-aware dataset"],"category":"machine_learning","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Can we design drugs straight from protein structures?  MolChord tackles structure-based drug design by aligning protein structures with molecular sequences and texts to generate ligands.  Why it matters: helps SBDD & drug discovery teams produce better-targeted candidates.\n\n**Challenges:** üéØ Problems MolChord addresses: - Aligning protein structural representations with molecular representations (structure ‚Üî se...","analyzed_at":"2025-11-04T15:02:39.244Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27667v1","arxiv_id":"2510.27667v1","title":"Deep learning denoising unlocks quantitative insights in operando\n  materials microscopy","abstract":"Operando microscopy provides direct insight into the dynamic chemical and\nphysical processes that govern functional materials, yet measurement noise\nlimits the effective resolution and undermines quantitative analysis. Here, we\npresent a general framework for integrating unsupervised deep learning-based\ndenoising into quantitative microscopy workflows across modalities and length\nscales. Using simulated data, we demonstrate that deep denoising preserves\nphysical fidelity, introduces minimal bias, and reduces uncertainty in model\nlearning with partial differential equation (PDE)-constrained optimization.\nApplied to experiments, denoising reveals nanoscale chemical and structural\nheterogeneity in scanning transmission X-ray microscopy (STXM) of lithium iron\nphosphate (LFP), enables automated particle segmentation and phase\nclassification in optical microscopy of graphite electrodes, and reduces\nnoise-induced variability by nearly 80% in neutron radiography to resolve\nheterogeneous lithium transport. Collectively, these results establish deep\ndenoising as a powerful, modality-agnostic enhancement that advances\nquantitative operando imaging and extends the reach of previously noise-limited\ntechniques.","authors":["Samuel Degnan-Morgenstern","Alexander E. Cohen","Rajeev Gopal","Megan Gober","George J. Nelson","Peng Bai","Martin Z. Bazant"],"published":"2025-10-31T17:34:05Z","updated":"2025-10-31T17:34:05Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27667v1","pdf_url":"http://arxiv.org/pdf/2510.27667v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ Tired of noisy operando images hiding nanoscale dynamics?  This paper introduces an unsupervised deep-learning denoising framework that restores quantitative fidelity in operando microscopy‚Äîunlocking clearer chemical/structural insight for materials and battery researchers.","challenges":"üéØ Key problems tackled: - Measurement noise limits effective resolution and undermines quantitative analysis in operando microscopy. - Noise-induced variability prevents reliable downstream inference (e.g., PDE-constrained model learning). - Existing pipelines struggle across modalities and length scales.","innovations":"‚ú® Core innovations: - An unsupervised deep denoising framework integrated into quantitative microscopy workflows. - Demonstrated modality-agnostic use across STXM, optical microscopy, and neutron radiography. - Emphasis on preserving physical fidelity and minimizing bias to enable downstream PDE-constrained optimization.","experiments":"üìä Most compelling result: The method reduced noise-induced variability by nearly 80% in neutron radiography, enabling resolution of heterogeneous lithium transport.  This proves denoising can transform previously noise-limited operando imaging into quantitative data.","insights":"ü§î What's next? - Research: Combine uncertainty-aware/physics-informed denoisers with real-time operando acquisition and PDE inversion. - Applications: Improved battery diagnostics, automated particle/phase mapping in materials discovery.  Could this make more operando techniques quantitatively reliable?","keywords":["deep denoising","unsupervised learning","operando microscopy","STXM","neutron radiography","lithium iron phosphate (LFP)","graphite electrodes","PDE-constrained optimization","modality-agnostic"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Tired of noisy operando images hiding nanoscale dynamics?  This paper introduces an unsupervised deep-learning denoising framework that restores quantitative fidelity in operando microscopy‚Äîunlocking clearer chemical/structural insight for materials and battery researchers.\n\n**Challenges:** üéØ Key problems tackled: - Measurement noise limits effective resolution and undermines quantitative analysis in operando m...","analyzed_at":"2025-11-04T15:03:07.243Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27666v1","arxiv_id":"2510.27666v1","title":"Whole-Body Proprioceptive Morphing: A Modular Soft Gripper for Robust\n  Cross-Scale Grasping","abstract":"Biological systems, such as the octopus, exhibit masterful cross-scale\nmanipulation by adaptively reconfiguring their entire form, a capability that\nremains elusive in robotics. Conventional soft grippers, while compliant, are\nmostly constrained by a fixed global morphology, and prior shape-morphing\nefforts have been largely confined to localized deformations, failing to\nreplicate this biological dexterity. Inspired by this natural exemplar, we\nintroduce the paradigm of collaborative, whole-body proprioceptive morphing,\nrealized in a modular soft gripper architecture. Our design is a distributed\nnetwork of modular self-sensing pneumatic actuators that enables the gripper to\nintelligently reconfigure its entire topology, achieving multiple morphing\nstates that are controllable to form diverse polygonal shapes. By integrating\nrich proprioceptive feedback from embedded sensors, our system can seamlessly\ntransition from a precise pinch to a large envelope grasp. We experimentally\ndemonstrate that this approach expands the grasping envelope and enhances\ngeneralization across diverse object geometries (standard and irregular) and\nscales (up to 10$\\times$), while also unlocking novel manipulation modalities\nsuch as multi-object and internal hook grasping. This work presents a low-cost,\neasy-to-fabricate, and scalable framework that fuses distributed actuation with\nintegrated sensing, offering a new pathway toward achieving biological levels\nof dexterity in robotic manipulation.","authors":["Dong Heon Han","Xiaohao Xu","Yuxi Chen","Yusheng Zhou","Xinqi Zhang","Jiaqi Wang","Daniel Bruder","Xiaonan Huang"],"published":"2025-10-31T17:34:04Z","updated":"2025-10-31T17:34:04Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27666v1","pdf_url":"http://arxiv.org/pdf/2510.27666v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ What if a robotic gripper could reconfigure its whole body like an octopus to pick tiny screws and big bottles with the same hand?  This paper introduces a modular soft gripper that performs whole-body proprioceptive morphing ‚Äî distributed self-sensing pneumatic modules that reconfigure topology for cross-scale, robust grasping.","challenges":"üéØ Key problems tackled: - Fixed global morphology: conventional soft grippers can‚Äôt change overall shape. - Localized morphing only: prior work limits deformation to small regions, losing whole-body adaptability. - Poor cross-scale generalization: hard to handle diverse object sizes and irregular geometries.","innovations":"‚ú® Core innovations: - Modular network of self-sensing pneumatic actuators. - Whole-body proprioceptive morphing: the gripper reconfigures its entire topology to form controllable polygonal shapes. - Integrated distributed sensing enables seamless transitions from precise pinch to large envelope grasps.  Novelty: combining distributed actuation + embedded proprioception to morph the entire gripper rather than only local parts.","experiments":"üìä Experimental proof: - Demonstrated expanded grasping envelope and generalization across diverse geometries and scales up to 10√ó. - Showed new modalities including multi-object grasping and internal hook grasping.  This proves whole-body morphing with proprioception can broaden what a single soft gripper can reliably pick and manipulate.","insights":"ü§î What‚Äôs next? - Research directions: integrate learned closed-loop control (vision + proprioception) for autonomous reconfiguration; explore dynamic/adaptive morphing policies via reinforcement learning. - Applications: versatile warehouse picking, cluttered multi-object manipulation, delicate biomedical or underwater manipulation. Could whole-body morphing become a standard module for generalist robot hands?","keywords":["soft gripper","proprioceptive morphing","modular actuators","pneumatic","whole-body sensing","cross-scale grasping","octopus-inspired","envelope grasp"],"category":"robotics","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ What if a robotic gripper could reconfigure its whole body like an octopus to pick tiny screws and big bottles with the same hand?  This paper introduces a modular soft gripper that performs whole-body proprioceptive morphing ‚Äî distributed self-sensing pneumatic modules that reconfigure topology for cross-scale, robust grasping.\n\n**Challenges:** üéØ Key problems tackled: - Fixed global morphology: conventional so...","analyzed_at":"2025-11-04T15:03:03.249Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27663v1","arxiv_id":"2510.27663v1","title":"Bayesian model selection and misspecification testing in imaging inverse\n  problems only from noisy and partial measurements","abstract":"Modern imaging techniques heavily rely on Bayesian statistical models to\naddress difficult image reconstruction and restoration tasks. This paper\naddresses the objective evaluation of such models in settings where ground\ntruth is unavailable, with a focus on model selection and misspecification\ndiagnosis. Existing unsupervised model evaluation methods are often unsuitable\nfor computational imaging due to their high computational cost and\nincompatibility with modern image priors defined implicitly via machine\nlearning models. We herein propose a general methodology for unsupervised model\nselection and misspecification detection in Bayesian imaging sciences, based on\na novel combination of Bayesian cross-validation and data fission, a randomized\nmeasurement splitting technique. The approach is compatible with any Bayesian\nimaging sampler, including diffusion and plug-and-play samplers. We demonstrate\nthe methodology through experiments involving various scoring rules and types\nof model misspecification, where we achieve excellent selection and detection\naccuracy with a low computational cost.","authors":["Tom Sprunck","Marcelo Pereyra","Tobias Liaudat"],"published":"2025-10-31T17:32:11Z","updated":"2025-10-31T17:32:11Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27663v1","pdf_url":"http://arxiv.org/pdf/2510.27663v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ Ever wondered how to pick or test Bayesian imaging models when you never have ground truth?  This paper introduces an unsupervised, low-cost method to do Bayesian model selection and misspecification detection from only noisy, partial measurements ‚Äî compatible with modern implicit priors (e.g., diffusion / plug‚Äëand‚Äëplay).","challenges":"üéØ Key problems tackled: - No ground truth: hard to evaluate image priors and Bayesian models from measurements alone. - Existing unsupervised methods: often too costly and incompatible with implicit learned priors. - Detecting model misspecification from partial/noisy data.","innovations":"‚ú® Core ideas / methods: - Combine Bayesian cross‚Äëvalidation with \"data fission\": randomized measurement splitting. - Use scoring rules on held‚Äëout measurement splits to evaluate models. - Works with any Bayesian imaging sampler (including diffusion and plug‚Äëand‚Äëplay). Novel twist: data fission enables principled unsupervised selection/testing for models defined implicitly by modern ML priors.","experiments":"üìä Experimental claims & proof: The abstract reports \"excellent selection and detection accuracy with a low computational cost\" across experiments using various scoring rules and misspecification types.  Concrete numeric results or benchmarks: Not specified in the paper.","insights":"ü§î What's next / broader implications: - Research: Explore adaptive or task‚Äëaware measurement splitting; extend to online/continual model monitoring in deployed systems. - Applications: Automated model validation for MRI/CT, microscopy, remote sensing where ground truth is unavailable. Could this enable safe, automated imaging pipelines that self‚Äëdetect bad priors?","keywords":["Bayesian model selection","misspecification testing","imaging inverse problems","data fission","Bayesian cross-validation","diffusion samplers","plug-and-play priors","scoring rules"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Ever wondered how to pick or test Bayesian imaging models when you never have ground truth?  This paper introduces an unsupervised, low-cost method to do Bayesian model selection and misspecification detection from only noisy, partial measurements ‚Äî compatible with modern implicit priors (e.g., diffusion / plug‚Äëand‚Äëplay).\n\n**Challenges:** üéØ Key problems tackled: - No ground truth: hard to evaluate image priors ...","analyzed_at":"2025-11-04T15:03:03.776Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27659v1","arxiv_id":"2510.27659v1","title":"Challenges in Credit Assignment for Multi-Agent Reinforcement Learning\n  in Open Agent Systems","abstract":"In the rapidly evolving field of multi-agent reinforcement learning (MARL),\nunderstanding the dynamics of open systems is crucial. Openness in MARL refers\nto the dynam-ic nature of agent populations, tasks, and agent types with-in a\nsystem. Specifically, there are three types of openness as reported in (Eck et\nal. 2023) [2]: agent openness, where agents can enter or leave the system at\nany time; task openness, where new tasks emerge, and existing ones evolve or\ndisappear; and type openness, where the capabil-ities and behaviors of agents\nchange over time. This report provides a conceptual and empirical review,\nfocusing on the interplay between openness and the credit assignment problem\n(CAP). CAP involves determining the contribution of individual agents to the\noverall system performance, a task that becomes increasingly complex in open\nenviron-ments. Traditional credit assignment (CA) methods often assume static\nagent populations, fixed and pre-defined tasks, and stationary types, making\nthem inadequate for open systems. We first conduct a conceptual analysis,\nin-troducing new sub-categories of openness to detail how events like agent\nturnover or task cancellation break the assumptions of environmental\nstationarity and fixed team composition that underpin existing CAP methods. We\nthen present an empirical study using representative temporal and structural\nalgorithms in an open environment. The results demonstrate that openness\ndirectly causes credit misattribution, evidenced by unstable loss functions and\nsignificant performance degradation.","authors":["Alireza Saleh Abadi","Leen-Kiat Soh"],"published":"2025-10-31T17:30:32Z","updated":"2025-10-31T17:30:32Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27659v1","pdf_url":"http://arxiv.org/pdf/2510.27659v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ How do you assign credit when agents can join, leave, or change on the fly?  This paper analyzes credit assignment in open MARL, introducing a finer taxonomy of openness and showing traditional credit-assignment methods fail in dynamic agent/task/type settings.  Important for multi-agent systems, robotics, and online ecosystems.","challenges":"üéØ Key problems tackled: - Agent openness: agents may enter or leave, breaking fixed-team assumptions. - Task openness: tasks can appear, evolve, or disappear, breaking stationarity. - Type openness: agent capabilities/behaviors change over time, confusing contribution signals. Traditional CA methods assume static populations/tasks/types and thus misattribute credit.","innovations":"‚ú® Core contributions & what's novel: - Introduced new sub-categories of openness to refine agent/task/type openness. - Performed an empirical study applying representative temporal and structural credit-assignment algorithms in an open environment. - Analyzed how events (agent turnover, task cancellation) concretely break CA assumptions. Novelty: explicit focus on the interplay between openness and the credit-assignment problem, both conceptually and empirically.","experiments":"üìä Most compelling quantitative result: Not specified in the paper. Main demonstrated breakthrough: empirical evidence that openness causes credit misattribution, producing unstable loss functions and significant performance degradation when using representative temporal and structural CA algorithms.","insights":"ü§î Next directions & broader impact: - Research directions: develop adaptive/continual credit-assignment methods robust to agent/task/type dynamics; create benchmarks/metrics for \"open MARL\" evaluation. - Applications: swarm robotics, dynamic multi-robot fleets, economic/market simulations where participants and tasks change. Can we design provably robust credit-assignment mechanisms for truly open agent systems?","keywords":["multi-agent reinforcement learning","open systems","credit assignment","openness taxonomy","agent turnover","task openness","type openness","credit misattribution"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ How do you assign credit when agents can join, leave, or change on the fly?  This paper analyzes credit assignment in open MARL, introducing a finer taxonomy of openness and showing traditional credit-assignment methods fail in dynamic agent/task/type settings.  Important for multi-agent systems, robotics, and online ecosystems.\n\n**Challenges:** üéØ Key problems tackled: - Agent openness: agents may enter or leav...","analyzed_at":"2025-11-04T15:03:39.696Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27655v1","arxiv_id":"2510.27655v1","title":"Community Detection on Model Explanation Graphs for Explainable AI","abstract":"Feature-attribution methods (e.g., SHAP, LIME) explain individual predictions\nbut often miss higher-order structure: sets of features that act in concert. We\npropose Modules of Influence (MoI), a framework that (i) constructs a model\nexplanation graph from per-instance attributions, (ii) applies community\ndetection to find feature modules that jointly affect predictions, and (iii)\nquantifies how these modules relate to bias, redundancy, and causality\npatterns. Across synthetic and real datasets, MoI uncovers correlated feature\ngroups, improves model debugging via module-level ablations, and localizes bias\nexposure to specific modules. We release stability and synergy metrics, a\nreference implementation, and evaluation protocols to benchmark module\ndiscovery in XAI.","authors":["Ehsan Moradi"],"published":"2025-10-31T17:27:56Z","updated":"2025-10-31T17:27:56Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27655v1","pdf_url":"http://arxiv.org/pdf/2510.27655v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ Ever feel feature-attribution maps (SHAP/LIME) miss the forest for the trees? MoI (Modules of Influence) builds a model explanation graph from per-instance attributions and uses community detection to find feature modules that act together ‚Äî helping debugging, bias checks, and richer XAI.","challenges":"üéØ Key problems tackled: - Feature-attribution methods explain single features but miss higher-order groups that act in concert. - Hard to debug models when correlated/redundant features jointly drive predictions. - Difficulty localizing bias and causal patterns to subsets of features.","innovations":"‚ú® Core novelties: - Construct a model explanation graph from per-instance feature attributions. - Apply community-detection to identify Modules of Influence (MoI) ‚Äî groups of features that jointly affect predictions. - Quantify module relations to bias, redundancy, and causality; release stability and synergy metrics + reference implementation. Novel twist: treating explanation outputs as a graph and using community detection to discover higher-order feature modules rather than single-feature attributions.","experiments":"üìä Most compelling quantitative result: Not specified in the paper. Main experimental breakthrough: Across synthetic and real datasets, MoI uncovers correlated feature groups, improves model debugging via module-level ablations, and localizes bias exposure to specific modules ‚Äî empirically demonstrating utility of module-level explanations.","insights":"ü§î What's next? - Research directions: (1) Combine MoI with causal discovery to test if discovered modules correspond to causal mechanisms. (2) Use module discovery for feature selection/model compression or to guide robust training. - Applications: fairness auditing (localize bias to modules), domain science (e.g., gene/pathway-level explanations). Could module-aware XAI become the standard for debugging complex models?","keywords":["Modules of Influence","MoI","explanation graph","community detection","feature attributions","SHAP","LIME","model debugging","bias localization","XAI","stability metric","synergy metric"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Ever feel feature-attribution maps (SHAP/LIME) miss the forest for the trees? MoI (Modules of Influence) builds a model explanation graph from per-instance attributions and uses community detection to find feature modules that act together ‚Äî helping debugging, bias checks, and richer XAI.\n\n**Challenges:** üéØ Key problems tackled: - Feature-attribution methods explain single features but miss higher-order groups ...","analyzed_at":"2025-11-04T15:03:36.098Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2510.27651v1","arxiv_id":"2510.27651v1","title":"Information-Theoretic Greedy Layer-wise Training for Traffic Sign\n  Recognition","abstract":"Modern deep neural networks (DNNs) are typically trained with a global\ncross-entropy loss in a supervised end-to-end manner: neurons need to store\ntheir outgoing weights; training alternates between a forward pass\n(computation) and a top-down backward pass (learning) which is biologically\nimplausible. Alternatively, greedy layer-wise training eliminates the need for\ncross-entropy loss and backpropagation. By avoiding the computation of\nintermediate gradients and the storage of intermediate outputs, it reduces\nmemory usage and helps mitigate issues such as vanishing or exploding\ngradients. However, most existing layer-wise training approaches have been\nevaluated only on relatively small datasets with simple deep architectures. In\nthis paper, we first systematically analyze the training dynamics of popular\nconvolutional neural networks (CNNs) trained by stochastic gradient descent\n(SGD) through an information-theoretic lens. Our findings reveal that networks\nconverge layer-by-layer from bottom to top and that the flow of information\nadheres to a Markov information bottleneck principle. Building on these\nobservations, we propose a novel layer-wise training approach based on the\nrecently developed deterministic information bottleneck (DIB) and the\nmatrix-based R\\'enyi's $\\alpha$-order entropy functional. Specifically, each\nlayer is trained jointly with an auxiliary classifier that connects directly to\nthe output layer, enabling the learning of minimal sufficient task-relevant\nrepresentations. We empirically validate the effectiveness of our training\nprocedure on CIFAR-10 and CIFAR-100 using modern deep CNNs and further\ndemonstrate its applicability to a practical task involving traffic sign\nrecognition. Our approach not only outperforms existing layer-wise training\nbaselines but also achieves performance comparable to SGD.","authors":["Shuyan Lyu","Zhanzimo Wu","Junliang Du"],"published":"2025-10-31T17:24:58Z","updated":"2025-10-31T17:24:58Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2510.27651v1","pdf_url":"http://arxiv.org/pdf/2510.27651v1.pdf","scraped_at":"2025-11-04T14:59:58.221Z","analysis":{"introduction":"üöÄ Curious how to train deep nets without backprop? This paper proposes an information-theoretic, greedy layer-wise training scheme using the Deterministic Information Bottleneck (DIB) + matrix-based R√©nyi entropy to train layers with auxiliary classifiers ‚Äî less memory, biologically plausible, validated on CIFAR & traffic signs.","challenges":"üéØ Problems solved: - Global cross-entropy + backprop is biologically implausible and memory-heavy. - Storing intermediate activations and computing full gradients hurts memory and causes vanishing/exploding gradients. - Prior layer-wise methods tested only on small/simple datasets.","innovations":"‚ú® Core innovations: - Systematic info-theoretic analysis of SGD training dynamics (layer-by-layer convergence, Markov IB behavior). - Greedy layer-wise training based on Deterministic Information Bottleneck (DIB). - Use of matrix-based R√©nyi's Œ±-order entropy functional to estimate information. - Train each layer jointly with an auxiliary classifier connected to the output to learn minimal sufficient task-relevant representations.","experiments":"üìä Key experimental takeaway: - Exact quantitative numbers: Not specified in the paper. - Qualitative result: On CIFAR-10/CIFAR-100 and a traffic sign recognition task, the method outperforms existing layer-wise training baselines and achieves performance comparable to SGD.","insights":"ü§î What's next? - Explore scaling to larger benchmarks (e.g., ImageNet) and to modern architectures (transformers/CNN hybrids). - Investigate on-device/low-memory training, continual learning or combining with self-supervised pretraining. Could layer-wise DIB enable more biologically plausible, efficient learning in deployed systems?","keywords":["greedy layer-wise training","deterministic information bottleneck","matrix-based R√©nyi entropy","auxiliary classifier","traffic sign recognition","CIFAR-10","CIFAR-100","convolutional neural networks","information-theoretic analysis","biologically plausible learning"],"category":"machine_learning","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Curious how to train deep nets without backprop? This paper proposes an information-theoretic, greedy layer-wise training scheme using the Deterministic Information Bottleneck (DIB) + matrix-based R√©nyi entropy to train layers with auxiliary classifiers ‚Äî less memory, biologically plausible, validated on CIFAR & traffic signs.\n\n**Challenges:** üéØ Problems solved: - Global cross-entropy + backprop is biologically...","analyzed_at":"2025-11-04T15:03:33.550Z","model":"openai/gpt-5-mini"}},{"id":"hf_every_activation_boosted__scaling_general_reasoner_to_1_trillion_open_language_foundation_1762268404186","title":"Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation","abstract":"We introduce Ling 2.0, a series reasoning-oriented language foundation built upon the principle that every activation boosts reasoning capability. Designed to scale from tens of billions to one trillion parameters under a unified Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity, cross-scale consistency, and efficiency guided by empirical scaling laws. The series includes three non-thinking (instruct) models - Ling-mini-2.0, Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and achieving up to 7-fold active-compute efficiency compared with dense counterparts. Ling 2.0 integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure: a high-sparsity MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale FP8 training with fine-grained heterogeneous pipelines. At the trillion scale, Ling-1T establishes a new Pareto frontier of reasoning accuracy versus computational efficiency, demonstrating that sparse activation, when properly aligned with reasoning objectives, enables scalable and efficient intelligence. Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for advancing future reasoning and thinking models, including the Ring series built upon the same base.","authors":[],"published":"2025-11-04","updated":"2025-11-04T15:00:04.186Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.22115","pdf_url":"","scraped_at":"2025-11-04T15:00:04.186Z","abstract_quality":7,"analysis":{"introduction":"üöÄ Tired of huge dense LMs wasting compute? Ling 2.0 scales reasoning from 16B to 1T parameters under a unified high-sparsity Mixture-of-Experts‚Äîclaiming up to 7√ó active-compute efficiency and a new Pareto frontier for reasoning vs cost. Aimed at efficient reasoning models.","challenges":"üéØ Problems tackled: - Compute inefficiency of dense models at large scale - Difficulty aligning sparsity with robust reasoning ability across scales - Engineering constraints for training at trillion-parameter scale (precision, heterogeneous pipelines)","innovations":"‚ú® Core methods & what's novel: - High-sparsity MoE backbone with MTP for efficient reasoning - Principle: ‚Äúevery activation boosts reasoning capability‚Äù (cross-scale consistency) - Reasoning-oriented data + mid-training Chain-of-Thought (CoT) activation - Reinforcement-based fine-tuning: DFT & Evo-CoT - Full-scale FP8 training with fine-grained heterogeneous pipelines","experiments":"üìä Key quantitative result: - Up to 7√ó active-compute efficiency vs dense counterparts (models from 16B to 1T params). - Ling-1T (1T) reportedly establishes a new Pareto frontier of reasoning accuracy vs computational efficiency.","insights":"Not provided","keywords":[],"category":"machine_learning","relevance_score":5,"summary":"**Introduction:** üöÄ Tired of huge dense LMs wasting compute? Ling 2.0 scales reasoning from 16B to 1T parameters under a unified high-sparsity Mixture-of-Experts‚Äîclaiming up to 7√ó active-compute efficiency and a new Pareto frontier for reasoning vs cost. Aimed at efficient reasoning models.\n\n**Challenges:** üéØ Problems tackled: - Compute inefficiency of dense models at large scale - Difficulty aligning sparsity with robust reasonin...","analyzed_at":"2025-11-04T15:04:20.470Z","model":"openai/gpt-5-mini"}},{"id":"hf_the_underappreciated_power_of_vision_models_for_graph_structural_understanding_1762268407991","title":"The Underappreciated Power of Vision Models for Graph Structural Understanding","abstract":"Graph Neural Networks operate through bottom-up message-passing, fundamentally differing from human visual perception, which intuitively captures global structures first. We investigate the underappreciated potential of vision models for graph understanding, finding they achieve performance comparable to GNNs on established benchmarks while exhibiting distinctly different learning patterns. These divergent behaviors, combined with limitations of existing benchmarks that conflate domain features with topological understanding, motivate our introduction of Graph. This benchmark evaluates models' ability to perceive global graph properties as humans do: recognizing organizational archetypes, detecting symmetry, sensing connectivity strength, and identifying critical elements. Our results reveal that vision models significantly outperform GNNs on tasks requiring holistic structural understanding and maintain generalizability across varying graph scales, while GNNs struggle with global pattern abstraction and degrade with increasing graph size. This work demonstrates that vision models possess remarkable yet underutilized capabilities for graph structural understanding, particularly for problems requiring global topological awareness and scale-invariant reasoning. These findings open new avenues to leverage this underappreciated potential for developing more effective graph foundation models for tasks dominated by holistic pattern recognition.","authors":[],"published":"2025-11-04","updated":"2025-11-04T15:00:07.991Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.24788","pdf_url":"","scraped_at":"2025-11-04T15:00:07.991Z","abstract_quality":7,"analysis":{"introduction":"üöÄ Can image models see graph structure better than GNNs?  This paper shows vision models can rival or beat GNNs at graph structural understanding, revealing an unexpected route to capture global topology‚Äîvaluable for ML researchers and foundation-model builders.","challenges":"üéØ Key problems tackled: - GNNs rely on local message-passing and struggle to capture global/topological patterns. - Existing benchmarks conflate domain cues with pure topology, hiding true structural ability. - Scale sensitivity: GNNs degrade as graph size grows.","innovations":"‚ú® Main contributions: - Demonstrate that off-the-shelf vision models can learn graph structure and match GNN performance on standard benchmarks. - Introduce a new benchmark, Graph., focused on global properties (archetypes, symmetry, connectivity, critical elements). - Analysis of differing learning patterns between vision models and GNNs. Novelty: repositioning vision models as effective tools for holistic, scale-invariant graph understanding.","experiments":"üìä Most compelling result: - Exact numeric improvements not specified in the paper. - Demonstrated: vision models significantly outperform GNNs on tasks requiring holistic structural understanding and generalize across graph scales, while GNNs struggle with global pattern abstraction and degrade with larger graphs.","insights":"ü§î What's next? - Explore hybrid architectures that combine vision-style global perception with GNN local reasoning. - Pretrain scale-invariant vision+graph foundation models for tasks like molecular topology, infrastructure resilience, and social-pattern detection. Could this shift how we build graph foundations?","keywords":["vision models","graph neural networks","graph structure","benchmark","global topology","scale invariance","Graph. benchmark","structural understanding"],"category":"machine_learning","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Can image models see graph structure better than GNNs?  This paper shows vision models can rival or beat GNNs at graph structural understanding, revealing an unexpected route to capture global topology‚Äîvaluable for ML researchers and foundation-model builders.\n\n**Challenges:** üéØ Key problems tackled: - GNNs rely on local message-passing and struggle to capture global/topological patterns. - Existing benchmarks ...","analyzed_at":"2025-11-04T15:04:03.081Z","model":"openai/gpt-5-mini"}},{"id":"hf_unilumos__fast_and_unified_image_and_video_relighting_with_physics_plausible_feedback_1762268412472","title":"UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback","abstract":"Relighting is a crucial task with both practical demand and artistic value, and recent diffusion models have shown strong potential by enabling rich and controllable lighting effects. However, as they are typically optimized in semantic latent space, where proximity does not guarantee physical correctness in visual space, they often produce unrealistic results, such as overexposed highlights, misaligned shadows, and incorrect occlusions. We address this with UniLumos, a unified relighting framework for both images and videos that brings RGB-space geometry feedback into a flow matching backbone. By supervising the model with depth and normal maps extracted from its outputs, we explicitly align lighting effects with the scene structure, enhancing physical plausibility. Nevertheless, this feedback requires high-quality outputs for supervision in visual space, making standard multi-step denoising computationally expensive. To mitigate this, we employ path consistency learning, allowing supervision to remain effective even under few-step training regimes. To enable fine-grained relighting control and supervision, we design a structured six-dimensional annotation protocol capturing core illumination attributes. Building upon this, we propose LumosBench, a disentangled attribute-level benchmark that evaluates lighting controllability via large vision-language models, enabling automatic and interpretable assessment of relighting precision across individual dimensions. Extensive experiments demonstrate that UniLumos achieves state-of-the-art relighting quality with significantly improved physical consistency, while delivering a 20x speedup for both image and video relighting. Code is available at https://github.com/alibaba-damo-academy/Lumos-Custom.","authors":[],"published":"2025-11-04","updated":"2025-11-04T15:00:12.472Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2511.01678","pdf_url":"","scraped_at":"2025-11-04T15:00:12.472Z","abstract_quality":7,"analysis":{"introduction":"üöÄ Tired of relighting that looks fake‚Äîblown highlights, misaligned shadows, wrong occlusions? UniLumos is a unified image+video relighting framework that injects RGB-space geometry feedback into a flow-matching backbone for more physically plausible, fast relighting for artists and vision apps.","challenges":"üéØ Key problems tackled: - Diffusion-based relighting in latent space produces physically implausible effects (overexposure, misaligned shadows, wrong occlusions). - High compute from multi-step denoising makes high-quality supervision expensive. - Limited fine-grained, attribute-level relighting control and evaluation.","innovations":"‚ú® Core innovations: - RGB-space geometry feedback: supervise with depth & normal maps extracted from model outputs to align lighting with scene structure. - Path consistency learning: keeps feedback effective under few-step denoising to cut compute. - Structured 6D illumination annotation + LumosBench: disentangled attribute benchmark using large VLMs for interpretable, attribute-level evaluation. - Unified image & video relighting in a flow-matching backbone.","experiments":"üìä Main quantitative result: Achieved a 20√ó speedup for both image and video relighting while delivering state-of-the-art relighting quality with significantly improved physical consistency (fewer unrealistic highlights/shadows and better occlusion alignment).","insights":"ü§î What's next? - Research: integrate learned geometry feedback end-to-end with real-time few-step samplers; extend to dynamic lighting in interactive or AR/VR settings. - Applications: film/post-production, real-time game/AR lighting, automated scene editing. Could this enable real-time, physically-plausible relighting on mobile devices?","keywords":["relighting","flow matching","diffusion","geometry feedback","depth maps","normal maps","path consistency learning","LumosBench","illumination control","video relighting"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Tired of relighting that looks fake‚Äîblown highlights, misaligned shadows, wrong occlusions? UniLumos is a unified image+video relighting framework that injects RGB-space geometry feedback into a flow-matching backbone for more physically plausible, fast relighting for artists and vision apps.\n\n**Challenges:** üéØ Key problems tackled: - Diffusion-based relighting in latent space produces physically implausible ef...","analyzed_at":"2025-11-04T15:04:03.103Z","model":"openai/gpt-5-mini"}},{"id":"hf_generalizing_test_time_compute_optimal_scaling_as_an_optimizable_graph_1762268416688","title":"Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph","abstract":"Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.","authors":[],"published":"2025-11-04","updated":"2025-11-04T15:00:16.688Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2511.00086","pdf_url":"","scraped_at":"2025-11-04T15:00:16.688Z","abstract_quality":7,"analysis":{"introduction":"üöÄ Want LLMs to spend extra compute at inference more wisely? This paper reframes Test-Time Scaling as a multi-LLM collaboration graph and introduces Agent-REINFORCE ‚Äî an LLM-agent method that searches compute-optimal model+architecture combos under a fixed budget. Who benefits: deployers seeking task-tailored, budget-aware inference.","challenges":"üéØ Key problems tackled: - Fixed collaboration architectures (topologies) in prior TTS work limit adaptability. - Single-model assumptions ignore benefits of multi-LLM combos. - Huge combinatorial search space + task-specific needs make finding optimal designs hard.","innovations":"‚ú® Core contributions: - Formalizes Test-Time Scaling as a probabilistic multi-LLM collaboration graph (nodes: roles+model assignments; edges: info flow). - Derives empirical insights (pilot studies) guiding search. - Agent-REINFORCE: LLM-agent-augmented pipeline that maps sampling‚Üífeedback‚Üíupdate (textual feedback acts like a gradient) to efficiently search graph space. Novel twist: using LLM-generated textual feedback as a gradient signal to update a probabilistic graph search.","experiments":"üìä Main experimental takeaway: Agent-REINFORCE outperforms traditional and LLM-based baselines in sample efficiency and search performance and finds graphs that trade off accuracy and inference latency under budget constraints. Exact numeric improvements: Not specified in the paper.","insights":"ü§î Where to next? - Research: extend probabilistic graph search to be hardware-aware (latency/cost profiles) and to adapt online per-task or per-user. - Applications: smarter multi-LLM orchestration for edge/cloud hybrid serving, multi-agent NLP pipelines. Could this enable dynamic, budget-aware LLM ensembles in production?","keywords":["Test-Time Scaling","TTS","Agent-REINFORCE","probabilistic graph","multi-LLM","inference optimization","compute-optimal","REINFORCE","LLM-agent","model collaboration"],"category":"machine_learning","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Want LLMs to spend extra compute at inference more wisely? This paper reframes Test-Time Scaling as a multi-LLM collaboration graph and introduces Agent-REINFORCE ‚Äî an LLM-agent method that searches compute-optimal model+architecture combos under a fixed budget. Who benefits: deployers seeking task-tailored, budget-aware inference.\n\n**Challenges:** üéØ Key problems tackled: - Fixed collaboration architectures (to...","analyzed_at":"2025-11-04T15:04:49.429Z","model":"openai/gpt-5-mini"}},{"id":"hf_unireditbench__a_unified_reasoning_based_image_editing_benchmark_1762268420412","title":"UniREditBench: A Unified Reasoning-based Image Editing Benchmark","abstract":"Recent advances in multi-modal generative models have driven substantial improvements in image editing. However, current generative models still struggle with handling diverse and complex image editing tasks that require implicit reasoning, underscoring the need for a comprehensive benchmark to systematically assess their performance across various reasoning scenarios. Existing benchmarks primarily focus on single-object attribute transformation in realistic scenarios, which, while effective, encounter two key challenges: (1) they largely overlook multi-object interactions as well as game-world scenarios that involve human-defined rules, which are common in real-life applications; (2) they only rely on textual references to evaluate the generated images, potentially leading to systematic misjudgments, especially in complex reasoning scenarios. To this end, this work proposes UniREditBench, a unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples, covering both real- and game-world scenarios across 8 primary dimensions and 18 sub-dimensions. To improve evaluation reliability, we introduce multimodal dual-reference evaluation, providing both textual and ground-truth image references for each sample assessment. Furthermore, we design an automated multi-scenario data synthesis pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel on this dataset and develop UniREdit-Bagel, demonstrating substantial improvements in both in-domain and out-of-distribution settings. Through thorough benchmarking of both open-source and closed-source image editing models, we reveal their strengths and weaknesses across various aspects.","authors":[],"published":"2025-11-04","updated":"2025-11-04T15:00:20.412Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2511.01295","pdf_url":"","scraped_at":"2025-11-04T15:00:20.412Z","abstract_quality":7,"analysis":{"introduction":"üöÄ Can image editors really ‚Äòreason‚Äô about complex scenes? UniREditBench introduces a unified reasoning-based image editing benchmark (2,700 curated samples) that tests models on real- and game-world edits with multimodal references ‚Äî crucial for robust, real-world editing tools.","challenges":"üéØ Key problems tackled: - Existing models struggle with diverse, implicit-reasoning editing tasks. - Benchmarks ignore multi-object interactions and game-world scenarios. - Current evaluations rely only on text references, risking misjudgment in complex cases.","innovations":"‚ú® Core contributions: - UniREditBench: 2,700 curated samples across real & game worlds, 8 primary dims, 18 sub-dims. - Multimodal dual-reference evaluation: text + ground-truth image. - Automated multi-scenario synthesis ‚Üí UniREdit-Data-100K with CoT annotations. - Fine-tuned Bagel ‚Üí UniREdit-Bagel. Novel: dual-reference eval + game-world scenarios + CoT for editing.","experiments":"üìä Most compelling quantitative result: Not specified in the paper. Main demonstrated breakthrough: UniREdit-Bagel (Bagel fine-tuned on UniREdit-Data-100K) yields substantial improvements in both in-domain and out-of-distribution image editing vs. baseline models (paper reports qualitative and benchmark gains).","insights":"ü§î Where to go next: - Research: integrate user-in-the-loop multimodal feedback and extend CoT-guided edits to temporal (video) editing. - Applications: more reliable AR/VR content creation, game asset editing, and multi-object scene manipulation tools. Could dual-reference benchmarks become the standard for robust image editing?","keywords":["image_editing_benchmark","UniREditBench","multimodal_evaluation","dual-reference","UniREdit-Data-100K","chain-of-thought","Bagel","UniREdit-Bagel","game-world_scenarios","multi-object_reasoning"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Can image editors really ‚Äòreason‚Äô about complex scenes? UniREditBench introduces a unified reasoning-based image editing benchmark (2,700 curated samples) that tests models on real- and game-world edits with multimodal references ‚Äî crucial for robust, real-world editing tools.\n\n**Challenges:** üéØ Key problems tackled: - Existing models struggle with diverse, implicit-reasoning editing tasks. - Benchmarks ignore ...","analyzed_at":"2025-11-04T15:04:51.878Z","model":"openai/gpt-5-mini"}},{"id":"hf_phuma__physically_grounded_humanoid_locomotion_dataset_1762268425722","title":"PHUMA: Physically-Grounded Humanoid Locomotion Dataset","abstract":"This paper presents research on phuma:, physically-grounded, humanoid. The full abstract is not available at this time. Please visit the paper's website for complete details about the methodology, results, and contributions.","authors":[],"published":"2025-11-04","updated":"2025-11-04T15:00:25.722Z","category":"machine-learning","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.26236","pdf_url":"","scraped_at":"2025-11-04T15:00:25.722Z","abstract_quality":5,"analysis":{"introduction":"üöÄ Can we benchmark humanoid walking with physics-aware data? PHUMA: Physically-Grounded Humanoid Locomotion Dataset ‚Äî this paper introduces PHUMA, a dataset focused on physically-grounded humanoid locomotion. Who it helps: Not specified in the paper.","challenges":"üéØ Challenges (The Problems Solved): - Not specified in the paper. - Not specified in the paper. - Not specified in the paper.","innovations":"‚ú® Innovations (The Novel Solution): - Introduces PHUMA: a physically-grounded humanoid locomotion dataset. - Further methodological details and technical novelties: Not specified in the paper.","experiments":"üìä Experiment (Proof & Breakthrough): Quantitative results, experimental setup, and demonstrated improvements: Not specified in the paper.","insights":"ü§î Insights (What's Next?): - Potential directions: apply PHUMA for sim-to-real humanoid control research; extend dataset with contact/force and multi-sensor modalities. - Potential applications: humanoid robotics control, realistic animation, biomechanics analysis. Could PHUMA accelerate physically-plausible bipedal control?","keywords":["PHUMA","humanoid","locomotion","dataset","physically-grounded","robotics","simulation","benchmark"],"category":"robotics","relevance_score":7,"technical_depth":"intermediate","summary":"**Introduction:** üöÄ Can we benchmark humanoid walking with physics-aware data? PHUMA: Physically-Grounded Humanoid Locomotion Dataset ‚Äî this paper introduces PHUMA, a dataset focused on physically-grounded humanoid locomotion. Who it helps: Not specified in the paper.\n\n**Challenges:** üéØ Challenges (The Problems Solved): - Not specified in the paper. - Not specified in the paper. - Not specified in the paper.","analyzed_at":"2025-11-04T15:04:54.589Z","model":"openai/gpt-5-mini"}},{"id":"hf_toolscope__an_agentic_framework_for_vision_guided_and_long_horizon_tool_use_1762268430430","title":"ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use","abstract":"Recently, large language models (LLMs) have demonstrated remarkable problem-solving capabilities by autonomously integrating with external tools for collaborative reasoning. However, due to the inherently complex and diverse nature of multimodal information, enabling multimodal large language models (MLLMs) to flexibly and efficiently utilize external tools during reasoning remains an underexplored challenge. In this work, we introduce ToolScope, an agentic framework designed to unify global planning with local multimodal perception, adopting a specialized Perceive tool to mitigates visual context degradation in long-horizon VQA task. ToolScope comprises three primary components: the Global Navigator, the Agentic Executor, and the Response Synthesizer. The Global Navigator functions as a \"telescope\", offering high-level strategic guidance. The Agentic Executor operates iteratively to augment MLLM with local perception through the integration of external tools-Search, Code, and Perceive. Finally, the Response Synthesizer consolidates and organizes the reasoning process into a coherent, user-friendly output. We evaluate ToolScope on four VQA benchmarks across diverse domains, including VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong generalization capabilities, achieving an average performance improvement of up to +6.69% across all datasets.","authors":[],"published":"2025-11-04","updated":"2025-11-04T15:00:30.430Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.27363","pdf_url":"","scraped_at":"2025-11-04T15:00:30.430Z","abstract_quality":7,"analysis":{"introduction":"üöÄ Want multimodal models that plan and perceive like humans for long tasks? ToolScope introduces an agentic framework that unifies global planning with local vision tools so MLLMs can call Search/Code/Perceive for long-horizon VQA ‚Äî useful for multimodal research and apps.","challenges":"üéØ Problems solved: - MLLMs struggle to flexibly call external tools during multimodal reasoning. - Visual context degradation in long-horizon VQA tasks. - The complexity and diversity of multimodal information hinder efficient tool use.","innovations":"‚ú® Core innovations: - Global Navigator: a high-level \"telescope\" planner for strategic guidance. - Agentic Executor: iterative, tool-driven local perception and action. - Perceive tool: specialized to mitigate visual context degradation in long-horizon VQA. - Response Synthesizer: consolidates reasoning into coherent outputs. Novelty: unifies global planning with local multimodal perception and agentic tool integration (Search, Code, Perceive).","experiments":"üìä Results: Evaluated on VQA 2.0, ScienceQA, MAT-Search and MathVista. ToolScope demonstrates strong generalization, reporting an average performance improvement of up to +6.69% across these datasets ‚Äî evidence that agentic planning+perception helps long-horizon VQA.","insights":"ü§î What's next? - Research: adapt ToolScope for embodied agents / continuous video streams; explore adaptive tool-selection policies and latency/efficiency trade-offs. - Applications: AR assistants, scientific image/math QA pipelines. Could agentic perception scale to real-time robotic or AR systems?","keywords":["ToolScope","agentic framework","MLLM","Perceive tool","Global Navigator","Agentic Executor","Response Synthesizer","VQA","multimodal","long-horizon reasoning"],"category":"machine_learning","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Want multimodal models that plan and perceive like humans for long tasks? ToolScope introduces an agentic framework that unifies global planning with local vision tools so MLLMs can call Search/Code/Perceive for long-horizon VQA ‚Äî useful for multimodal research and apps.\n\n**Challenges:** üéØ Problems solved: - MLLMs struggle to flexibly call external tools during multimodal reasoning. - Visual context degradation...","analyzed_at":"2025-11-04T15:05:31.418Z","model":"openai/gpt-5-mini"}},{"id":"hf_rover__benchmarking_reciprocal_cross_modal_reasoning_for_omnimodal_generation_1762268434135","title":"ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation","abstract":"Unified multimodal models (UMMs) have emerged as a powerful paradigm for seamlessly unifying text and image understanding and generation. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning, i.e., textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce ROVER to address this pressing need to test reciprocal cross-modal reasoning, the use of one modality to guide, verify, or refine outputs in the other, an ability central to the vision of unified multimodal intelligence. ROVER is a human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, which contains 1312 tasks grounded in 1876 images, spanning two complementary settings. Verbally-augmented reasoning for visual generation evaluates whether models can use verbal prompts and reasoning chains to guide faithful image synthesis. Visually-augmented reasoning for verbal generation evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes for question answering. Experiments on 17 unified models reveal two key findings: (i) Cross-modal reasoning determines visual generation quality, with interleaved models significantly outperforming non-interleaved ones; notably, combining strong unimodal models fails to achieve comparable reasoning. (ii) Models show dissociation between physical and symbolic reasoning: they succeed at interpreting perceptual concepts literally but fail to construct visual ions for symbolic tasks, where faulty reasoning harms performance. These results highlight reciprocal cross-modal reasoning as a critical frontier for enabling true omnimodal generation.","authors":[],"published":"2025-11-04","updated":"2025-11-04T15:00:34.135Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2511.01163","pdf_url":"","scraped_at":"2025-11-04T15:00:34.135Z","abstract_quality":7,"analysis":{"introduction":"üöÄ Want multimodal AIs that can think across sight and language, not just one at a time? ROVER introduces a human-annotated benchmark to test reciprocal cross-modal reasoning ‚Äî using one modality to guide, verify or refine outputs in the other. Crucial for true omnimodal generation.","challenges":"üéØ Problems tackled: - Prevailing evaluations treat visual and textual reasoning in isolation, missing cross-modal verification. - No standard benchmark for using language to steer image synthesis or images to strengthen textual reasoning. - Lack of tests for reciprocal guidance between modalities.","innovations":"‚ú® What ROVER brings: - A human-annotated benchmark of 1,312 tasks grounded in 1,876 images. - Two complementary settings: verbally-augmented reasoning for visual generation; visually-augmented reasoning for verbal generation. - Explicit focus on reciprocal cross-modal reasoning (guiding/ verifying/ refining across modalities).","experiments":"üìä Key empirical takeaway: - Evaluated 17 unified models on ROVER. Finding: cross-modal reasoning strongly drives visual generation quality ‚Äî interleaved models significantly outperform non-interleaved ones, and simply combining strong unimodal models does not match their performance. (Dataset sizes: 1,312 tasks; 1,876 images.)","insights":"ü§î What‚Äôs next? - Explore training objectives and architectures that enable symbolic visual construction and tight interleaving of perception+reasoning. - Apply reciprocal cross-modal checks to safety/verification in image generation and multimodal QA assistants. Could interleaved, reasoning-aware models unlock more reliable omnimodal agents?","keywords":["ROVER","reciprocal cross-modal reasoning","omnimodal generation","multimodal benchmark","visual generation","text generation","interleaved models"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Want multimodal AIs that can think across sight and language, not just one at a time? ROVER introduces a human-annotated benchmark to test reciprocal cross-modal reasoning ‚Äî using one modality to guide, verify or refine outputs in the other. Crucial for true omnimodal generation.\n\n**Challenges:** üéØ Problems tackled: - Prevailing evaluations treat visual and textual reasoning in isolation, missing cross-modal ve...","analyzed_at":"2025-11-04T15:05:17.567Z","model":"openai/gpt-5-mini"}},{"id":"hf_world_simulation_with_video_foundation_models_for_physical_ai_1762268440037","title":"World Simulation with Video Foundation Models for Physical AI","abstract":"This paper presents research on world, simulation, video. The full abstract is not available at this time. Please visit the paper's website for complete details about the methodology, results, and contributions.","authors":[],"published":"2025-11-04","updated":"2025-11-04T15:00:40.037Z","category":"reinforcement_learning","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2511.00062","pdf_url":"","scraped_at":"2025-11-04T15:00:40.037Z","abstract_quality":5,"analysis":{"introduction":"üöÄ What if video models could simulate entire physical worlds for robots? ü§ñüé• Main breakthrough: Not specified in the paper. Why it matters / who benefits: Not specified in the paper.","challenges":"üéØ Key problems addressed: - Not specified in the paper. - Not specified in the paper. - Not specified in the paper.","innovations":"‚ú® Core methods / novel ideas: - Not specified in the paper. - Not specified in the paper. - Not specified in the paper.","experiments":"üìä Most compelling quantitative result: Not specified in the paper. (Paper published: 2025-11-04)","insights":"ü§î Potential next steps & applications: Not specified in the paper.","keywords":["world simulation","video foundation models","physical AI","simulation","video models"],"category":"robotics","relevance_score":6,"technical_depth":"Not specified in the paper.","summary":"**Introduction:** üöÄ What if video models could simulate entire physical worlds for robots? ü§ñüé• Main breakthrough: Not specified in the paper. Why it matters / who benefits: Not specified in the paper.\n\n**Challenges:** üéØ Key problems addressed: - Not specified in the paper. - Not specified in the paper. - Not specified in the paper.","analyzed_at":"2025-11-04T15:05:34.725Z","model":"openai/gpt-5-mini"}},{"id":"hf_towards_universal_video_retrieval__generalizing_video_embedding_via_synthesized_multimodal_pyramid_curriculum_1762268443407","title":"Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum","abstract":"The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval.","authors":[],"published":"2025-11-04","updated":"2025-11-04T15:00:43.407Z","category":"reinforcement_learning","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2510.27571","pdf_url":"","scraped_at":"2025-11-04T15:00:43.407Z","abstract_quality":7,"analysis":{"introduction":"üöÄ Feeling stuck with video search that only works on narrow tests? This paper proposes a path to truly universal video retrieval: a co-designed suite (UVRB), 1.55M synthesized multimodal pairs, and a Modality Pyramid curriculum to train a General Video Embedder (GVE). Who benefits? Anyone building robust, cross-domain video search.","challenges":"üéØ Problems tackled: - Structural misalignment: current paradigm and benchmarks are too narrow. - Data & task gap: limited data and single-task training hinder universal capability. - Lack of diagnostics: no evaluation that demands multi-dimensional generalization.","innovations":"‚ú® Core novelties: - UVRB: a diagnostic benchmark of 16 datasets to measure multi-dimensional generalization. - Scalable synthesis workflow: generates 1.55M high-quality multimodal pairs to fill semantic gaps. - Modality Pyramid curriculum: training schedule that exploits latent interconnections across diverse data to train GVE. - Co-design approach: evaluation, data, and modeling designed together (the unique twist).","experiments":"üìä Key result: - GVE achieves state-of-the-art zero-shot generalization on the new UVRB benchmark, showing the framework enables broad transfer. - Additional finding: popular benchmarks are poor predictors of general ability and partially relevant retrieval is a dominant scenario. - Numeric improvements not specified in the paper.","insights":"ü§î Next steps & applications: - Research: explore curriculum variants that adapt to downstream tasks (e.g., relevance-graded retrieval) and synth data realism vs. scale trade-offs. - Applications: large-scale cross-domain video search, content moderation, multimedia knowledge discovery. Could a synthesized multimodal curriculum power universally robust retrieval across industries?","keywords":["video retrieval","zero-shot generalization","multimodal synthesis","benchmarking","curriculum learning","video embedding","UVRB","Modality Pyramid","GVE"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Feeling stuck with video search that only works on narrow tests? This paper proposes a path to truly universal video retrieval: a co-designed suite (UVRB), 1.55M synthesized multimodal pairs, and a Modality Pyramid curriculum to train a General Video Embedder (GVE). Who benefits? Anyone building robust, cross-domain video search.\n\n**Challenges:** üéØ Problems tackled: - Structural misalignment: current paradigm a...","analyzed_at":"2025-11-04T15:06:00.675Z","model":"openai/gpt-5-mini"}}]