{"date":"2025-10-18","papers":[{"id":"hf_when_models_lie__we_learn__multilingual_span_level_hallucination_detection_with_psiloqa_1760779439435","title":"When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA","authors":[],"abstract":"Hallucination detection remains a fundamental challenge for the safe and reliable deployment of large language models (LLMs), especially in applications requiring factual accuracy. Existing hallucination benchmarks often operate at the sequence level and are limited to English, lacking the fine-grained, multilingual supervision needed for a comprehensive evaluation. In this work, we introduce PsiloQA, a large-scale, multilingual dataset annotated with span-level hallucinations across 14 languages. PsiloQA is constructed through an automated three-stage pipeline: generating question-answer pairs from Wikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse LLMs in a no-context setting, and automatically annotating hallucinated spans using GPT-4o by comparing against golden answers and retrieved context. We evaluate a wide range of hallucination detection methods -- including uncertainty quantification, LLM-based tagging, and fine-tuned encoder models -- and show that encoder-based models achieve the strongest performance across languages. Furthermore, PsiloQA demonstrates effective cross-lingual generalization and supports robust knowledge transfer to other benchmarks, all while being significantly more cost-efficient than human-annotated datasets. Our dataset and results advance the development of scalable, fine-grained hallucination detection in multilingual settings.","published":"2025-10-18","source":"huggingface","url":"https://huggingface.co/papers/2510.04849","analysis":{"introduction":"ðŸš€ Want LLMs to stop confidently inventing facts? Meet PsiloQA â€” a new multilingual, span-level hallucination dataset across 14 languages built with an automated GPT-4o pipeline. It aims to pinpoint exactly which spans lie, helping safer, more factual LLMs for global users.","challenges":"ðŸŽ¯ Key problems tackled: - Existing benchmarks are mostly sequence-level and English-only, so they miss fine-grained errors. - Lack of multilingual, span-level supervision makes detection and evaluation brittle. - Human annotation at scale is costly and slow.","innovations":"âœ¨ Core contributions: - PsiloQA: large-scale multilingual dataset with span-level hallucination annotations (14 languages). - Automated three-stage pipeline: (1) QA generation from Wikipedia with GPT-4o, (2) elicit no-context answers from diverse LLMs, (3) automatic span-level annotation via GPT-4o vs gold + retrieved context. - Systematic eval of uncertainty metrics, LLM tagging, and fine-tuned encoder detectors. Novelty: fine-grained, multilingual span annotations created cost-efficiently with an automated pipeline.","experiments":"ðŸ“Š Quantitative highlight: Not specified in the paper. Qualitative findings: encoder-based models achieved the strongest performance across languages; PsiloQA supports cross-lingual generalization and transfers to other benchmarks while being far more cost-efficient than human annotation.","insights":"ðŸ¤” Next directions & applications: - Research: adapt span-level detectors to context-rich or long-document settings and stress-test on low-resource languages. - Applications: integrate span-level alerts in retrieval-augmented generation systems and fact-checking pipelines. Could automated multilingual span labels enable real-time hallucination mitigation in deployed LLMs?","keywords":["hallucination detection","PsiloQA","multilingual","span-level","LLMs","GPT-4o","encoder models","cross-lingual generalization","automated annotation"],"category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want LLMs to stop confidently inventing facts? Meet PsiloQA â€” a new multilingual, span-level hallucination dataset across 14 languages built with an automated GPT-4o pipeline. It aims to pinpoint exactly which spans lie, helping safer, more factual LLMs for global users.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Existing benchmarks are mostly sequence-level and English-only, so they miss fine-grained errors. -...","analyzed_at":"2025-10-18T09:25:51.019Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-18T09:23:59.435Z","views":0,"archive_metadata":{"archived_at":"2025-10-18T09:26:40.741Z","original_id":"hf_when_models_lie__we_learn__multilingual_span_level_hallucination_detection_with_psiloqa_1760779439435","views_at_archive":0}},{"id":"hf_information_gain_based_policy_optimization__a_simple_and_effective_approach_for_multi_turn_llm_agents_1760779463323","title":"Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents","authors":[],"abstract":"Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.","published":"2025-10-18","source":"huggingface","url":"https://huggingface.co/papers/2510.14967","analysis":{"introduction":"ðŸš€ Stuck with sparse final-reward training for LLM agents? IGPO reframes multi-turn interaction as incremental info acquisition: give dense, intrinsic turn-level rewards based on the model's own belief updates to train better multi-turn agents. Helps multi-turn reasoning & efficiency.","challenges":"ðŸŽ¯ Key problems tackled: - Reward sparsity: outcome-only rewards at final answer give little signal. - Advantage collapse: long trajectories make rollouts indistinguishable. - Poor credit assignment: turn-level dependencies vanish in long-horizon tasks.","innovations":"âœ¨ Core ideas: - Introduced Information Gain-based Policy Optimization (IGPO). - Turn-level intrinsic rewards = marginal increase in policy probability of the correct answer (belief updates). - Derived from the model's own belief updates (no external reward model or costly Monte Carlo) and combined with outcome-level rewards for dense trajectories.","experiments":"ðŸ“Š Quantitative highlight: Not specified in the paper. Qualitative result: Experiments (in-domain & out-of-domain) show IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.","insights":"ðŸ¤” What's next? - Research: Analyze theoretical properties of belief-update rewards; extend IGPO to mixed-modality agents or hierarchical planners. - Applications: better search-based assistants, more sample-efficient multi-step tool-using agents. Could IGPO generalize to robotics or multimodal planning?","keywords":["IGPO","information gain","intrinsic rewards","reinforcement learning","LLM agents","multi-turn","credit assignment","sparse rewards","sample efficiency"],"category":"reinforcement_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Stuck with sparse final-reward training for LLM agents? IGPO reframes multi-turn interaction as incremental info acquisition: give dense, intrinsic turn-level rewards based on the model's own belief updates to train better multi-turn agents. Helps multi-turn reasoning & efficiency.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Reward sparsity: outcome-only rewards at final answer give little signal. - Advantage co...","analyzed_at":"2025-10-18T09:26:37.820Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-18T09:24:23.323Z","views":0,"archive_metadata":{"archived_at":"2025-10-18T09:26:40.741Z","original_id":"hf_information_gain_based_policy_optimization__a_simple_and_effective_approach_for_multi_turn_llm_agents_1760779463323","views_at_archive":0}},{"id":"hf_bitnet_distillation_1760779467647","title":"BitNet Distillation","authors":[],"abstract":"In this paper, we present BitNet Distillation (BitDistill), a lightweight\npipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into\n1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream\ntasks, achieving strong task-specific performance with minimal computational\ncost. Specifically, BitDistill incorporates three key techniques: the SubLN\nmodule, as introduced in BitNet; multi-head attention distillation, based on\nMiniLM; and continual pre-training, which serves as a crucial warm-up step to\nmitigate the scalability issue of the performance gap between finetuned\nfull-precision and 1.58-bit LLMs on specific tasks. Experimental results show\nthat BitDistill achieves performance comparable to the full-precision\ncounterpart models across model size, while enabling up to 10x memory savings\nand 2.65x faster inference on CPUs. Code is available at\nhttps://github.com/microsoft/BitNet.","published":"2025-10-18","source":"huggingface","url":"https://huggingface.co/papers/2510.13998","analysis":{"introduction":"ðŸš€ Want to run big LLMs with tiny memory? BitDistill fine-tunes off-the-shelf full-precision LLMs into 1.58-bit ternary models (weights = {-1,0,1}) for downstream tasks â€” keeping performance while slashing compute and memory. Great for CPU/edge deployments.","challenges":"ðŸŽ¯ Key problems addressed: - Large memory & compute cost of full-precision LLMs for task-specific use. - Big performance gap when naively quantizing fine-tuned models to very low bits. - Need for a lightweight pipeline to get task-specific ternary models efficiently.","innovations":"âœ¨ Core ideas & whatâ€™s novel: - SubLN module (from BitNet) integrated into fine-tuning. - Multi-head attention distillation (inspired by MiniLM) to transfer task knowledge. - Continual pre-training as a warm-up to close the scalability/performance gap. Novel twist: combining these to produce 1.58-bit ternary LLMs that match full-precision task performance.","experiments":"ðŸ“Š Main empirical highlight: BitDistill yields 1.58-bit (ternary) models that achieve performance comparable to full-precision counterparts across model sizes, while offering up to 10x memory savings and 2.65x faster CPU inference.","insights":"ðŸ¤” Where next? - Research: explore hardware-aware quantization (activations/mixed-precision) and combining BitDistill with pruning or LoRA for even smaller personalized models. - Applications: on-device assistants, low-cost cloud inference for task-specialized LLMs. Could this enable mainstream edge LLMs?","keywords":["quantization","ternary","1.58-bit","LLM","BitDistill","SubLN","multi-head attention distillation","continual pre-training","model_compression","inference_efficiency"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want to run big LLMs with tiny memory? BitDistill fine-tunes off-the-shelf full-precision LLMs into 1.58-bit ternary models (weights = {-1,0,1}) for downstream tasks â€” keeping performance while slashing compute and memory. Great for CPU/edge deployments.\n\n**Challenges:** ðŸŽ¯ Key problems addressed: - Large memory & compute cost of full-precision LLMs for task-specific use. - Big performance gap when naively quant...","analyzed_at":"2025-10-18T09:26:40.600Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-18T09:24:27.647Z","views":0,"archive_metadata":{"archived_at":"2025-10-18T09:26:40.741Z","original_id":"hf_bitnet_distillation_1760779467647","views_at_archive":0}},{"id":"hf_paddleocr_vl__boosting_multilingual_document_parsing_via_a_0_9b_ultra_compact_vision_language_model_1760779471414","title":"PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model","authors":[],"abstract":"In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios.","published":"2025-10-18","source":"huggingface","url":"https://huggingface.co/papers/2510.14528","analysis":{"introduction":"ðŸš€ Ever wanted a powerful document parser that runs cheaply and supports dozens of languages?  PaddleOCR-VL introduces a 0.9B ultra-compact vision-language model combining a NaViT-style dynamic-resolution visual encoder with ERNIE-4.5-0.3B to enable SOTA multilingual document parsing for 109 languages â€” fast and resource-efficient.","challenges":"ðŸŽ¯ Problems tackled: - Large VLMs consume heavy compute and memory, hindering deployment on constrained hardware. - Multilingual document parsing across many scripts/languages is hard to scale. - Reliable recognition of complex elements (tables, formulas, charts) remains challenging.","innovations":"âœ¨ Core contributions: - PaddleOCR-VL-0.9B: an ultra-compact (0.9B) vision-language model. - NaViT-style dynamic-resolution visual encoder to handle variable image detail efficiently. - Integration with ERNIE-4.5-0.3B language model for robust multimodal understanding. - Supports 109 languages while keeping minimal resource consumption. Novelty: combining dynamic-resolution visual encoding with a small LLM to reach SOTA document parsing with low compute.","experiments":"ðŸ“Š Results & proof: The paper reports SOTA performance on widely used public benchmarks and internal benchmarks for both page-level document parsing and element-level recognition, significantly outperforming existing solutions and matching competitiveness with top-tier VLMs while running fast and resource-efficiently. Exact numeric gains Not specified in the paper.","insights":"ðŸ¤” What's next (potential ideas): - Explore on-device/edge deployment and quantization strategies tailored to the NaViT + ERNIE stack. - Investigate few-shot or continual learning to adapt the compact VLM to new document layouts or low-resource languages. Potential broader impact: real-time form processing, mobile document assistants, automated compliance/audit pipelines. Could this enable universal, low-cost document understanding everywhere?","keywords":["PaddleOCR-VL","vision-language","NaViT","ERNIE-4.5-0.3B","document parsing","multilingual OCR","compact models","0.9B","109 languages"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Ever wanted a powerful document parser that runs cheaply and supports dozens of languages?  PaddleOCR-VL introduces a 0.9B ultra-compact vision-language model combining a NaViT-style dynamic-resolution visual encoder with ERNIE-4.5-0.3B to enable SOTA multilingual document parsing for 109 languages â€” fast and resource-efficient.\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Large VLMs consume heavy compute and memory,...","analyzed_at":"2025-10-18T09:26:40.703Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-18T09:24:31.414Z","views":0,"archive_metadata":{"archived_at":"2025-10-18T09:26:40.741Z","original_id":"hf_paddleocr_vl__boosting_multilingual_document_parsing_via_a_0_9b_ultra_compact_vision_language_model_1760779471414","views_at_archive":0}},{"id":"hf_laser__reinforcement_learning_with_last_token_self_rewarding_1760779460127","title":"LaSeR: Reinforcement Learning with Last-Token Self-Rewarding","authors":[],"abstract":"Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.","published":"2025-10-18","source":"huggingface","url":"https://huggingface.co/papers/2510.14943","analysis":{"introduction":"ðŸš€ Want faster, self-verifying LLM reasoning?  LaSeR shows a surprising shortcut: the true reasoning reward can be read from a model's last-token next-token log-prob difference.  Result: unify reasoning + verification with only one extra token inference â€” faster and test-time usable.","challenges":"ðŸŽ¯ Problems tackled: - No verification signals available at test time for RLVR. - Prior approaches require sequential solution+verifier generation (two prompts) â€” slow and inefficient. - Hard to jointly train reasoning and self-verification without extra runtime cost.","innovations":"âœ¨ Key ideas: - Theoretical reduction: RL self-verification reward = last-token self-reward score (next-token log-prob diff scaled by KL). - LaSeR algorithm: augment RLVR loss with an MSE loss to align last-token self-reward scores to verifier rewards. - Minimal runtime cost: uses next-token distribution at the final token (one extra token inference).","experiments":"ðŸ“Š Not specified in the paper. Qualitative result: experiments report improved reasoning performance and a strong learned self-rewarding capability, which boosts inference-time scaling performance compared to prior RLVR practice.","insights":"ðŸ¤” What's next? - Research: extend from a single last-token signal to multi-token self-rewarding or combine LaSeR with external verifiers for hybrid verification. - Applications: more efficient on-device LLM reasoning, safer autonomous agents that self-assess answers. Could this enable cheap, reliable test-time verification?","keywords":["RLVR","LaSeR","self-rewarding","last-token","reinforcement learning","LLM verification","MSE alignment","next-token probability"],"category":"machine_learning","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want faster, self-verifying LLM reasoning?  LaSeR shows a surprising shortcut: the true reasoning reward can be read from a model's last-token next-token log-prob difference.  Result: unify reasoning + verification with only one extra token inference â€” faster and test-time usable.\n\n**Challenges:** ðŸŽ¯ Problems tackled: - No verification signals available at test time for RLVR. - Prior approaches require sequentia...","analyzed_at":"2025-10-18T09:26:17.001Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-18T09:24:20.127Z","views":0,"archive_metadata":{"archived_at":"2025-10-18T09:26:40.741Z","original_id":"hf_laser__reinforcement_learning_with_last_token_self_rewarding_1760779460127","views_at_archive":0}},{"id":"arxiv_2510.14973v1","title":"Attention Is All You Need for KV Cache in Diffusion LLMs","authors":["Quan Nguyen-Tri","Mukul Ranjan","Zhiqiang Shen"],"abstract":"This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.","published":"2025-10-16T17:59:48Z","source":"arxiv","url":"http://arxiv.org/abs/2510.14973v1","analysis":{"introduction":"ðŸš€ Tired of slow diffusion LLM decoding? Elastic-Cache adaptively refreshes KV caches (when + where) to cut redundant QKV recomputation â€” training-free and architecture-agnostic. Big win: much lower decoding latency for diffusion LLMs (math & code tasks).","challenges":"ðŸŽ¯ Problems tackled: - Existing decoders recompute QKV for all tokens at every denoising step & layer â†’ massive redundancy. - KV states change little across steps (esp. shallow layers) but are still recomputed. - Latency/throughput bottlenecks make diffusion LLMs impractical for deployment.","innovations":"âœ¨ Key ideas: - Attention-aware drift test on the most-attended token to decide WHEN to refresh. - Depth-aware schedule: recompute from a chosen deeper layer onward (reuse shallow-layer caches). - Block-wise caching for off-window MASK tokens. Novel twist: jointly deciding WHEN+WHERE using the most-attended token as a conservative KV-drift bound.","experiments":"ðŸ“Š Standout results: - Up to 45.1Ã— decoding speedup on longer sequences; 8.7Ã— on GSM8K (256 tokens); 4.8Ã— on HumanEval. - Throughput: 6.8Ã— on GSM8K. Tests on LLaDA-Instruct / LLaDA-1.5 / LLaDA-V (math & code) with negligible quality loss and higher accuracy than baseline.","insights":"ðŸ¤” Next steps & impacts: - Research: extend adaptive cache refresh to other transformer decoders; combine Elastic-Cache with confidence-based heuristics or hardware-aware scheduling. - Applications: lower-latency, energy-efficient inference for real-time assistants and code/math tools. Could this enable wider deployment of diffusion LLMs?","keywords":["Elastic-Cache","KV cache","diffusion LLMs","attention-aware drift","depth-aware schedule","MASK caching","LLaDA","decoding latency"],"category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Tired of slow diffusion LLM decoding? Elastic-Cache adaptively refreshes KV caches (when + where) to cut redundant QKV recomputation â€” training-free and architecture-agnostic. Big win: much lower decoding latency for diffusion LLMs (math & code tasks).\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Existing decoders recompute QKV for all tokens at every denoising step & layer â†’ massive redundancy. - KV states change li...","analyzed_at":"2025-10-18T09:25:33.821Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-18T09:23:51.132Z","views":0,"archive_metadata":{"archived_at":"2025-10-18T09:26:40.741Z","original_id":"arxiv_2510.14973v1","views_at_archive":0}},{"id":"arxiv_2510.14979v1","title":"From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale","authors":["Haiwen Diao","Mingxuan Li","Silei Wu","Linjun Dai","Xiaohua Wang","Hanming Deng","Lewei Lu","Dahua Lin","Ziwei Liu"],"abstract":"The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.","published":"2025-10-16T17:59:58Z","source":"arxiv","url":"http://arxiv.org/abs/2510.14979v1","analysis":{"introduction":"ðŸš€ Curious if a single model can learn pixels and words natively? Meet NEO: a family of native Vision-Language Models built from first principles that align pixelâ†”word in a shared space and aim to rival modular VLMs. Important for researchers and practitioners seeking compact, unified multimodal systems.","challenges":"ðŸŽ¯ Key problems tackled: - Unclear fundamental limits of native VLMs vs modular VLMs and whether those barriers can be overcome. - Lack of accessibility and democratization for native VLM research. - Difficulty aligning pixel and word representations and integrating vision+language strengths in a single model.","innovations":"âœ¨ Core contributions: - NEO: a novel family of native VLMs designed from first principles. - Introduces native VLM primitives that align pixel and word representations in a shared semantic space. - Builds a dense, monolithic model that integrates vision and language capabilities and embodies cross-modal properties. - Provides reusable components to foster a cost-effective, extensible ecosystem (code & models released).","experiments":"ðŸ“Š Experimental highlight: - Quantitative: NEO is trained with only 390M imageâ€“text examples and reportedly develops visual perception from scratch while mitigating visionâ€“language conflicts. - What it proves: Native, monolithic VLMs can be competitive with top-tier modular counterparts. - Note: The abstract does not report specific benchmark numbers or % improvements.","insights":"ðŸ¤” What's next (potential directions & impact): - Research direction: Study scaling laws and trade-offs between monolithic native primitives and modular architectures (efficiency, robustness, interpretability). - Application idea: Apply native VLM primitives to embodied agents, multimodal search, or low-cost deployment scenarios to test real-world gains. Could native native VLMs simplify multimodal stacks across industries?","keywords":["NEO","native VLM","vision-language","pixel-word alignment","monolithic model","cross-modal primitives","multimodal","open-source","390M image-text"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Curious if a single model can learn pixels and words natively? Meet NEO: a family of native Vision-Language Models built from first principles that align pixelâ†”word in a shared space and aim to rival modular VLMs. Important for researchers and practitioners seeking compact, unified multimodal systems.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Unclear fundamental limits of native VLMs vs modular VLMs and whethe...","analyzed_at":"2025-10-18T09:24:49.652Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-18T09:23:51.132Z","views":0,"archive_metadata":{"archived_at":"2025-10-18T09:26:40.741Z","original_id":"arxiv_2510.14979v1","views_at_archive":0}},{"id":"arxiv_2510.14977v1","title":"Terra: Explorable Native 3D World Model with Point Latents","authors":["Yuanhui Huang","Weiliang Chen","Wenzhao Zheng","Xin Tao","Pengfei Wan","Jie Zhou","Jiwen Lu"],"abstract":"World models have garnered increasing attention for comprehensive modeling of\nthe real world. However, most existing methods still rely on pixel-aligned\nrepresentations as the basis for world evolution, neglecting the inherent 3D\nnature of the physical world. This could undermine the 3D consistency and\ndiminish the modeling efficiency of world models. In this paper, we present\nTerra, a native 3D world model that represents and generates explorable\nenvironments in an intrinsic 3D latent space. Specifically, we propose a novel\npoint-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into\na latent point representation, which is subsequently decoded as 3D Gaussian\nprimitives to jointly model geometry and appearance. We then introduce a sparse\npoint flow matching network (SPFlow) for generating the latent point\nrepresentation, which simultaneously denoises the positions and features of the\npoint latents. Our Terra enables exact multi-view consistency with native 3D\nrepresentation and architecture, and supports flexible rendering from any\nviewpoint with only a single generation process. Furthermore, Terra achieves\nexplorable world modeling through progressive generation in the point latent\nspace. We conduct extensive experiments on the challenging indoor scenes from\nScanNet v2. Terra achieves state-of-the-art performance in both reconstruction\nand generation with high 3D consistency.","published":"2025-10-16T17:59:56Z","source":"arxiv","url":"http://arxiv.org/abs/2510.14977v1","analysis":{"introduction":"ðŸš€ What if world models were truly 3D-native instead of pixel-aligned? Terra proposes a native 3D world model that uses point latents to represent and generate explorable environments in an intrinsic 3D latent space â€” enabling exact multi-view consistency and flexible single-pass rendering. Benefits: better 3D consistency for VR/robotics/simulations.","challenges":"ðŸŽ¯ Key problems tackled: - Existing world models rely on pixel-aligned representations, neglecting the physical world's 3D nature. - This undermines 3D consistency across views and reduces modeling efficiency. - Limited support for explorable, progressive 3D generation in latent space.","innovations":"âœ¨ Core contributions: - Point-to-Gaussian VAE (P2G-VAE): encodes 3D inputs into latent point representations and decodes them as 3D Gaussian primitives to jointly model geometry & appearance. - Sparse Point Flow (SPFlow): a flow/matching network that generates point latents while denoising positions and features. - Native 3D latent architecture enabling exact multi-view consistency and single-generation flexible rendering. - Progressive generation in point latent space to enable explorable worlds. Novelty: a point-latent â†’ Gaussian-primitive pipeline plus a sparse flow generator, bringing a true 3D-native latent world model.","experiments":"ðŸ“Š Results: Terra was evaluated on challenging indoor scenes from ScanNet v2 and achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency. Specific numeric improvements or % gains: Not specified in the paper.","insights":"ðŸ¤” Where this could lead: - Future research: extend Terra to dynamic scenes (temporal point-latents) and scale to larger/outdoor environments or denser primitives. - Applications: AR/VR scene authoring, robot navigation/sim-to-real environments, game content generation. Could point-latent world models become the standard backbone for interactive 3D agents and content creation?","keywords":["point latents","P2G-VAE","SPFlow","3D world model","Gaussian primitives","multi-view consistency","ScanNet v2","explorable environments"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ What if world models were truly 3D-native instead of pixel-aligned? Terra proposes a native 3D world model that uses point latents to represent and generate explorable environments in an intrinsic 3D latent space â€” enabling exact multi-view consistency and flexible single-pass rendering. Benefits: better 3D consistency for VR/robotics/simulations.","analyzed_at":"2025-10-18T09:25:08.154Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-18T09:23:51.132Z","views":0,"archive_metadata":{"archived_at":"2025-10-18T09:26:40.741Z","original_id":"arxiv_2510.14977v1","views_at_archive":0}},{"id":"arxiv_2510.14974v1","title":"pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation","authors":["Hansheng Chen","Kai Zhang","Hao Tan","Leonidas Guibas","Gordon Wetzstein","Sai Bi"],"abstract":"Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard $\\ell_2$ flow matching loss. By simply\nmimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.","published":"2025-10-16T17:59:51Z","source":"arxiv","url":"http://arxiv.org/abs/2510.14974v1","analysis":{"introduction":"ðŸš€ Ever wish high-quality generative models could sample in just a few steps? pi-Flow introduces a policy-based flow student that predicts a network-free policy to generate dynamic velocities â€” enabling fast, few-step ODE sampling that preserves teacher quality. Who wins: image generative models and large-model deployment.","challenges":"ðŸŽ¯ Problems tackled: - Format mismatch: teacher velocity predictors vs. student \"shortcut\" predictors causes complex distillation. - Qualityâ€“diversity trade-off in existing few-step distillation. - High cost of extra network evaluations for substep integration.","innovations":"âœ¨ Core ideas: - pi-Flow: student flow modifies output layer to predict a network-free policy at one timestep. - Policy generates dynamic flow velocities for future substeps with negligible overhead. - Imitation distillation: match policy's velocity to the teacher's along the policy trajectory using an L2 flow-matching loss. Novelty: network-free dynamic policy enabling accurate ODE substeps without extra network calls, avoiding prior qualityâ€“diversity compromises.","experiments":"ðŸ“Š Key result: On ImageNet 256^2 pi-Flow achieves a 1-NFE FID of 2.85, outperforming MeanFlow with the same DiT backbone. Also: on FLUX.1-12B and Qwen-Image-20B at 4 NFEs, pi-Flow attains substantially better diversity than state-of-the-art few-step methods while maintaining teacher-level quality.","insights":"ðŸ¤” Next steps & applications: - Explore conditional or multimodal extensions (e.g., text-to-image) and hierarchical policies for even fewer NFEs. - Apply imitation-distilled policies to video or fast on-device sampling to cut inference cost. Could pi-Flow become the standard for practical, low-cost high-quality sampling?","keywords":["pi-Flow","imitation distillation","flow matching","few-step generation","ODE integration","ImageNet","FID","DiT","quality-diversity tradeoff","generative_models"],"category":"generative_models","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Ever wish high-quality generative models could sample in just a few steps? pi-Flow introduces a policy-based flow student that predicts a network-free policy to generate dynamic velocities â€” enabling fast, few-step ODE sampling that preserves teacher quality. Who wins: image generative models and large-model deployment.\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Format mismatch: teacher velocity predictors vs. stud...","analyzed_at":"2025-10-18T09:25:34.780Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-18T09:23:51.132Z","views":0,"archive_metadata":{"archived_at":"2025-10-18T09:26:40.741Z","original_id":"arxiv_2510.14974v1","views_at_archive":0}},{"id":"arxiv_2510.14978v1","title":"Learning an Image Editing Model without Image Editing Pairs","authors":["Nupur Kumari","Sheng-Yu Wang","Nanxuan Zhao","Yotam Nitzan","Yuheng Li","Krishna Kumar Singh","Richard Zhang","Eli Shechtman","Jun-Yan Zhu","Xun Huang"],"abstract":"Recent image editing models have achieved impressive results while following\nnatural language editing instructions, but they rely on supervised fine-tuning\nwith large datasets of input-target pairs. This is a critical bottleneck, as\nsuch naturally occurring pairs are hard to curate at scale. Current workarounds\nuse synthetic training pairs that leverage the zero-shot capabilities of\nexisting models. However, this can propagate and magnify the artifacts of the\npretrained model into the final trained model. In this work, we present a new\ntraining paradigm that eliminates the need for paired data entirely. Our\napproach directly optimizes a few-step diffusion model by unrolling it during\ntraining and leveraging feedback from vision-language models (VLMs). For each\ninput and editing instruction, the VLM evaluates if an edit follows the\ninstruction and preserves unchanged content, providing direct gradients for\nend-to-end optimization. To ensure visual fidelity, we incorporate distribution\nmatching loss (DMD), which constrains generated images to remain within the\nimage manifold learned by pretrained models. We evaluate our method on standard\nbenchmarks and include an extensive ablation study. Without any paired data,\nour method performs on par with various image editing diffusion models trained\non extensive supervised paired data, under the few-step setting. Given the same\nVLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.","published":"2025-10-16T17:59:57Z","source":"arxiv","url":"http://arxiv.org/abs/2510.14978v1","analysis":{"introduction":"ðŸš€ Tired of chasing curated inputâ€“target image pairs? This paper trains an image editing diffusion model with NO paired data â€” it unrolls a few-step diffusion during training and uses visionâ€“language models for direct feedback. Big win for scalable, instruction-driven editing.","challenges":"ðŸŽ¯ Key problems tackled: - Paired inputâ€“target image editing data are scarce and hard to collect. - Synthetic training pairs can propagate and magnify pretrained-model artifacts. - Reliance on supervised fine-tuning with large paired datasets limits scalability.","innovations":"âœ¨ Core innovations: - Unroll a few-step diffusion model during training for end-to-end optimization. - Use a visionâ€“language model (VLM) to score edits on instruction-following and content preservation, providing direct gradients. - Introduce Distribution Matching Loss (DMD) to keep outputs on the pretrained image manifold. Novel twist: removes need for any paired image-edit examples by combining unrolled diffusion + VLM feedback.","experiments":"ðŸ“Š Main empirical claim: Without any paired data, the method performs on par with image-editing diffusion models trained on large supervised paired datasets in the few-step setting, and it outperforms RL-based Flow-GRPO when using the same VLM. (No numerical % improvements provided.)","insights":"ðŸ¤” What's next? - Research directions: scale to longer multi-step/high-res edits and explore stronger or specialized VLM critics; study stability/robustness of VLM gradients. - Applications: scalable photo retouching, content-preserving creative tools, assistive editing for non-experts. Could VLM-guided, unrolled training replace supervised pair collections for more editing tasks?","keywords":["diffusion models","image editing","vision-language model","unrolled training","distribution matching loss","DMD","no-paired-data","few-step editing","Flow-GRPO"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Tired of chasing curated inputâ€“target image pairs? This paper trains an image editing diffusion model with NO paired data â€” it unrolls a few-step diffusion during training and uses visionâ€“language models for direct feedback. Big win for scalable, instruction-driven editing.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Paired inputâ€“target image editing data are scarce and hard to collect. - Synthetic training pair...","analyzed_at":"2025-10-18T09:25:17.470Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-18T09:23:51.132Z","views":1,"archive_metadata":{"archived_at":"2025-10-18T09:26:40.741Z","original_id":"arxiv_2510.14978v1","views_at_archive":1}}],"metadata":{"total_papers":10,"categories":{"natural_language_processing":2,"reinforcement_learning":1,"machine_learning":5,"generative_models":1,"computer_vision":1},"sources":{"huggingface":5,"arxiv":5},"average_score":8.9,"unique_keywords":["hallucination detection","PsiloQA","multilingual","span-level","LLMs","GPT-4o","encoder models","cross-lingual generalization","automated annotation","IGPO","information gain","intrinsic rewards","reinforcement learning","LLM agents","multi-turn","credit assignment","sparse rewards","sample efficiency","quantization","ternary","1.58-bit","LLM","BitDistill","SubLN","multi-head attention distillation","continual pre-training","model_compression","inference_efficiency","PaddleOCR-VL","vision-language","NaViT","ERNIE-4.5-0.3B","document parsing","multilingual OCR","compact models","0.9B","109 languages","RLVR","LaSeR","self-rewarding","last-token","LLM verification","MSE alignment","next-token probability","Elastic-Cache","KV cache","diffusion LLMs","attention-aware drift","depth-aware schedule","MASK caching"],"total_views":1,"created_at":"2025-10-18T09:26:40.741Z","source":"daily_update","auto_archived":true,"papers_archived":10,"total_papers_analyzed":18}}