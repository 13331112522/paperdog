{"date":"2025-11-04","papers":[{"id":"hf_generalizing_test_time_compute_optimal_scaling_as_an_optimizable_graph_1762268416688","title":"Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph","authors":[],"abstract":"Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.","published":"2025-11-04","source":"huggingface","url":"https://huggingface.co/papers/2511.00086","analysis":{"introduction":"ðŸš€ Want LLMs to spend extra compute at inference more wisely? This paper reframes Test-Time Scaling as a multi-LLM collaboration graph and introduces Agent-REINFORCE â€” an LLM-agent method that searches compute-optimal model+architecture combos under a fixed budget. Who benefits: deployers seeking task-tailored, budget-aware inference.","challenges":"ðŸŽ¯ Key problems tackled: - Fixed collaboration architectures (topologies) in prior TTS work limit adaptability. - Single-model assumptions ignore benefits of multi-LLM combos. - Huge combinatorial search space + task-specific needs make finding optimal designs hard.","innovations":"âœ¨ Core contributions: - Formalizes Test-Time Scaling as a probabilistic multi-LLM collaboration graph (nodes: roles+model assignments; edges: info flow). - Derives empirical insights (pilot studies) guiding search. - Agent-REINFORCE: LLM-agent-augmented pipeline that maps samplingâ†’feedbackâ†’update (textual feedback acts like a gradient) to efficiently search graph space. Novel twist: using LLM-generated textual feedback as a gradient signal to update a probabilistic graph search.","experiments":"ðŸ“Š Main experimental takeaway: Agent-REINFORCE outperforms traditional and LLM-based baselines in sample efficiency and search performance and finds graphs that trade off accuracy and inference latency under budget constraints. Exact numeric improvements: Not specified in the paper.","insights":"ðŸ¤” Where to next? - Research: extend probabilistic graph search to be hardware-aware (latency/cost profiles) and to adapt online per-task or per-user. - Applications: smarter multi-LLM orchestration for edge/cloud hybrid serving, multi-agent NLP pipelines. Could this enable dynamic, budget-aware LLM ensembles in production?","keywords":["Test-Time Scaling","TTS","Agent-REINFORCE","probabilistic graph","multi-LLM","inference optimization","compute-optimal","REINFORCE","LLM-agent","model collaboration"],"category":"machine_learning","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want LLMs to spend extra compute at inference more wisely? This paper reframes Test-Time Scaling as a multi-LLM collaboration graph and introduces Agent-REINFORCE â€” an LLM-agent method that searches compute-optimal model+architecture combos under a fixed budget. Who benefits: deployers seeking task-tailored, budget-aware inference.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Fixed collaboration architectures (to...","analyzed_at":"2025-11-04T15:04:49.429Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-11-04T15:00:16.688Z","views":0,"archive_metadata":{"archived_at":"2025-11-04T15:06:00.715Z","original_id":"hf_generalizing_test_time_compute_optimal_scaling_as_an_optimizable_graph_1762268416688","views_at_archive":0}},{"id":"hf_towards_universal_video_retrieval__generalizing_video_embedding_via_synthesized_multimodal_pyramid_curriculum_1762268443407","title":"Towards Universal Video Retrieval: Generalizing Video Embedding via Synthesized Multimodal Pyramid Curriculum","authors":[],"abstract":"The prevailing video retrieval paradigm is structurally misaligned, as narrow benchmarks incentivize correspondingly limited data and single-task training. Therefore, universal capability is suppressed due to the absence of a diagnostic evaluation that defines and demands multi-dimensional generalization. To break this cycle, we introduce a framework built on the co-design of evaluation, data, and modeling. First, we establish the Universal Video Retrieval Benchmark (UVRB), a suite of 16 datasets designed not only to measure performance but also to diagnose critical capability gaps across tasks and domains. Second, guided by UVRB's diagnostics, we introduce a scalable synthesis workflow that generates 1.55 million high-quality pairs to populate the semantic space required for universality. Finally, we devise the Modality Pyramid, a curriculum that trains our General Video Embedder (GVE) by explicitly leveraging the latent interconnections within our diverse data. Extensive experiments show GVE achieves state-of-the-art zero-shot generalization on UVRB. In particular, our analysis reveals that popular benchmarks are poor predictors of general ability and that partially relevant retrieval is a dominant but overlooked scenario. Overall, our co-designed framework provides a practical path to escape the limited scope and advance toward truly universal video retrieval.","published":"2025-11-04","source":"huggingface","url":"https://huggingface.co/papers/2510.27571","analysis":{"introduction":"ðŸš€ Feeling stuck with video search that only works on narrow tests? This paper proposes a path to truly universal video retrieval: a co-designed suite (UVRB), 1.55M synthesized multimodal pairs, and a Modality Pyramid curriculum to train a General Video Embedder (GVE). Who benefits? Anyone building robust, cross-domain video search.","challenges":"ðŸŽ¯ Problems tackled: - Structural misalignment: current paradigm and benchmarks are too narrow. - Data & task gap: limited data and single-task training hinder universal capability. - Lack of diagnostics: no evaluation that demands multi-dimensional generalization.","innovations":"âœ¨ Core novelties: - UVRB: a diagnostic benchmark of 16 datasets to measure multi-dimensional generalization. - Scalable synthesis workflow: generates 1.55M high-quality multimodal pairs to fill semantic gaps. - Modality Pyramid curriculum: training schedule that exploits latent interconnections across diverse data to train GVE. - Co-design approach: evaluation, data, and modeling designed together (the unique twist).","experiments":"ðŸ“Š Key result: - GVE achieves state-of-the-art zero-shot generalization on the new UVRB benchmark, showing the framework enables broad transfer. - Additional finding: popular benchmarks are poor predictors of general ability and partially relevant retrieval is a dominant scenario. - Numeric improvements not specified in the paper.","insights":"ðŸ¤” Next steps & applications: - Research: explore curriculum variants that adapt to downstream tasks (e.g., relevance-graded retrieval) and synth data realism vs. scale trade-offs. - Applications: large-scale cross-domain video search, content moderation, multimedia knowledge discovery. Could a synthesized multimodal curriculum power universally robust retrieval across industries?","keywords":["video retrieval","zero-shot generalization","multimodal synthesis","benchmarking","curriculum learning","video embedding","UVRB","Modality Pyramid","GVE"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Feeling stuck with video search that only works on narrow tests? This paper proposes a path to truly universal video retrieval: a co-designed suite (UVRB), 1.55M synthesized multimodal pairs, and a Modality Pyramid curriculum to train a General Video Embedder (GVE). Who benefits? Anyone building robust, cross-domain video search.\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Structural misalignment: current paradigm a...","analyzed_at":"2025-11-04T15:06:00.675Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-11-04T15:00:43.407Z","views":0,"archive_metadata":{"archived_at":"2025-11-04T15:06:00.715Z","original_id":"hf_towards_universal_video_retrieval__generalizing_video_embedding_via_synthesized_multimodal_pyramid_curriculum_1762268443407","views_at_archive":0}},{"id":"hf_unireditbench__a_unified_reasoning_based_image_editing_benchmark_1762268420412","title":"UniREditBench: A Unified Reasoning-based Image Editing Benchmark","authors":[],"abstract":"Recent advances in multi-modal generative models have driven substantial improvements in image editing. However, current generative models still struggle with handling diverse and complex image editing tasks that require implicit reasoning, underscoring the need for a comprehensive benchmark to systematically assess their performance across various reasoning scenarios. Existing benchmarks primarily focus on single-object attribute transformation in realistic scenarios, which, while effective, encounter two key challenges: (1) they largely overlook multi-object interactions as well as game-world scenarios that involve human-defined rules, which are common in real-life applications; (2) they only rely on textual references to evaluate the generated images, potentially leading to systematic misjudgments, especially in complex reasoning scenarios. To this end, this work proposes UniREditBench, a unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples, covering both real- and game-world scenarios across 8 primary dimensions and 18 sub-dimensions. To improve evaluation reliability, we introduce multimodal dual-reference evaluation, providing both textual and ground-truth image references for each sample assessment. Furthermore, we design an automated multi-scenario data synthesis pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel on this dataset and develop UniREdit-Bagel, demonstrating substantial improvements in both in-domain and out-of-distribution settings. Through thorough benchmarking of both open-source and closed-source image editing models, we reveal their strengths and weaknesses across various aspects.","published":"2025-11-04","source":"huggingface","url":"https://huggingface.co/papers/2511.01295","analysis":{"introduction":"ðŸš€ Can image editors really â€˜reasonâ€™ about complex scenes? UniREditBench introduces a unified reasoning-based image editing benchmark (2,700 curated samples) that tests models on real- and game-world edits with multimodal references â€” crucial for robust, real-world editing tools.","challenges":"ðŸŽ¯ Key problems tackled: - Existing models struggle with diverse, implicit-reasoning editing tasks. - Benchmarks ignore multi-object interactions and game-world scenarios. - Current evaluations rely only on text references, risking misjudgment in complex cases.","innovations":"âœ¨ Core contributions: - UniREditBench: 2,700 curated samples across real & game worlds, 8 primary dims, 18 sub-dims. - Multimodal dual-reference evaluation: text + ground-truth image. - Automated multi-scenario synthesis â†’ UniREdit-Data-100K with CoT annotations. - Fine-tuned Bagel â†’ UniREdit-Bagel. Novel: dual-reference eval + game-world scenarios + CoT for editing.","experiments":"ðŸ“Š Most compelling quantitative result: Not specified in the paper. Main demonstrated breakthrough: UniREdit-Bagel (Bagel fine-tuned on UniREdit-Data-100K) yields substantial improvements in both in-domain and out-of-distribution image editing vs. baseline models (paper reports qualitative and benchmark gains).","insights":"ðŸ¤” Where to go next: - Research: integrate user-in-the-loop multimodal feedback and extend CoT-guided edits to temporal (video) editing. - Applications: more reliable AR/VR content creation, game asset editing, and multi-object scene manipulation tools. Could dual-reference benchmarks become the standard for robust image editing?","keywords":["image_editing_benchmark","UniREditBench","multimodal_evaluation","dual-reference","UniREdit-Data-100K","chain-of-thought","Bagel","UniREdit-Bagel","game-world_scenarios","multi-object_reasoning"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Can image editors really â€˜reasonâ€™ about complex scenes? UniREditBench introduces a unified reasoning-based image editing benchmark (2,700 curated samples) that tests models on real- and game-world edits with multimodal references â€” crucial for robust, real-world editing tools.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Existing models struggle with diverse, implicit-reasoning editing tasks. - Benchmarks ignore ...","analyzed_at":"2025-11-04T15:04:51.878Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-11-04T15:00:20.412Z","views":0,"archive_metadata":{"archived_at":"2025-11-04T15:06:00.715Z","original_id":"hf_unireditbench__a_unified_reasoning_based_image_editing_benchmark_1762268420412","views_at_archive":0}},{"id":"hf_rover__benchmarking_reciprocal_cross_modal_reasoning_for_omnimodal_generation_1762268434135","title":"ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation","authors":[],"abstract":"Unified multimodal models (UMMs) have emerged as a powerful paradigm for seamlessly unifying text and image understanding and generation. However, prevailing evaluations treat these abilities in isolation, such that tasks with multimodal inputs and outputs are scored primarily through unimodal reasoning, i.e., textual benchmarks emphasize language-based reasoning, while visual benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce ROVER to address this pressing need to test reciprocal cross-modal reasoning, the use of one modality to guide, verify, or refine outputs in the other, an ability central to the vision of unified multimodal intelligence. ROVER is a human-annotated benchmark that explicitly targets reciprocal cross-modal reasoning, which contains 1312 tasks grounded in 1876 images, spanning two complementary settings. Verbally-augmented reasoning for visual generation evaluates whether models can use verbal prompts and reasoning chains to guide faithful image synthesis. Visually-augmented reasoning for verbal generation evaluates whether models can generate intermediate visualizations that strengthen their own reasoning processes for question answering. Experiments on 17 unified models reveal two key findings: (i) Cross-modal reasoning determines visual generation quality, with interleaved models significantly outperforming non-interleaved ones; notably, combining strong unimodal models fails to achieve comparable reasoning. (ii) Models show dissociation between physical and symbolic reasoning: they succeed at interpreting perceptual concepts literally but fail to construct visual ions for symbolic tasks, where faulty reasoning harms performance. These results highlight reciprocal cross-modal reasoning as a critical frontier for enabling true omnimodal generation.","published":"2025-11-04","source":"huggingface","url":"https://huggingface.co/papers/2511.01163","analysis":{"introduction":"ðŸš€ Want multimodal AIs that can think across sight and language, not just one at a time? ROVER introduces a human-annotated benchmark to test reciprocal cross-modal reasoning â€” using one modality to guide, verify or refine outputs in the other. Crucial for true omnimodal generation.","challenges":"ðŸŽ¯ Problems tackled: - Prevailing evaluations treat visual and textual reasoning in isolation, missing cross-modal verification. - No standard benchmark for using language to steer image synthesis or images to strengthen textual reasoning. - Lack of tests for reciprocal guidance between modalities.","innovations":"âœ¨ What ROVER brings: - A human-annotated benchmark of 1,312 tasks grounded in 1,876 images. - Two complementary settings: verbally-augmented reasoning for visual generation; visually-augmented reasoning for verbal generation. - Explicit focus on reciprocal cross-modal reasoning (guiding/ verifying/ refining across modalities).","experiments":"ðŸ“Š Key empirical takeaway: - Evaluated 17 unified models on ROVER. Finding: cross-modal reasoning strongly drives visual generation quality â€” interleaved models significantly outperform non-interleaved ones, and simply combining strong unimodal models does not match their performance. (Dataset sizes: 1,312 tasks; 1,876 images.)","insights":"ðŸ¤” Whatâ€™s next? - Explore training objectives and architectures that enable symbolic visual construction and tight interleaving of perception+reasoning. - Apply reciprocal cross-modal checks to safety/verification in image generation and multimodal QA assistants. Could interleaved, reasoning-aware models unlock more reliable omnimodal agents?","keywords":["ROVER","reciprocal cross-modal reasoning","omnimodal generation","multimodal benchmark","visual generation","text generation","interleaved models"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want multimodal AIs that can think across sight and language, not just one at a time? ROVER introduces a human-annotated benchmark to test reciprocal cross-modal reasoning â€” using one modality to guide, verify or refine outputs in the other. Crucial for true omnimodal generation.\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Prevailing evaluations treat visual and textual reasoning in isolation, missing cross-modal ve...","analyzed_at":"2025-11-04T15:05:17.567Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-11-04T15:00:34.135Z","views":0,"archive_metadata":{"archived_at":"2025-11-04T15:06:00.715Z","original_id":"hf_rover__benchmarking_reciprocal_cross_modal_reasoning_for_omnimodal_generation_1762268434135","views_at_archive":0}},{"id":"hf_toolscope__an_agentic_framework_for_vision_guided_and_long_horizon_tool_use_1762268430430","title":"ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool Use","authors":[],"abstract":"Recently, large language models (LLMs) have demonstrated remarkable problem-solving capabilities by autonomously integrating with external tools for collaborative reasoning. However, due to the inherently complex and diverse nature of multimodal information, enabling multimodal large language models (MLLMs) to flexibly and efficiently utilize external tools during reasoning remains an underexplored challenge. In this work, we introduce ToolScope, an agentic framework designed to unify global planning with local multimodal perception, adopting a specialized Perceive tool to mitigates visual context degradation in long-horizon VQA task. ToolScope comprises three primary components: the Global Navigator, the Agentic Executor, and the Response Synthesizer. The Global Navigator functions as a \"telescope\", offering high-level strategic guidance. The Agentic Executor operates iteratively to augment MLLM with local perception through the integration of external tools-Search, Code, and Perceive. Finally, the Response Synthesizer consolidates and organizes the reasoning process into a coherent, user-friendly output. We evaluate ToolScope on four VQA benchmarks across diverse domains, including VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong generalization capabilities, achieving an average performance improvement of up to +6.69% across all datasets.","published":"2025-11-04","source":"huggingface","url":"https://huggingface.co/papers/2510.27363","analysis":{"introduction":"ðŸš€ Want multimodal models that plan and perceive like humans for long tasks? ToolScope introduces an agentic framework that unifies global planning with local vision tools so MLLMs can call Search/Code/Perceive for long-horizon VQA â€” useful for multimodal research and apps.","challenges":"ðŸŽ¯ Problems solved: - MLLMs struggle to flexibly call external tools during multimodal reasoning. - Visual context degradation in long-horizon VQA tasks. - The complexity and diversity of multimodal information hinder efficient tool use.","innovations":"âœ¨ Core innovations: - Global Navigator: a high-level \"telescope\" planner for strategic guidance. - Agentic Executor: iterative, tool-driven local perception and action. - Perceive tool: specialized to mitigate visual context degradation in long-horizon VQA. - Response Synthesizer: consolidates reasoning into coherent outputs. Novelty: unifies global planning with local multimodal perception and agentic tool integration (Search, Code, Perceive).","experiments":"ðŸ“Š Results: Evaluated on VQA 2.0, ScienceQA, MAT-Search and MathVista. ToolScope demonstrates strong generalization, reporting an average performance improvement of up to +6.69% across these datasets â€” evidence that agentic planning+perception helps long-horizon VQA.","insights":"ðŸ¤” What's next? - Research: adapt ToolScope for embodied agents / continuous video streams; explore adaptive tool-selection policies and latency/efficiency trade-offs. - Applications: AR assistants, scientific image/math QA pipelines. Could agentic perception scale to real-time robotic or AR systems?","keywords":["ToolScope","agentic framework","MLLM","Perceive tool","Global Navigator","Agentic Executor","Response Synthesizer","VQA","multimodal","long-horizon reasoning"],"category":"machine_learning","relevance_score":8,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Want multimodal models that plan and perceive like humans for long tasks? ToolScope introduces an agentic framework that unifies global planning with local vision tools so MLLMs can call Search/Code/Perceive for long-horizon VQA â€” useful for multimodal research and apps.\n\n**Challenges:** ðŸŽ¯ Problems solved: - MLLMs struggle to flexibly call external tools during multimodal reasoning. - Visual context degradation...","analyzed_at":"2025-11-04T15:05:31.418Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-11-04T15:00:30.430Z","views":0,"archive_metadata":{"archived_at":"2025-11-04T15:06:00.715Z","original_id":"hf_toolscope__an_agentic_framework_for_vision_guided_and_long_horizon_tool_use_1762268430430","views_at_archive":0}},{"id":"arxiv_2510.27677v1","title":"Vision Transformer for Robust Occluded Person Reidentification in\n  Complex Surveillance Scenes","authors":["Bo Li","Duyuan Zheng","Xinyang Liu","Qingwen Li","Hong Li","Hongyan Cui","Ge Gao","Chen Liu"],"abstract":"Person re-identification (ReID) in surveillance is challenged by occlusion,\nviewpoint distortion, and poor image quality. Most existing methods rely on\ncomplex modules or perform well only on clear frontal images. We propose Sh-ViT\n(Shuffling Vision Transformer), a lightweight and robust model for occluded\nperson ReID. Built on ViT-Base, Sh-ViT introduces three components: First, a\nShuffle module in the final Transformer layer to break spatial correlations and\nenhance robustness to occlusion and blur; Second, scenario-adapted augmentation\n(geometric transforms, erasing, blur, and color adjustment) to simulate\nsurveillance conditions; Third, DeiT-based knowledge distillation to improve\nlearning with limited labels.To support real-world evaluation, we construct the\nMyTT dataset, containing over 10,000 pedestrians and 30,000+ images from base\nstation inspections, with frequent equipment occlusion and camera variations.\nExperiments show that Sh-ViT achieves 83.2% Rank-1 and 80.1% mAP on MyTT,\noutperforming CNN and ViT baselines, and 94.6% Rank-1 and 87.5% mAP on\nMarket1501, surpassing state-of-the-art methods.In summary, Sh-ViT improves\nrobustness to occlusion and blur without external modules, offering a practical\nsolution for surveillance-based personnel monitoring.","published":"2025-10-31T17:43:50Z","source":"arxiv","url":"http://arxiv.org/abs/2510.27677v1","analysis":{"introduction":"ðŸš€ Frustrated with ReID breaking under occlusion or blur? Sh-ViT (Shuffling Vision Transformer) is a lightweight ViT-Base model that boosts robustness to occlusion and poor-quality surveillance images â€” no external modules required â€” built for real-world monitoring.","challenges":"ðŸŽ¯ Key problems tackled: - Occlusion: people partially blocked by equipment or objects. - Poor image quality & viewpoint distortion: blur and camera variation in surveillance. - Limited labeled data for robust training (paper uses distillation to help).","innovations":"âœ¨ What they introduce: - Shuffle module in the final Transformer layer: breaks spatial correlations to improve robustness to occlusion/blur. - Scenario-adapted augmentation: geometric transforms, erasing, blur, color tweaks to simulate surveillance. - DeiT-based knowledge distillation to improve learning with limited labels. Novelty: robustness gain via internal token shuffling instead of external occlusion modules.","experiments":"ðŸ“Š Results & proof: Sh-ViT achieves 83.2% Rank-1 & 80.1% mAP on the new MyTT dataset (real base-station surveillance), and 94.6% Rank-1 & 87.5% mAP on Market1501 â€” outperforming CNN/ViT baselines and reported SOTA, showing improved occlusion/blur robustness.","insights":"ðŸ¤” Where this can lead: - Research: combine token shuffling with explicit occlusion masks or temporal fusion for multi-camera tracking; explore domain adaptation for diverse surveillance sites. - Applications: more robust crowd/security monitoring, retail analytics, edge deployment for station cameras. Could this replace heavy occlusion modules in practical deployments?","keywords":["Vision Transformer","Occluded ReID","Shuffle module","Data augmentation","Knowledge distillation","MyTT dataset","Surveillance"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Frustrated with ReID breaking under occlusion or blur? Sh-ViT (Shuffling Vision Transformer) is a lightweight ViT-Base model that boosts robustness to occlusion and poor-quality surveillance images â€” no external modules required â€” built for real-world monitoring.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Occlusion: people partially blocked by equipment or objects. - Poor image quality & viewpoint distortion: b...","analyzed_at":"2025-11-04T15:01:53.123Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-11-04T14:59:58.221Z","views":1,"archive_metadata":{"archived_at":"2025-11-04T15:06:00.715Z","original_id":"arxiv_2510.27677v1","views_at_archive":1}},{"id":"arxiv_2510.27672v1","title":"Culture Cartography: Mapping the Landscape of Cultural Knowledge","authors":["Caleb Ziems","William Held","Jane Yu","Amir Goldberg","David Grusky","Diyi Yang"],"abstract":"To serve global users safely and productively, LLMs need culture-specific\nknowledge that might not be learned during pre-training. How do we find such\nknowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The\nmost common solutions are single-initiative: either researchers define\nchallenging questions that users passively answer (traditional annotation), or\nusers actively produce data that researchers structure as benchmarks (knowledge\nextraction). The process would benefit from mixed-initiative collaboration,\nwhere users guide the process to meaningfully reflect their cultures, and LLMs\nsteer the process towards more challenging questions that meet the researcher's\ngoals. We propose a mixed-initiative methodology called CultureCartography.\nHere, an LLM initializes annotation with questions for which it has\nlow-confidence answers, making explicit both its prior knowledge and the gaps\ntherein. This allows a human respondent to fill these gaps and steer the model\ntowards salient topics through direct edits. We implement this methodology as a\ntool called CultureExplorer. Compared to a baseline where humans answer\nLLM-proposed questions, we find that CultureExplorer more effectively produces\nknowledge that leading models like DeepSeek R1 and GPT-4o are missing, even\nwith web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B\nby up to 19.2% on related culture benchmarks.","published":"2025-10-31T17:37:34Z","source":"arxiv","url":"http://arxiv.org/abs/2510.27672v1","analysis":{"introduction":"ðŸš€ Can LLMs truly understand local culture?  CultureCartography introduces a mixed-initiative approach where LLMs expose their knowledge gaps and humans fill them via CultureExplorer â€” a practical way to build culture-specific knowledge for safer, more useful AI for global users. ðŸŒ","challenges":"ðŸŽ¯ Problems tackled: - Culture-specific knowledge often missing from pretraining, hurting global usability. - Single-initiative workflows (only annotation or only extraction) fail to reflect what communities find salient. - LLMs don't explicitly reveal which cultural gaps they have.","innovations":"âœ¨ Key ideas: - Mixed-initiative methodology: CultureCartography. - Tool: CultureExplorer â€” LLMs propose low-confidence questions, humans answer/edit to steer topics. - Novel twist: models explicitly expose gaps and users guide content, creating targeted cultural knowledge.","experiments":"ðŸ“Š Results: - Fine-tuning on collected data boosted Llama-3.1-8B accuracy by up to 19.2% on related culture benchmarks. - CultureExplorer produced knowledge that leading models (DeepSeek R1, GPT-4o) were missing, even vs. web search â€” showing mixed-initiative data fills real gaps.","insights":"ðŸ¤” What's next? - Extend mixed-initiative mapping to marginalized or domain-specific knowledge (e.g., local laws, dialects). - Build community-curated, updateable cultural KBs to keep models current. Could this reshape localization and culturally-aware assistants? ðŸŒ","keywords":["CultureCartography","CultureExplorer","mixed-initiative","cultural knowledge","LLMs","fine-tuning","Llama-3.1-8B","GPT-4o","DeepSeek R1"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Can LLMs truly understand local culture?  CultureCartography introduces a mixed-initiative approach where LLMs expose their knowledge gaps and humans fill them via CultureExplorer â€” a practical way to build culture-specific knowledge for safer, more useful AI for global users. ðŸŒ\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Culture-specific knowledge often missing from pretraining, hurting global usability. - Single-...","analyzed_at":"2025-11-04T15:02:22.360Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-11-04T14:59:58.221Z","views":0,"archive_metadata":{"archived_at":"2025-11-04T15:06:00.715Z","original_id":"arxiv_2510.27672v1","views_at_archive":0}},{"id":"arxiv_2510.27688v1","title":"Continuous Autoregressive Language Models","authors":["Chenze Shao","Darren Li","Fandong Meng","Jie Zhou"],"abstract":"The efficiency of large language models (LLMs) is fundamentally limited by\ntheir sequential, token-by-token generation process. We argue that overcoming\nthis bottleneck requires a new design axis for LLM scaling: increasing the\nsemantic bandwidth of each generative step. To this end, we introduce\nContinuous Autoregressive Language Models (CALM), a paradigm shift from\ndiscrete next-token prediction to continuous next-vector prediction. CALM uses\na high-fidelity autoencoder to compress a chunk of K tokens into a single\ncontinuous vector, from which the original tokens can be reconstructed with\nover 99.9\\% accuracy. This allows us to model language as a sequence of\ncontinuous vectors instead of discrete tokens, which reduces the number of\ngenerative steps by a factor of K. The paradigm shift necessitates a new\nmodeling toolkit; therefore, we develop a comprehensive likelihood-free\nframework that enables robust training, evaluation, and controllable sampling\nin the continuous domain. Experiments show that CALM significantly improves the\nperformance-compute trade-off, achieving the performance of strong discrete\nbaselines at a significantly lower computational cost. More importantly, these\nfindings establish next-vector prediction as a powerful and scalable pathway\ntowards ultra-efficient language models. Code:\nhttps://github.com/shaochenze/calm. Project:\nhttps://shaochenze.github.io/blog/2025/CALM.","published":"2025-10-31T17:58:11Z","source":"arxiv","url":"http://arxiv.org/abs/2510.27688v1","analysis":{"introduction":"ðŸš€ Ever frustrated by slow token-by-token LLM outputs? CALM flips the script: it predicts continuous next-vectors instead of tokens, using an autoencoder to compress K tokens into one vector (reconstructed >99.9%). Fewer steps â†’ big efficiency gains for inference-heavy apps.","challenges":"ðŸŽ¯ Problems tackled: - Sequential token-by-token generation bottlenecks LLM throughput. - Low semantic bandwidth per generation step makes scaling inefficient. - Existing models/tools donâ€™t support robust training/evaluation in a continuous generative domain.","innovations":"âœ¨ Innovations: - High-fidelity autoencoder that compresses K tokens into a single continuous vector (>99.9% reconstruction). - Continuous autoregressive modeling: next-vector prediction replaces next-token prediction. - A likelihood-free framework for training, evaluation, and controllable sampling in continuous space. Novelty: a paradigm shift from discrete tokens to continuous vectors, reducing steps by factor K.","experiments":"ðŸ“Š Experiment highlight: The autoencoder reconstructs K-token chunks with >99.9% accuracy. CALM models sequences of continuous vectors, cutting generative steps by a factor of K, and achieves the performance of strong discrete baselines at significantly lower computational cost. Exact benchmark numbers: Not specified in the paper.","insights":"ðŸ¤” Whatâ€™s next? - Research directions: adaptive/variable chunk sizes (dynamic K) and hybrid discreteâ€“continuous decoding strategies. - Applications: faster, lower-cost LLM inference for chatbots and on-device models; potential energy/cost savings in deployment. Could next-vector prediction be the key to ultra-efficient LLMs?","keywords":["CALM","continuous autoregressive","next-vector prediction","autoencoder","likelihood-free","LLM efficiency","semantic bandwidth"],"category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Ever frustrated by slow token-by-token LLM outputs? CALM flips the script: it predicts continuous next-vectors instead of tokens, using an autoencoder to compress K tokens into one vector (reconstructed >99.9%). Fewer steps â†’ big efficiency gains for inference-heavy apps.\n\n**Challenges:** ðŸŽ¯ Problems tackled: - Sequential token-by-token generation bottlenecks LLM throughput. - Low semantic bandwidth per generati...","analyzed_at":"2025-11-04T15:01:25.510Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-11-04T14:59:58.221Z","views":10,"archive_metadata":{"archived_at":"2025-11-04T15:06:00.715Z","original_id":"arxiv_2510.27688v1","views_at_archive":10}},{"id":"arxiv_2510.27666v1","title":"Whole-Body Proprioceptive Morphing: A Modular Soft Gripper for Robust\n  Cross-Scale Grasping","authors":["Dong Heon Han","Xiaohao Xu","Yuxi Chen","Yusheng Zhou","Xinqi Zhang","Jiaqi Wang","Daniel Bruder","Xiaonan Huang"],"abstract":"Biological systems, such as the octopus, exhibit masterful cross-scale\nmanipulation by adaptively reconfiguring their entire form, a capability that\nremains elusive in robotics. Conventional soft grippers, while compliant, are\nmostly constrained by a fixed global morphology, and prior shape-morphing\nefforts have been largely confined to localized deformations, failing to\nreplicate this biological dexterity. Inspired by this natural exemplar, we\nintroduce the paradigm of collaborative, whole-body proprioceptive morphing,\nrealized in a modular soft gripper architecture. Our design is a distributed\nnetwork of modular self-sensing pneumatic actuators that enables the gripper to\nintelligently reconfigure its entire topology, achieving multiple morphing\nstates that are controllable to form diverse polygonal shapes. By integrating\nrich proprioceptive feedback from embedded sensors, our system can seamlessly\ntransition from a precise pinch to a large envelope grasp. We experimentally\ndemonstrate that this approach expands the grasping envelope and enhances\ngeneralization across diverse object geometries (standard and irregular) and\nscales (up to 10$\\times$), while also unlocking novel manipulation modalities\nsuch as multi-object and internal hook grasping. This work presents a low-cost,\neasy-to-fabricate, and scalable framework that fuses distributed actuation with\nintegrated sensing, offering a new pathway toward achieving biological levels\nof dexterity in robotic manipulation.","published":"2025-10-31T17:34:04Z","source":"arxiv","url":"http://arxiv.org/abs/2510.27666v1","analysis":{"introduction":"ðŸš€ What if a robotic gripper could reconfigure its whole body like an octopus to pick tiny screws and big bottles with the same hand?  This paper introduces a modular soft gripper that performs whole-body proprioceptive morphing â€” distributed self-sensing pneumatic modules that reconfigure topology for cross-scale, robust grasping.","challenges":"ðŸŽ¯ Key problems tackled: - Fixed global morphology: conventional soft grippers canâ€™t change overall shape. - Localized morphing only: prior work limits deformation to small regions, losing whole-body adaptability. - Poor cross-scale generalization: hard to handle diverse object sizes and irregular geometries.","innovations":"âœ¨ Core innovations: - Modular network of self-sensing pneumatic actuators. - Whole-body proprioceptive morphing: the gripper reconfigures its entire topology to form controllable polygonal shapes. - Integrated distributed sensing enables seamless transitions from precise pinch to large envelope grasps.  Novelty: combining distributed actuation + embedded proprioception to morph the entire gripper rather than only local parts.","experiments":"ðŸ“Š Experimental proof: - Demonstrated expanded grasping envelope and generalization across diverse geometries and scales up to 10Ã—. - Showed new modalities including multi-object grasping and internal hook grasping.  This proves whole-body morphing with proprioception can broaden what a single soft gripper can reliably pick and manipulate.","insights":"ðŸ¤” Whatâ€™s next? - Research directions: integrate learned closed-loop control (vision + proprioception) for autonomous reconfiguration; explore dynamic/adaptive morphing policies via reinforcement learning. - Applications: versatile warehouse picking, cluttered multi-object manipulation, delicate biomedical or underwater manipulation. Could whole-body morphing become a standard module for generalist robot hands?","keywords":["soft gripper","proprioceptive morphing","modular actuators","pneumatic","whole-body sensing","cross-scale grasping","octopus-inspired","envelope grasp"],"category":"robotics","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ What if a robotic gripper could reconfigure its whole body like an octopus to pick tiny screws and big bottles with the same hand?  This paper introduces a modular soft gripper that performs whole-body proprioceptive morphing â€” distributed self-sensing pneumatic modules that reconfigure topology for cross-scale, robust grasping.\n\n**Challenges:** ðŸŽ¯ Key problems tackled: - Fixed global morphology: conventional so...","analyzed_at":"2025-11-04T15:03:03.249Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-11-04T14:59:58.221Z","views":3,"archive_metadata":{"archived_at":"2025-11-04T15:06:00.715Z","original_id":"arxiv_2510.27666v1","views_at_archive":3}},{"id":"arxiv_2510.27680v1","title":"PETAR: Localized Findings Generation with Mask-Aware Vision-Language\n  Modeling for PET Automated Reporting","authors":["Danyal Maqbool","Changhee Lee","Zachary Huemann","Samuel D. Church","Matthew E. Larson","Scott B. Perlman","Tomas A. Romero","Joshua D. Warner","Meghan Lubner","Xin Tie","Jameson Merkow","Junjie Hu","Steve Y. Cho","Tyler J. Bradshaw"],"abstract":"Recent advances in vision-language models (VLMs) have enabled impressive\nmultimodal reasoning, yet most medical applications remain limited to 2D\nimaging. In this work, we extend VLMs to 3D positron emission tomography and\ncomputed tomography (PET/CT), a domain characterized by large volumetric data,\nsmall and dispersed lesions, and lengthy radiology reports. We introduce a\nlarge-scale dataset comprising over 11,000 lesion-level descriptions paired\nwith 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid\nrule-based and large language model (LLM) pipeline. Building upon this dataset,\nwe propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET,\nCT, and lesion contours for spatially grounded report generation. PETAR bridges\nglobal contextual reasoning with fine-grained lesion awareness, producing\nclinically coherent and localized findings. Comprehensive automated and human\nevaluations demonstrate that PETAR substantially improves PET/CT report\ngeneration quality, advancing 3D medical vision-language understanding.","published":"2025-10-31T17:49:01Z","source":"arxiv","url":"http://arxiv.org/abs/2510.27680v1","analysis":{"introduction":"ðŸš€ Ever wished PET/CT reports could be auto-generated from full 3D scans with lesion-level detail?  PETAR presents PETAR-4B: a 3D, mask-aware vision-language model that generates spatially grounded PET/CT findings.  Why it matters: speeds and improves radiology reporting for oncology and nuclear medicine.","challenges":"ðŸŽ¯ Key problems addressed: - Most VLMs focus on 2D images, not large 3D PET/CT volumes. - Lesions are small, dispersed, and need precise spatial grounding. - Radiology reports are long and require lesion-level, clinically coherent language.","innovations":"âœ¨ Core contributions: - Built a large dataset: >11,000 lesion-level descriptions paired with 3D segmentations from >5,000 PET/CT exams via a hybrid rule-based + LLM pipeline. - PETAR-4B: a 3D mask-aware vision-language model that ingests PET, CT, and lesion contours. - Novelty: extends VLMs to volumetric PET/CT and explicitly uses lesion masks for spatially grounded report generation, bridging global context with fine-grained lesion awareness.","experiments":"ðŸ“Š Results: Not specified in the paper.  What the paper reports: comprehensive automated and human evaluations showing PETAR \"substantially improves PET/CT report generation quality,\" demonstrating improved clinical coherence and localized findings over prior approaches.","insights":"ðŸ¤” What's next (inspired directions & applications): - Research: pretraining large 3D multimodal VLMs across PET/CT/MRI and exploring lesion-level outcome prediction and uncertainty quantification. - Applications: interactive radiology assistants, triage tools for oncology imaging, and cross-center adaptation for clinical deployment.  Could mask-aware 3D VLMs become routine in clinical reporting?","keywords":["PETAR-4B","PET/CT","3D vision-language","mask-aware","lesion segmentation","radiology report generation","dataset","VLM","LLM pipeline"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** ðŸš€ Ever wished PET/CT reports could be auto-generated from full 3D scans with lesion-level detail?  PETAR presents PETAR-4B: a 3D, mask-aware vision-language model that generates spatially grounded PET/CT findings.  Why it matters: speeds and improves radiology reporting for oncology and nuclear medicine.\n\n**Challenges:** ðŸŽ¯ Key problems addressed: - Most VLMs focus on 2D images, not large 3D PET/CT volumes. - Lesi...","analyzed_at":"2025-11-04T15:01:56.685Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-11-04T14:59:58.221Z","views":0,"archive_metadata":{"archived_at":"2025-11-04T15:06:00.715Z","original_id":"arxiv_2510.27680v1","views_at_archive":0}}],"metadata":{"total_papers":10,"categories":{"machine_learning":6,"computer_vision":2,"natural_language_processing":1,"robotics":1},"sources":{"huggingface":5,"arxiv":5},"average_score":8.8,"unique_keywords":["Test-Time Scaling","TTS","Agent-REINFORCE","probabilistic graph","multi-LLM","inference optimization","compute-optimal","REINFORCE","LLM-agent","model collaboration","video retrieval","zero-shot generalization","multimodal synthesis","benchmarking","curriculum learning","video embedding","UVRB","Modality Pyramid","GVE","image_editing_benchmark","UniREditBench","multimodal_evaluation","dual-reference","UniREdit-Data-100K","chain-of-thought","Bagel","UniREdit-Bagel","game-world_scenarios","multi-object_reasoning","ROVER","reciprocal cross-modal reasoning","omnimodal generation","multimodal benchmark","visual generation","text generation","interleaved models","ToolScope","agentic framework","MLLM","Perceive tool","Global Navigator","Agentic Executor","Response Synthesizer","VQA","multimodal","long-horizon reasoning","Vision Transformer","Occluded ReID","Shuffle module","Data augmentation"],"total_views":14,"created_at":"2025-11-04T15:06:00.715Z","source":"daily_update","auto_archived":true,"papers_archived":10,"total_papers_analyzed":25}}