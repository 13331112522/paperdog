{"date":"2025-10-05","papers":[{"id":"hf_rlp__reinforcement_as_a_pretraining_objective_1759651505970","title":"RLP: Reinforcement as a Pretraining Objective","authors":[],"abstract":"Let‚Äôs discuss: Why Reinforcement Pretraining (RLP) for reasoning\\nMotivation.Today‚Äôs LLMs learn almost everything with next‚Äëtoken prediction, then only after pretraining try to teach themselves to reason via SFT and RLHF/RLVR. That split means the base model never practices ‚Äúthinking before predicting‚Äù while it‚Äôs learning from raw text. We asked: what if exploration happened during pretraining itself, and we rewarded thoughts that genuinely help predict the next token‚Äîwithout any task‚Äëspecific verifiers?\\nWhat we built.RLP is a pretraining objective that treats a short chain‚Äëof‚Äëthought (CoT) as an action taken before predicting each token.\\n\\nFor each position, the model samples a brief thought, then scores the true next token twice:\\n\\nwith the thought, and 2) with a ‚Äúno‚Äëthink‚Äù baseline (a slowly updated EMA teacher).\\n\\n\\nThe reward is the increase in the model‚Äôs log‚Äëlikelihood of the observed next token with the thought compared to without it.\\n\\nWe update only the thought tokens using a clipped, group‚Äërelative advantage objective; we do not backprop through the reward scores.\\n\\nThe signal is dense (every position gets a reward), verifier‚Äëfree (works on ordinary text), and scales to full documents (no entropy filtering or curated checkers).\\n\\n\\nWhy earlier approaches (e.g., RPT) fell short.Prior ‚Äúreinforcement pretraining‚Äù with prefix‚Äëmatching rewards (RPT) usually relies on:\\n\\nSparse, binary rewards tied to next‚Äëtoken correctness that ignore the content of the thought,\\nAuxiliary entropy filters to pick a subset of tokens to train on, and\\nExperiments on distilled checkpoints, leaving open whether it helps base models.RLP instead delivers a continuous, per‚Äëtoken improvement signal, trains on all tokens in full documents, needs no side model or heuristics, and is explicitly designed to shape base‚Äëmodel thinking early.\\n\\nKey results.\\n\\nQwen3‚Äë1.7B, base pretraining:\\n\\nOverall average across our math‚Äëand‚Äëscience suite improves by about 19% vs. the base model (36.03 vs. 30.32) and about 17% vs. continuous pretraining on the same tokens (36.03 vs. 30.85).\\nUnder compute matching, we gave the CPT baseline 35√ó more tokens to equalize FLOPs; RLP still leads on the same setup by +5.32 points overall on the NC corpus (43.36 vs. 38.04).\\nVersus RPT with matched data/compute, RLP improves Overall Avg by +1.66 points (about +4% relative) and also leads on Math and Science aggregates.\\n\\n\\nAfter identical post‚Äëtraining (SFT + RLVR) for all models:\\n\\nGains compound: RLP+Post 42.51 vs. Base+Post 39.34 (about +8% relative) and vs. CPT+Post 39.90 (about +6.5% relative).\\nThe biggest lifts are on reasoning‚Äëheavy tasks such as AIME25 and MMLU‚ÄëPro.\\n\\n\\nScaling to a 12B hybrid (NeMo‚Äë12B):\\n\\nApplying only 250M RLP tokens to an intermediate checkpoint boosts the overall average from 42.81 to 61.32 (+18.51 points; about +43% relative).\\nScience Avg rises from 34.51 to 57.26 (about +23 points), showing strong cross‚Äëdomain transfer, not just math‚Äëspecific gains.\\n\\n\\nDomain breadth and data practicality:\\n\\nRLP works on SFT‚Äëstyle reasoning corpora and general pretraining sources (academic papers, textbooks, web QA).\\nImprovements persist even when the baseline sees vastly more tokens to match FLOPs, indicating the benefits come from the objective rather than from a larger data budget.\\n\\n\\n\\nIn short, RLP rewards ‚Äúuseful thoughts‚Äù during pretraining. The signal is simple, dense, verifier‚Äëfree, and compatible with standard pipelines‚Äîyielding stronger base models whose gains survive and compound after alignment.\\nPaper: https://arxiv.org/pdf/2510.01265Code: https://github.com/NVlabs/RLP\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T02:25:02.436Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;64414b62603214724ebd2636&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg&quot;,&quot;fullname&quot;:&quot;Ali&quot;,&quot;name&quot;:&quot;ahatamiz&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:4}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.8267005681991577},&quot;editors&quot;:[&quot;ahatamiz&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;‚ù§Ô∏è&quot;,&quot;users&quot;:[&quot;SieraL&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68e07972462eb02f83342642&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267},&quot;createdAt&quot;:&quot;2025-10-04T01:33:38.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;This is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper. \\n\\nThe following papers were recommended by the Semantic Scholar API \\n\\n* [Reinforcement Mid-Training](https://huggingface.co/papers/2509.24375) (2025)\\n* [Reinforcement Learning on Pre-Training Data](https://huggingface.co/papers/2509.19249) (2025)\\n* [VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models](https://huggingface.co/papers/2509.19803) (2025)\\n* [Learning to Reason as Action Abstractions with Scalable Mid-Training RL](https://huggingface.co/papers/2509.25810) (2025)\\n* [Proximal Supervised Fine-Tuning](https://huggingface.co/papers/2508.17784) (2025)\\n* [One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient](https://huggingface.co/papers/2509.26313) (2025)\\n* [On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification](https://huggingface.co/papers/2508.05629) (2025)\\n\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n\\n If you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\\n\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`&quot;,&quot;html&quot;:&quot;This is an automated message from the Librarian Bot. I found the following papers similar to this paper. \\nThe following papers were recommended by the Semantic Scholar API \\n\\nReinforcement Mid-Training (2025)\\nReinforcement Learning on Pre-Training Data (2025)\\nVCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models (2025)\\nLearning to Reason as Action Abstractions with Scalable Mid-Training RL (2025)\\nProximal Supervised Fine-Tuning (2025)\\nOne-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy Gradient (2025)\\nOn the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification (2025)\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n If you want recommendations for any Paper on Hugging Face checkout this Space\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: \\n\\n@librarian-bot\\n\\t recommend\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T01:33:38.340Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7649551630020142},&quot;editors&quot;:[&quot;librarian-bot&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2510.01265&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03ba8&quot;,&quot;name&quot;:&quot;Ali Hatamizadeh&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03ba9&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6338dd1776421c0543150467&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4539dcec644e40be33f4a0d419fa66cb.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Syeda Nahida Akter&quot;,&quot;user&quot;:&quot;SieraL&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Syeda Nahida Akter&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:33:13.536Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03baa&quot;,&quot;name&quot;:&quot;Shrimai Prabhumoye&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03bab&quot;,&quot;name&quot;:&quot;Jan Kautz&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03bac&quot;,&quot;name&quot;:&quot;Mostofa Patwary&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03bad&quot;,&quot;name&quot;:&quot;Mohammad Shoeybi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03bae&quot;,&quot;name&quot;:&quot;Bryan Catanzaro&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df26d8df49fb0df1e03baf&quot;,&quot;name&quot;:&quot;Yejin Choi&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-26T17:53:54.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-03T00:55:02.431Z&quot;,&quot;title&quot;:&quot;RLP: Reinforcement as a Pretraining Objective&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64414b62603214724ebd2636&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ali&quot;,&quot;user&quot;:&quot;ahatamiz&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;The dominant paradigm for training large reasoning models starts with\\npre-training using next-token prediction loss on vast amounts of data.\\nReinforcement learning, while powerful in scaling reasoning, is introduced only\\nas the very last phase of post-training, preceded by supervised fine-tuning.\\nWhile dominant, is this an optimal way of training? In this paper, we present\\nRLP, an information-driven reinforcement pretraining objective, that brings the\\ncore spirit of reinforcement learning -- exploration -- to the last phase of\\npretraining. The key idea is to treat chain-of-thought as an exploratory\\naction, with rewards computed based on the information gain it provides for\\npredicting future tokens. This training objective essentially encourages the\\nmodel to think for itself before predicting what comes next, thus teaching an\\nindependent thinking behavior earlier in the pretraining. More concretely, the\\nreward signal measures the increase in log-likelihood of the next token when\\nconditioning on both context and a sampled reasoning chain, compared to\\nconditioning on context alone. This approach yields a verifier-free dense\\nreward signal, allowing for efficient training for the full document stream\\nduring pretraining. Specifically, RLP reframes reinforcement learning for\\nreasoning as a pretraining objective on ordinary text, bridging the gap between\\nnext-token prediction and the emergence of useful chain-of-thought reasoning.\\nPretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an\\neight-benchmark math-and-science suite by 19%. With identical post-training,\\nthe gains compound, with the largest improvements on reasoning-heavy tasks such\\nas AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2\\nincreases the overall average from 42.81% to 61.32% and raises the average on\\nscientific reasoning by 23%, demonstrating scalability across architectures and\\nmodel sizes.&quot;,&quot;upvotes&quot;:26,&quot;discussionId&quot;:&quot;68df26d8df49fb0df1e03bb0&quot;,&quot;projectPage&quot;:&quot;https://t.co/6PNJYfiAoJ&quot;,&quot;githubRepo&quot;:&quot;https://github.com/NVlabs/RLP&quot;,&quot;ai_summary&quot;:&quot;RLP, an information-driven reinforcement pretraining objective, enhances reasoning models by integrating exploration into pretraining, leading to significant performance improvements across various benchmarks.&quot;,&quot;ai_keywords&quot;:[&quot;reinforcement learning&quot;,&quot;pre-training&quot;,&quot;next-token prediction&quot;,&quot;chain-of-thought&quot;,&quot;information gain&quot;,&quot;log-likelihood&quot;,&quot;dense reward signal&quot;,&quot;Qwen3-1.7B-Base&quot;,&quot;AIME25&quot;,&quot;MMLU-Pro&quot;,&quot;Nemotron-Nano-12B-v2&quot;,&quot;scientific reasoning&quot;],&quot;githubStars&quot;:81,&quot;organization&quot;:{&quot;_id&quot;:&quot;60262b67268c201cdc8b7d43&quot;,&quot;name&quot;:&quot;nvidia&quot;,&quot;fullname&quot;:&quot;NVIDIA&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png&quot;}},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;64414b62603214724ebd2636&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64414b62603214724ebd2636/x9JVcJRZKZE7hdEII1JRR.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ali&quot;,&quot;user&quot;:&quot;ahatamiz&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6338dd1776421c0543150467&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4539dcec644e40be33f4a0d419fa66cb.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Syeda Nahida Akter&quot;,&quot;user&quot;:&quot;SieraL&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;5df85abada6d0311fd3d5408&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Li Dong&quot;,&quot;user&quot;:&quot;unilm&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62ea79dd01ed9b0e8f61ccd3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chengsong Huang&quot;,&quot;user&quot;:&quot;ChengsongHuang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;684d57f26e04c265777ead3f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/cuOj-bQqukSZreXgUJlfm.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Joakim Lee&quot;,&quot;user&quot;:&quot;Reinforcement4All&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66980b9c9baa4382e1678809&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1a516bb7aa7871834c19de708cdd853a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shrimai Prabhumoye&quot;,&quot;user&quot;:&quot;shrimai19&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;681b86baef3886afeea020ea&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fqxZlJxIh803nWvndGOfW.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Anwar&quot;,&quot;user&quot;:&quot;abdoali5672&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64d98ef7a4839890b25eb78b&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Fangyuan Yu&quot;,&quot;user&quot;:&quot;Ksgk-fy&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65bb837dbfb878f46c77de4c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/65bb837dbfb878f46c77de4c/PKyQ_-wTNH1Hyv5HxhWdX.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Prithiv Sakthi&quot;,&quot;user&quot;:&quot;prithivMLmods&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;620783f24e28382272337ba4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/620783f24e28382272337ba4/zkUveQPNiDfYjgGhuFErj.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;GuoLiangTang&quot;,&quot;user&quot;:&quot;Tommy930&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6612aedf09f16e7347dfa7e1&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6612aedf09f16e7347dfa7e1/bPYjBXCedY_1fSIPjoBTY.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Nishith Jain&quot;,&quot;user&quot;:&quot;KingNish&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;organization&quot;:{&quot;_id&quot;:&quot;60262b67268c201cdc8b7d43&quot;,&quot;name&quot;:&quot;nvidia&quot;,&quot;fullname&quot;:&quot;NVIDIA&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png&quot;}}\"> Papers arxiv:2510.01265","published":"2025-10-05","source":"huggingface","url":"https://huggingface.co/papers/2510.01265","analysis":{"introduction":"üöÄ What if LLMs learned to ‚Äúthink before predicting‚Äù during pretraining? RLP (Reinforcement as a Pretraining Objective) treats short chain-of-thoughts as actions and rewards thoughts that improve next-token likelihood ‚Äî bringing exploration into core pretraining for stronger reasoning base models.","challenges":"üéØ Key problems addressed: - Base models only learn next-token prediction; reasoning is deferred to post-training (SFT/RL), so they never practice internal thinking early. - Prior RL-pretraining (RPT) used sparse binary rewards and heavy heuristics. - Many approaches rely on distilled checkpoints or curated verifiers.","innovations":"‚ú® Core innovations: - Treat a brief chain-of-thought (CoT) sampled at each token position as an action. - Reward = increase in log-likelihood of the true next token vs a ‚Äúno-think‚Äù EMA teacher (information gain). - Update only the thought tokens with a clipped, group-relative advantage objective; do not backprop through rewards. - Dense, verifier-free per-token signal that scales to full documents.","experiments":"üìä Most compelling result: Applying RLP to a 12B hybrid (NeMo-12B) for 250M RLP tokens raised overall avg from 42.81 to 61.32 (+18.51 points, ~+43% relative), showing large gains in reasoning performance and cross-domain transfer (Science avg 34.51 ‚Üí 57.26).","insights":"ü§î What's next (potential directions & apps): - Investigate RLP over longer pretraining schedules or with multimodal inputs to seed cross-modal reasoning. - Study how RLP affects calibration, hallucination, and reliability of CoT outputs. Potential apps: stronger base models for scientific QA, math/problem solving, and complex decision-support. Could RLP reduce reliance on heavy post-training RL?","keywords":["reinforcement learning","pretraining","next-token prediction","chain-of-thought","information gain","log-likelihood","dense reward","Qwen3-1.7B","NeMo-12B","reasoning"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ What if LLMs learned to ‚Äúthink before predicting‚Äù during pretraining? RLP (Reinforcement as a Pretraining Objective) treats short chain-of-thoughts as actions and rewards thoughts that improve next-token likelihood ‚Äî bringing exploration into core pretraining for stronger reasoning base models.\n\n**Challenges:** üéØ Key problems addressed: - Base models only learn next-token prediction; reasoning is deferred to po...","analyzed_at":"2025-10-05T08:08:12.361Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-05T08:05:05.970Z","views":0,"archive_metadata":{"archived_at":"2025-10-05T08:08:43.268Z","original_id":"hf_rlp__reinforcement_as_a_pretraining_objective_1759651505970","views_at_archive":0}},{"id":"hf_clue__non_parametric_verification_from_experience_via_hidden_state_clustering_1759651508961","title":"CLUE: Non-parametric Verification from Experience via Hidden-State Clustering","authors":[],"abstract":"This is an automated message from the Librarian Bot. I found the following papers similar to this paper. \\nThe following papers were recommended by the Semantic Scholar API \\n\\nDeep Think with Confidence (2025)\\nThe LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations (2025)\\nLatent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning (2025)\\nCan LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models (2025)\\nLearning to Refine: Self-Refinement of Parallel Reasoning in LLMs (2025)\\nEvolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation (2025)\\nBridging the Knowledge-Prediction Gap in LLMs on Multiple-Choice Questions (2025)\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n If you want recommendations for any Paper on Hugging Face checkout this Space\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: \\n\\n@librarian-bot\\n\\t recommend\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T01:34:29.173Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7361921668052673},&quot;editors&quot;:[&quot;librarian-bot&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2510.01591&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bb7&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;62ffa3f8311cad266f9af236&quot;,&quot;avatarUrl&quot;:&quot;/avatars/203dac40bc546ee25a01d8715a4b3049.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhenwen Liang&quot;,&quot;user&quot;:&quot;invokerliang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhenwen Liang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:33:10.962Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bb8&quot;,&quot;name&quot;:&quot;Ruosen Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bb9&quot;,&quot;name&quot;:&quot;Yujun Zhou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bba&quot;,&quot;name&quot;:&quot;Linfeng Song&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bbb&quot;,&quot;name&quot;:&quot;Dian Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bbc&quot;,&quot;name&quot;:&quot;Xinya Du&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bbd&quot;,&quot;name&quot;:&quot;Haitao Mi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df27e9df49fb0df1e03bbe&quot;,&quot;name&quot;:&quot;Dong Yu&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-10-02T02:14:33.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-03T00:04:05.892Z&quot;,&quot;title&quot;:&quot;CLUE: Non-parametric Verification from Experience via Hidden-State\\n Clustering&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;62ffa3f8311cad266f9af236&quot;,&quot;avatarUrl&quot;:&quot;/avatars/203dac40bc546ee25a01d8715a4b3049.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhenwen Liang&quot;,&quot;user&quot;:&quot;invokerliang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Assessing the quality of Large Language Model (LLM) outputs presents a\\ncritical challenge. Previous methods either rely on text-level information\\n(e.g., reward models, majority voting), which can overfit to superficial cues,\\nor on calibrated confidence from token probabilities, which would fail on\\nless-calibrated models. Yet both of these signals are, in fact, partial\\nprojections of a richer source of information: the model's internal hidden\\nstates. Early layers, closer to token embeddings, preserve semantic and lexical\\nfeatures that underpin text-based judgments, while later layers increasingly\\nalign with output logits, embedding confidence-related information. This paper\\nexplores hidden states directly as a unified foundation for verification. We\\nshow that the correctness of a solution is encoded as a geometrically separable\\nsignature within the trajectory of hidden activations. To validate this, we\\npresent Clue (Clustering and Experience-based Verification), a deliberately\\nminimalist, non-parametric verifier. With no trainable parameters, CLUE only\\nsummarizes each reasoning trace by an hidden state delta and classifies\\ncorrectness via nearest-centroid distance to ``success'' and ``failure''\\nclusters formed from past experience. The simplicity of this method highlights\\nthe strength of the underlying signal. Empirically, CLUE consistently\\noutperforms LLM-as-a-judge baselines and matches or exceeds modern\\nconfidence-based methods in reranking candidates, improving both top-1 and\\nmajority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24\\nwith a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0%\\n(top-maj@16).&quot;,&quot;upvotes&quot;:20,&quot;discussionId&quot;:&quot;68df27e9df49fb0df1e03bbf&quot;,&quot;ai_summary&quot;:&quot;Hidden states in Large Language Models encode correctness as a separable signature, enabling a minimalist verifier (CLUE) to outperform text-level and confidence-based methods in reranking and accuracy.&quot;,&quot;ai_keywords&quot;:[&quot;Large Language Model&quot;,&quot;hidden states&quot;,&quot;token embeddings&quot;,&quot;semantic features&quot;,&quot;lexical features&quot;,&quot;output logits&quot;,&quot;confidence-related information&quot;,&quot;Clue&quot;,&quot;Clustering and Experience-based Verification&quot;,&quot;nearest-centroid distance&quot;,&quot;AIME&quot;,&quot;GPQA&quot;],&quot;organization&quot;:{&quot;_id&quot;:&quot;66543b6e420092799d2f625c&quot;,&quot;name&quot;:&quot;tencent&quot;,&quot;fullname&quot;:&quot;Tencent&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png&quot;}},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;62ffa3f8311cad266f9af236&quot;,&quot;avatarUrl&quot;:&quot;/avatars/203dac40bc546ee25a01d8715a4b3049.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhenwen Liang&quot;,&quot;user&quot;:&quot;invokerliang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65a05abf07184d32fa002d41&quot;,&quot;avatarUrl&quot;:&quot;/avatars/3a23e7e568d2024381ed31b56c1c461a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yujun Zhou&quot;,&quot;user&quot;:&quot;yujunzhou&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65037565da2d88e201f63b7a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/d1b6ce17236360e9583b8bb4cb87e506.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Runpeng Dai&quot;,&quot;user&quot;:&quot;Leo-Dai&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6850ab3fb73a72cdc6159f6f&quot;,&quot;avatarUrl&quot;:&quot;/avatars/74c58336655cdfe7d3fb9854d426240d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Haolin Liu&quot;,&quot;user&quot;:&quot;lhl616&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66e4aa8d4926518abbf5cae2&quot;,&quot;avatarUrl&quot;:&quot;/avatars/dcff2521e0292b602f86c76fc4b5bbae.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;XiangqiWang&quot;,&quot;user&quot;:&quot;qisein&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62ea79dd01ed9b0e8f61ccd3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chengsong Huang&quot;,&quot;user&quot;:&quot;ChengsongHuang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6623ea65b642e29cdf90a1b4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e32e90574c1162b2be87ed78604e3e4d.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;TongZheng&quot;,&quot;user&quot;:&quot;TongZheng1999&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64770d4f5ef58684691e4e57&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7afe56ead9b1233b027e15d0e974f6af.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;KehanGuo&quot;,&quot;user&quot;:&quot;kguo2&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;642447e873f7a0d40b30d677&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/642447e873f7a0d40b30d677/hbUnfIhKZCeSSPv9wGAzk.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;LZX&quot;,&quot;user&quot;:&quot;zli12321&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65147a1426fbd558dbd08f1b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/86574ee2d5c22e940be1c4e50be88675.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Haitao Mi&quot;,&quot;user&quot;:&quot;haitaominlp&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62d58fd53bf5e059f7cc3245&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7a4f3ee4a37245f67efd26749d66a706.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Dian Yu&quot;,&quot;user&quot;:&quot;yudian&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;686c18db78fcc7beecc6634c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/686c18db78fcc7beecc6634c/yWGyDby4xhMboII07I9nX.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tencent-IMO&quot;,&quot;user&quot;:&quot;Tencent-IMO&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;organization&quot;:{&quot;_id&quot;:&quot;66543b6e420092799d2f625c&quot;,&quot;name&quot;:&quot;tencent&quot;,&quot;fullname&quot;:&quot;Tencent&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png&quot;}}\"> Papers arxiv:2510.01591","published":"2025-10-05","source":"huggingface","url":"https://huggingface.co/papers/2510.01591","analysis":{"introduction":"üöÄ Question: Can a model's hidden activations tell us if its answer is right?   CLUE answers yes ‚Äî a tiny, non-parametric verifier that reads hidden-state trajectories and checks correctness by clustering past ‚Äúsuccess‚Äù vs ‚Äúfailure‚Äù traces.   Why care? Better verification & reranking for LLM outputs.","challenges":"üéØ Key problems tackled: - Existing text-level methods (reward models, majority voting) can overfit superficial cues. - Token-prob confidence is unreliable for less-calibrated models. - No simple unified signal that captures both semantic and confidence info.","innovations":"‚ú® Core ideas / what's new: - CLUE: a non-parametric verifier using hidden-state \"deltas\" from reasoning traces. - Classifies correctness by nearest-centroid distance to success/failure clusters built from past experience. - No trainable parameters ‚Äî novelty: hidden trajectories form a geometrically separable correctness signature.","experiments":"üìä Most compelling result: - On AIME 24 with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0% (top-maj@16). - Empirically outperforms LLM-as-judge baselines and matches/exceeds confidence-based methods on AIME 24/25 and GPQA.","insights":"ü§î What's next (potential directions & impact): - Explore adapting CLUE across models/tasks or multimodal traces; test continual \"experience banks\" that grow over time. - Combine non-parametric hidden-state signals with lightweight learned verifiers for robustness. Could this make LLM assistants measurably more reliable?","keywords":["CLUE","hidden states","non-parametric verifier","nearest-centroid","hidden-state delta","LLM verification","AIME","GPQA"],"category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Question: Can a model's hidden activations tell us if its answer is right?   CLUE answers yes ‚Äî a tiny, non-parametric verifier that reads hidden-state trajectories and checks correctness by clustering past ‚Äúsuccess‚Äù vs ‚Äúfailure‚Äù traces.   Why care? Better verification & reranking for LLM outputs.\n\n**Challenges:** üéØ Key problems tackled: - Existing text-level methods (reward models, majority voting) can overfit...","analyzed_at":"2025-10-05T08:08:43.158Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-05T08:05:08.961Z","views":0,"archive_metadata":{"archived_at":"2025-10-05T08:08:43.268Z","original_id":"hf_clue__non_parametric_verification_from_experience_via_hidden_state_clustering_1759651508961","views_at_archive":0}},{"id":"hf_modernvbert__towards_smaller_visual_document_retrievers_1759651503050","title":"ModernVBERT: Towards Smaller Visual Document Retrievers","authors":[],"abstract":"Multimodal embedding models are gaining prevalence, notably for document retrieval as efficient alternatives to text-only pipelines. These models are typically built by finetuning large vision-language decoders (VLMs) with contrastive losses on text-image pairs. In this work, we show that, while cost-efficient, this repurposing approach often bottlenecks retrieval performance. Through controlled experiments, we establish a principled recipe for improving visual document retrieval models. We notably measure the impact of attention masking, image resolution, modality alignment data regimes, and late interaction centered contrastive objectives which emerge as central performance factors. Building on these insights, we release ModernVBERT, a compact 250M-parameter vision-language encoder that outperforms models up to 10 times larger when finetuned on document retrieval tasks. Models and code are made available.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T11:20:48.587Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;60f2e021adf471cbdf8bb660&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg&quot;,&quot;fullname&quot;:&quot;Manuel Faysse&quot;,&quot;name&quot;:&quot;manu&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:181}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.8902249336242676},&quot;editors&quot;:[&quot;manu&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;üî•&quot;,&quot;users&quot;:[&quot;fsommers&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68e07a7297c0804cbe6bf182&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267},&quot;createdAt&quot;:&quot;2025-10-04T01:37:54.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;This is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper. \\n\\nThe following papers were recommended by the Semantic Scholar API \\n\\n* [MetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction](https://huggingface.co/papers/2509.18095) (2025)\\n* [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://huggingface.co/papers/2509.19203) (2025)\\n* [Training LLMs to be Better Text Embedders through Bidirectional Reconstruction](https://huggingface.co/papers/2509.03020) (2025)\\n* [CMRAG: Co-modality-based visual document retrieval and question answering](https://huggingface.co/papers/2509.02123) (2025)\\n* [SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models](https://huggingface.co/papers/2509.15432) (2025)\\n* [Zero-shot Multimodal Document Retrieval via Cross-modal Question Generation](https://huggingface.co/papers/2508.17079) (2025)\\n* [DocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning](https://huggingface.co/papers/2509.23883) (2025)\\n\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n\\n If you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\\n\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`&quot;,&quot;html&quot;:&quot;This is an automated message from the Librarian Bot. I found the following papers similar to this paper. \\nThe following papers were recommended by the Semantic Scholar API \\n\\nMetaEmbed: Scaling Multimodal Retrieval at Test-Time with Flexible Late Interaction (2025)\\nVision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions (2025)\\nTraining LLMs to be Better Text Embedders through Bidirectional Reconstruction (2025)\\nCMRAG: Co-modality-based visual document retrieval and question answering (2025)\\nSERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models (2025)\\nZero-shot Multimodal Document Retrieval via Cross-modal Question Generation (2025)\\nDocPruner: A Storage-Efficient Framework for Multi-Vector Visual Document Retrieval via Adaptive Patch-Level Embedding Pruning (2025)\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n If you want recommendations for any Paper on Hugging Face checkout this Space\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: \\n\\n@librarian-bot\\n\\t recommend\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T01:37:54.955Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.6963590383529663},&quot;editors&quot;:[&quot;librarian-bot&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2510.01149&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68de63af70ada21878c75026&quot;,&quot;name&quot;:&quot;Paul Teiletche&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68de63af70ada21878c75027&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;661e945eebe3616a1b09e279&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/661e945eebe3616a1b09e279/U3DL1BNouUpcusCKAPZm0.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Quentin Mac√©&quot;,&quot;user&quot;:&quot;QuentinJG&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Quentin Mac√©&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:52:04.668Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68de63af70ada21878c75028&quot;,&quot;name&quot;:&quot;Max Conti&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68de63af70ada21878c75029&quot;,&quot;name&quot;:&quot;Antonio Loison&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68de63af70ada21878c7502a&quot;,&quot;name&quot;:&quot;Gautier Viaud&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68de63af70ada21878c7502b&quot;,&quot;name&quot;:&quot;Pierre Colombo&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68de63af70ada21878c7502c&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;60f2e021adf471cbdf8bb660&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Manuel Faysse&quot;,&quot;user&quot;:&quot;manu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Manuel Faysse&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:52:47.596Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-10-01T17:41:17.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-03T09:50:48.566Z&quot;,&quot;title&quot;:&quot;ModernVBERT: Towards Smaller Visual Document Retrievers&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60f2e021adf471cbdf8bb660&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Manuel Faysse&quot;,&quot;user&quot;:&quot;manu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Multimodal embedding models are gaining prevalence, notably for document\\nretrieval as efficient alternatives to text-only pipelines. These models are\\ntypically built by finetuning large vision-language decoders (VLMs) with\\ncontrastive losses on text-image pairs. In this work, we show that, while\\ncost-efficient, this repurposing approach often bottlenecks retrieval\\nperformance. Through controlled experiments, we establish a principled recipe\\nfor improving visual document retrieval models. We notably measure the impact\\nof attention masking, image resolution, modality alignment data regimes, and\\nlate interaction centered contrastive objectives which emerge as central\\nperformance factors. Building on these insights, we release ModernVBERT, a\\ncompact 250M-parameter vision-language encoder that outperforms models up to 10\\ntimes larger when finetuned on document retrieval tasks. Models and code are\\nmade available at https://huggingface.co/ModernVBERT.&quot;,&quot;upvotes&quot;:26,&quot;discussionId&quot;:&quot;68de63b070ada21878c7502d&quot;,&quot;projectPage&quot;:&quot;https://huggingface.co/ModernVBERT&quot;,&quot;ai_summary&quot;:&quot;ModernVBERT, a compact vision-language encoder, outperforms larger models in document retrieval by optimizing attention masking, image resolution, modality alignment, and contrastive objectives.&quot;,&quot;ai_keywords&quot;:[&quot;multimodal embedding models&quot;,&quot;document retrieval&quot;,&quot;vision-language decoders&quot;,&quot;contrastive losses&quot;,&quot;attention masking&quot;,&quot;image resolution&quot;,&quot;modality alignment&quot;,&quot;contrastive objectives&quot;,&quot;vision-language encoder&quot;],&quot;organization&quot;:{&quot;_id&quot;:&quot;68dc126476aff34f469efbc4&quot;,&quot;name&quot;:&quot;ModernVBERT&quot;,&quot;fullname&quot;:&quot;ModernVBERT&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/6651baf4b34bbdaec88333e7/qxlWj1d9iagGW6T8yCkJV.png&quot;}},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;60f2e021adf471cbdf8bb660&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Manuel Faysse&quot;,&quot;user&quot;:&quot;manu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;661e945eebe3616a1b09e279&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/661e945eebe3616a1b09e279/U3DL1BNouUpcusCKAPZm0.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Quentin Mac√©&quot;,&quot;user&quot;:&quot;QuentinJG&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6720a87e392e9cea0187fde6&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6720a87e392e9cea0187fde6/vW8DW31UvdKD809UyYCS4.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Max Conti&quot;,&quot;user&quot;:&quot;mlconti&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;618a65e17304dc918c6602ff&quot;,&quot;avatarUrl&quot;:&quot;/avatars/8af1112094169da80c65e24ab71c7e59.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Gautier Viaud&quot;,&quot;user&quot;:&quot;gautierviaud&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64be3b2b805e5b64572eec44&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VDCwEAgQ2G0fGCvB5L8cA.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Alexander Micklewright&quot;,&quot;user&quot;:&quot;alexmick&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66e16a677c2eb2da5109fb5c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66e16a677c2eb2da5109fb5c/wN2ZQ4iRQ5zmdC0WTQsnD.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Antoine EDY&quot;,&quot;user&quot;:&quot;antoineedy-illuin&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67a8dc9e560939c755f39fb5&quot;,&quot;avatarUrl&quot;:&quot;/avatars/9451f17dcd2894ede2c95dcfdfa43582.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Victor Xing&quot;,&quot;user&quot;:&quot;vxing&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67ab5868b028527841ce09f7&quot;,&quot;avatarUrl&quot;:&quot;/avatars/5e8b7a341a807bd3e08c2cb98ca20dd3.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Iker TARDIO&quot;,&quot;user&quot;:&quot;tardioik&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b708f9ae6ee066e29045c3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/824a8973e1013a615538df58ef9dfa17.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Marine Neyret&quot;,&quot;user&quot;:&quot;mneyret&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64a2e1ab812528832070a4b4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c2fef6d21cfd4b66085127e7f3223025.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Victor Alibert&quot;,&quot;user&quot;:&quot;victoralibert&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63b6cb6b029518c6bbfda056&quot;,&quot;avatarUrl&quot;:&quot;/avatars/28849f397a60eb33c359cf536f109485.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tom Brendl√©&quot;,&quot;user&quot;:&quot;tombrendle&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62b5db0a73ab76290041245a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/6c94a42e8f647578e7d31fcdfcbd591e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Paul des Garets&quot;,&quot;user&quot;:&quot;pdesgarets-illuin&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;organization&quot;:{&quot;_id&quot;:&quot;68dc126476aff34f469efbc4&quot;,&quot;name&quot;:&quot;ModernVBERT&quot;,&quot;fullname&quot;:&quot;ModernVBERT&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/6651baf4b34bbdaec88333e7/qxlWj1d9iagGW6T8yCkJV.png&quot;}}\"> Papers arxiv:2510.01149","published":"2025-10-05","source":"huggingface","url":"https://huggingface.co/papers/2510.01149","analysis":{"introduction":"üöÄ Want faster, cheaper visual document search without huge models? ModernVBERT presents a compact 250M-parameter vision-language encoder that outperforms models up to 10√ó larger for document retrieval ‚Äî a win for efficient, real-world retrieval pipelines.","challenges":"üéØ Key problems tackled: - Repurposing large VLM decoders with contrastive finetuning often bottlenecks retrieval performance. - High compute/cost of large multimodal models for retrieval. - Lack of principled study on factors (attention masking, image resolution, alignment) that drive retrieval quality.","innovations":"‚ú® Core contributions: - Controlled ablations measuring attention masking, image resolution, modality-alignment data regimes. - Emphasis on late-interaction‚Äìcentered contrastive objectives tailored for visual document retrieval. - A principled \"recipe\" to optimize those factors. - Release of ModernVBERT: a compact 250M-parameter vision-language encoder that leverages these insights.","experiments":"üìä Main experimental takeaway: ModernVBERT (250M params) outperforms models up to 10√ó larger when finetuned on document retrieval tasks ‚Äî demonstrating compact, targeted encoders can beat bloated VLM repurposing.  Detailed benchmarks/metrics/datasets: Not specified in the paper.","insights":"ü§î What's next (inspired ideas): - Explore combining ModernVBERT with multi-vector/patch-level indexing or late-interaction re-ranking. - Test zero-shot/transfer performance across more document domains and industry-scale corpora. Could compact, optimized encoders replace heavy VLMs in production search stacks?","keywords":["ModernVBERT","visual document retrieval","vision-language encoder","contrastive learning","late interaction","attention masking","image resolution","modality alignment"],"category":"machine_learning","relevance_score":9,"technical_depth":"intermediate","summary":"**Introduction:** üöÄ Want faster, cheaper visual document search without huge models? ModernVBERT presents a compact 250M-parameter vision-language encoder that outperforms models up to 10√ó larger for document retrieval ‚Äî a win for efficient, real-world retrieval pipelines.\n\n**Challenges:** üéØ Key problems tackled: - Repurposing large VLM decoders with contrastive finetuning often bottlenecks retrieval performance. - High compute/co...","analyzed_at":"2025-10-05T08:08:10.670Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-05T08:05:03.050Z","views":0,"archive_metadata":{"archived_at":"2025-10-05T08:08:43.268Z","original_id":"hf_modernvbert__towards_smaller_visual_document_retrievers_1759651503050","views_at_archive":0}},{"id":"hf_exgrpo__learning_to_reason_from_experience_1759651491425","title":"ExGRPO: Learning to Reason from Experience","authors":[],"abstract":"We present a systematic study of what makes reasoning experiences valuable in RLVR and propose a framework that leverages these insights to exploit high-value experiences for efficient RLVR.\\nModel Collection: https://huggingface.co/collections/rzzhan/exgrpo-68d8e302efdfe325187d5c96\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T16:37:02.337Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;62495cb96ee7ee6b646db130&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;,&quot;fullname&quot;:&quot;Runzhe Zhan&quot;,&quot;name&quot;:&quot;rzzhan&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3}},&quot;numEdits&quot;:1,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.8738799691200256},&quot;editors&quot;:[&quot;rzzhan&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68dfdd5bcb032d78f051fabb&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63f3502a520c14618925825a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e986a2a6625e7be6890616a417f908d2.svg&quot;,&quot;fullname&quot;:&quot;Yafu Li&quot;,&quot;name&quot;:&quot;yaful&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:4},&quot;createdAt&quot;:&quot;2025-10-03T14:27:39.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;Github: https://github.com/ElliottYan/LUFFY/tree/main/ExGRPO&quot;,&quot;html&quot;:&quot;Github: https://github.com/ElliottYan/LUFFY/tree/main/ExGRPO\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T14:27:39.338Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63f3502a520c14618925825a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e986a2a6625e7be6890616a417f908d2.svg&quot;,&quot;fullname&quot;:&quot;Yafu Li&quot;,&quot;name&quot;:&quot;yaful&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:4}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.6876582503318787},&quot;editors&quot;:[&quot;yaful&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/e986a2a6625e7be6890616a417f908d2.svg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;üöÄ&quot;,&quot;users&quot;:[&quot;rzzhan&quot;,&quot;yoonkg&quot;],&quot;count&quot;:2}],&quot;isReport&quot;:false},&quot;replies&quot;:[{&quot;id&quot;:&quot;68dffb0bbad50a573804827a&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;62495cb96ee7ee6b646db130&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;,&quot;fullname&quot;:&quot;Runzhe Zhan&quot;,&quot;name&quot;:&quot;rzzhan&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3},&quot;createdAt&quot;:&quot;2025-10-03T16:34:19.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:true,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;Added to the metadata.&quot;,&quot;html&quot;:&quot;Added to the metadata.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T16:35:01.404Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;62495cb96ee7ee6b646db130&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;,&quot;fullname&quot;:&quot;Runzhe Zhan&quot;,&quot;name&quot;:&quot;rzzhan&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3}},&quot;numEdits&quot;:1,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.860673725605011},&quot;editors&quot;:[&quot;rzzhan&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false,&quot;parentCommentId&quot;:&quot;68dfdd5bcb032d78f051fabb&quot;}}]},{&quot;id&quot;:&quot;68e079efc0a1176a4dee8f5d&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267},&quot;createdAt&quot;:&quot;2025-10-04T01:35:43.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;This is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper. \\n\\nThe following papers were recommended by the Semantic Scholar API \\n\\n* [Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse](https://huggingface.co/papers/2509.25808) (2025)\\n* [ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism](https://huggingface.co/papers/2508.11356) (2025)\\n* [More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration](https://huggingface.co/papers/2510.02227) (2025)\\n* [MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources](https://huggingface.co/papers/2509.21268) (2025)\\n* [Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR](https://huggingface.co/papers/2508.14029) (2025)\\n* [CLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning](https://huggingface.co/papers/2509.25004) (2025)\\n* [Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle](https://huggingface.co/papers/2508.05612) (2025)\\n\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n\\n If you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\\n\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`&quot;,&quot;html&quot;:&quot;This is an automated message from the Librarian Bot. I found the following papers similar to this paper. \\nThe following papers were recommended by the Semantic Scholar API \\n\\nImproving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse (2025)\\nETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism (2025)\\nMore Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration (2025)\\nMMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and Open Resources (2025)\\nBeyond Pass@1: Self-Play with Variational Problem Synthesis Sustains RLVR (2025)\\nCLPO: Curriculum Learning meets Policy Optimization for LLM Reasoning (2025)\\nShuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle (2025)\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n If you want recommendations for any Paper on Hugging Face checkout this Space\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: \\n\\n@librarian-bot\\n\\t recommend\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T01:35:43.760Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7339387536048889},&quot;editors&quot;:[&quot;librarian-bot&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2510.02245&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd2&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;62495cb96ee7ee6b646db130&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Runzhe Zhan&quot;,&quot;user&quot;:&quot;rzzhan&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Runzhe Zhan&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-03T12:29:46.232Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd3&quot;,&quot;name&quot;:&quot;Yafu Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd4&quot;,&quot;name&quot;:&quot;Zhi Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd5&quot;,&quot;name&quot;:&quot;Xiaoye Qu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd6&quot;,&quot;name&quot;:&quot;Dongrui Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd7&quot;,&quot;name&quot;:&quot;Jing Shao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd8&quot;,&quot;name&quot;:&quot;Derek F. Wong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68df3b51df49fb0df1e03cd9&quot;,&quot;name&quot;:&quot;Yu Cheng&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-10-02T17:31:30.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-03T01:31:39.660Z&quot;,&quot;title&quot;:&quot;ExGRPO: Learning to Reason from Experience&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;62495cb96ee7ee6b646db130&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Runzhe Zhan&quot;,&quot;user&quot;:&quot;rzzhan&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm\\nfor improving the reasoning ability of large language models. However, standard\\non-policy training discards rollout experiences after a single update, leading\\nto computational inefficiency and instability. While prior work on RL has\\nhighlighted the benefits of reusing past experience, the role of experience\\ncharacteristics in shaping learning dynamics of large reasoning models remains\\nunderexplored. In this paper, we are the first to investigate what makes a\\nreasoning experience valuable and identify rollout correctness and entropy as\\neffective indicators of experience value. Based on these insights, we propose\\nExGRPO (Experiential Group Relative Policy Optimization), a framework that\\norganizes and prioritizes valuable experiences, and employs a mixed-policy\\nobjective to balance exploration with experience exploitation. Experiments on\\nfive backbone models (1.5B-8B parameters) show that ExGRPO consistently\\nimproves reasoning performance on mathematical/general benchmarks, with an\\naverage gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO\\nstabilizes training on both stronger and weaker models where on-policy methods\\nfail. These results highlight principled experience management as a key\\ningredient for efficient and scalable RLVR.&quot;,&quot;upvotes&quot;:59,&quot;discussionId&quot;:&quot;68df3b52df49fb0df1e03cda&quot;,&quot;githubRepo&quot;:&quot;https://github.com/ElliottYan/LUFFY/tree/main/ExGRPO&quot;,&quot;ai_summary&quot;:&quot;ExGRPO, a framework that prioritizes valuable reasoning experiences, improves and stabilizes reinforcement learning from verifiable rewards for large language models.&quot;,&quot;ai_keywords&quot;:[&quot;reinforcement learning from verifiable rewards&quot;,&quot;RLVR&quot;,&quot;on-policy training&quot;,&quot;rollout experiences&quot;,&quot;experience characteristics&quot;,&quot;rollout correctness&quot;,&quot;entropy&quot;,&quot;ExGRPO&quot;,&quot;Experiential Group Relative Policy Optimization&quot;,&quot;mixed-policy objective&quot;,&quot;exploration&quot;,&quot;experience exploitation&quot;,&quot;reasoning performance&quot;,&quot;mathematical benchmarks&quot;,&quot;general benchmarks&quot;],&quot;githubStars&quot;:328},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;62495cb96ee7ee6b646db130&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62495cb96ee7ee6b646db130/UwBXmvcMq7LMvBWUw0xo3.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Runzhe Zhan&quot;,&quot;user&quot;:&quot;rzzhan&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;682fe5e1c8ddcbd4645ec29d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/843a5c6ba36230c02cce4a8bf9d47319.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Mehmet Yƒ±lmaz&quot;,&quot;user&quot;:&quot;mY1lmaz&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;682fe2641af275ff5c4a06e0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f939d34170c74fc873a2abb6e97680ff.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Emily Carter&quot;,&quot;user&quot;:&quot;EmilyC11&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68075d12dadb28dddc14a509&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7a9d86c6fb56ece2ed5bf36350142612.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Finch&quot;,&quot;user&quot;:&quot;KKKepler&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68076f20757bde45c40abdd8&quot;,&quot;avatarUrl&quot;:&quot;/avatars/5eca1d2a379774310e7f7874a5be0013.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;rao&quot;,&quot;user&quot;:&quot;bingqing1&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63f3502a520c14618925825a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e986a2a6625e7be6890616a417f908d2.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yafu Li&quot;,&quot;user&quot;:&quot;yaful&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;680b35fcdf6ff26584543fee&quot;,&quot;avatarUrl&quot;:&quot;/avatars/baf73994882d15e47583fca812ca5f50.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Michael Brooks&quot;,&quot;user&quot;:&quot;MichaelBrooks&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6830504c47fb9b0c7253f0dc&quot;,&quot;avatarUrl&quot;:&quot;/avatars/48a7033fa83c8a2cdcd9c90dea7d5199.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ada Nwosu&quot;,&quot;user&quot;:&quot;AdaNwosu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6808ac6516fed0aa450a26c4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/895088b3f28ec57ccf825ad4275df117.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Dan Schuurmans&quot;,&quot;user&quot;:&quot;Schuurmans&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68075e1a0a34ff7fff3d9c86&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f1a59c5e7548fee66112b0b78a40841e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Clair&quot;,&quot;user&quot;:&quot;DanClair77&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;680771da2df2acd9cea8a282&quot;,&quot;avatarUrl&quot;:&quot;/avatars/cffe5727d84290d9f0589cba111e4776.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;zhu&quot;,&quot;user&quot;:&quot;haoran7&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64264095ba51f8a2136946a0&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhaochen Su&quot;,&quot;user&quot;:&quot;Warrieryes&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:2}\"> Papers arxiv:2510.02245","published":"2025-10-05","source":"huggingface","url":"https://huggingface.co/papers/2510.02245","analysis":{"introduction":"üöÄ Want LLMs that learn reasoning faster and more stably? ExGRPO identifies and exploits \"high-value\" reasoning experiences to make Reinforcement Learning from Verifiable Rewards (RLVR) far more efficient. Big win for researchers training 1.5B‚Äì8B reasoning models. üî•","challenges":"üéØ Problems tackled: - On-policy RLVR discards rollouts after one update ‚Üí computational inefficiency & instability. - Which past experiences are worth reusing is unclear for reasoning models. - Prior replay techniques aren‚Äôt tailored to reasoning-specific signals.","innovations":"‚ú® Key ideas: - Identifies experience value via rollout correctness & entropy. - Organizes/prioritizes experiences (experience groups). - Mixed-policy objective to balance exploration vs. exploiting high-value experiences. Novelty: principled experience management for RLVR.","experiments":"üìä Main result: ExGRPO improves reasoning on math/general benchmarks across 1.5B‚Äì8B backbones, giving an average gain of +3.5 / +7.6 points over on-policy RLVR, and stabilizes training where on-policy methods fail. Concrete proof of efficient, scalable RLVR.","insights":"ü§î What's next? - Apply experience-aware replay to multimodal or retrieval-augmented LLMs to see if gains extend beyond text reasoning. - Combine ExGRPO with curriculum or self-play to synthesize harder problems for continuous improvement. Could this make automated problem-solving systems more reliable?","keywords":["reinforcement learning from verifiable rewards","RLVR","on-policy training","rollout experiences","experience characteristics","rollout correctness","entropy","ExGRPO","Experiential Group Relative Policy Optimization","mixed-policy objective","exploration","experience exploitation","reasoning performance","mathematical benchmarks","general benchmarks"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Want LLMs that learn reasoning faster and more stably? ExGRPO identifies and exploits \"high-value\" reasoning experiences to make Reinforcement Learning from Verifiable Rewards (RLVR) far more efficient. Big win for researchers training 1.5B‚Äì8B reasoning models. üî•\n\n**Challenges:** üéØ Problems tackled: - On-policy RLVR discards rollouts after one update ‚Üí computational inefficiency & instability. - Which past exp...","analyzed_at":"2025-10-05T08:07:43.426Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-05T08:04:51.425Z","views":0,"archive_metadata":{"archived_at":"2025-10-05T08:08:43.268Z","original_id":"hf_exgrpo__learning_to_reason_from_experience_1759651491425","views_at_archive":0}},{"id":"hf_longcodezip__compress_long_context_for_code_language_models_1759651484780","title":"LongCodeZip: Compress Long Context for Code Language Models","authors":[],"abstract":"How to compress long code context? üìö\\nCheck out our LongCodeZip! Paper just got accepted to ASE 2025. üî•\\nCode: https://github.com/YerbaPage/LongCodeZipPaper: https://huggingface.co/papers/2510.00446\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T02:07:13.310Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;645b0c3ec35da9c7afd95421&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg&quot;,&quot;fullname&quot;:&quot;Yuling&quot;,&quot;name&quot;:&quot;YerbaPage&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:255}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.6779364943504333},&quot;editors&quot;:[&quot;YerbaPage&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68df436560e76f4d99b9f50b&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;657f461be50ca9a699f8754d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a2153251b473de201ffd4df72c1b67fc.svg&quot;,&quot;fullname&quot;:&quot;Atharva Srivastava&quot;,&quot;name&quot;:&quot;computerponr&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:6},&quot;createdAt&quot;:&quot;2025-10-03T03:30:45.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;Congratulation boys!!\\n&quot;,&quot;html&quot;:&quot;Congratulation boys!!\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T03:30:45.115Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;657f461be50ca9a699f8754d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a2153251b473de201ffd4df72c1b67fc.svg&quot;,&quot;fullname&quot;:&quot;Atharva Srivastava&quot;,&quot;name&quot;:&quot;computerponr&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:6}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.5558434128761292},&quot;editors&quot;:[&quot;computerponr&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/a2153251b473de201ffd4df72c1b67fc.svg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;üëç&quot;,&quot;users&quot;:[&quot;YerbaPage&quot;],&quot;count&quot;:1},{&quot;reaction&quot;:&quot;‚ù§Ô∏è&quot;,&quot;users&quot;:[&quot;YerbaPage&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68dfd776198a4f889fdcaca6&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;65bb837dbfb878f46c77de4c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/65bb837dbfb878f46c77de4c/PKyQ_-wTNH1Hyv5HxhWdX.jpeg&quot;,&quot;fullname&quot;:&quot;Prithiv Sakthi&quot;,&quot;name&quot;:&quot;prithivMLmods&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3324},&quot;createdAt&quot;:&quot;2025-10-03T14:02:30.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;Ranks function-level chunks conditional perplexions relative to the instruction, proportionating the ATB.\\nThis is awesome work, guys! Congratulations! üî•üëè\\n\\n![3Pf5RWob0LBpyT0yMoM2-](https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/-95dGqfouU1goxqLXaD1R.png)\\n&quot;,&quot;html&quot;:&quot;Ranks function-level chunks conditional perplexions relative to the instruction, proportionating the ATB.This is awesome work, guys! Congratulations! üî•üëè\\n\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T14:02:30.502Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;65bb837dbfb878f46c77de4c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/65bb837dbfb878f46c77de4c/PKyQ_-wTNH1Hyv5HxhWdX.jpeg&quot;,&quot;fullname&quot;:&quot;Prithiv Sakthi&quot;,&quot;name&quot;:&quot;prithivMLmods&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3324}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.6244713664054871},&quot;editors&quot;:[&quot;prithivMLmods&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/65bb837dbfb878f46c77de4c/PKyQ_-wTNH1Hyv5HxhWdX.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ü§ó&quot;,&quot;users&quot;:[&quot;YerbaPage&quot;],&quot;count&quot;:1},{&quot;reaction&quot;:&quot;‚ù§Ô∏è&quot;,&quot;users&quot;:[&quot;YerbaPage&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68dfe47bcb032d78f0530f88&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66d1892971ba7a722e0c0d78&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pxLH2RFr3TNMP0NVSREMU.jpeg&quot;,&quot;fullname&quot;:&quot;Arthur EDMOND&quot;,&quot;name&quot;:&quot;Shumatsurontek&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:2},&quot;createdAt&quot;:&quot;2025-10-03T14:58:03.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;This is promising will try !&quot;,&quot;html&quot;:&quot;This is promising will try !\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-03T14:58:03.194Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66d1892971ba7a722e0c0d78&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pxLH2RFr3TNMP0NVSREMU.jpeg&quot;,&quot;fullname&quot;:&quot;Arthur EDMOND&quot;,&quot;name&quot;:&quot;Shumatsurontek&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:2}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9590368866920471},&quot;editors&quot;:[&quot;Shumatsurontek&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/pxLH2RFr3TNMP0NVSREMU.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;üöÄ&quot;,&quot;users&quot;:[&quot;YerbaPage&quot;],&quot;count&quot;:1},{&quot;reaction&quot;:&quot;üëç&quot;,&quot;users&quot;:[&quot;YerbaPage&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}},{&quot;id&quot;:&quot;68e07a88462eb02f83344a79&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267},&quot;createdAt&quot;:&quot;2025-10-04T01:38:16.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;This is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper. \\n\\nThe following papers were recommended by the Semantic Scholar API \\n\\n* [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://huggingface.co/papers/2509.13723) (2025)\\n* [SCOPE: A Generative Approach for LLM Prompt Compression](https://huggingface.co/papers/2508.15813) (2025)\\n* [Impact-driven Context Filtering For Cross-file Code Completion](https://huggingface.co/papers/2508.05970) (2025)\\n* [UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression](https://huggingface.co/papers/2509.15763) (2025)\\n* [PagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference](https://huggingface.co/papers/2509.04377) (2025)\\n* [SynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion](https://huggingface.co/papers/2508.15495) (2025)\\n* [SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://huggingface.co/papers/2508.06447) (2025)\\n\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n\\n If you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\\n\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: `@librarian-bot recommend`&quot;,&quot;html&quot;:&quot;This is an automated message from the Librarian Bot. I found the following papers similar to this paper. \\nThe following papers were recommended by the Semantic Scholar API \\n\\nDSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning (2025)\\nSCOPE: A Generative Approach for LLM Prompt Compression (2025)\\nImpact-driven Context Filtering For Cross-file Code Completion (2025)\\nUniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression (2025)\\nPagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference (2025)\\nSynthCoder: A Synthetical Strategy to Tune LLMs for Code Completion (2025)\\nSlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning (2025)\\n\\n Please give a thumbs up to this comment if you found it helpful!\\n If you want recommendations for any Paper on Hugging Face checkout this Space\\n You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: \\n\\n@librarian-bot\\n\\t recommend\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-04T01:38:16.670Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;63d3e0e8ff1384ce6c5dd17d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;,&quot;fullname&quot;:&quot;Librarian Bot (Bot)&quot;,&quot;name&quot;:&quot;librarian-bot&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:267}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.6809963583946228},&quot;editors&quot;:[&quot;librarian-bot&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2510.00446&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68ddef306024653e8a3ed0e9&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;645b0c3ec35da9c7afd95421&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuling&quot;,&quot;user&quot;:&quot;YerbaPage&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yuling Shi&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-10-02T13:55:10.013Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ddef306024653e8a3ed0ea&quot;,&quot;name&quot;:&quot;Yichun Qian&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ddef306024653e8a3ed0eb&quot;,&quot;name&quot;:&quot;Hongyu Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ddef306024653e8a3ed0ec&quot;,&quot;name&quot;:&quot;Beijun Shen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68ddef306024653e8a3ed0ed&quot;,&quot;name&quot;:&quot;Xiaodong Gu&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-10-01T02:54:57.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-03T00:37:13.283Z&quot;,&quot;title&quot;:&quot;LongCodeZip: Compress Long Context for Code Language Models&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;645b0c3ec35da9c7afd95421&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuling&quot;,&quot;user&quot;:&quot;YerbaPage&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Code generation under long contexts is becoming increasingly critical as\\nLarge Language Models (LLMs) are required to reason over extensive information\\nin the codebase. While recent advances enable code LLMs to process long inputs,\\nhigh API costs and generation latency remain substantial bottlenecks. Existing\\ncontext pruning techniques, such as LLMLingua, achieve promising results for\\ngeneral text but overlook code-specific structures and dependencies, leading to\\nsuboptimal performance in programming tasks. In this paper, we propose\\nLongCodeZip, a novel plug-and-play code compression framework designed\\nspecifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)\\ncoarse-grained compression, which identifies and ranks function-level chunks\\nusing conditional perplexity with respect to the instruction, retaining only\\nthe most relevant functions; and (2) fine-grained compression, which segments\\nretained functions into blocks based on perplexity and selects an optimal\\nsubset under an adaptive token budget to maximize relevance. Evaluations across\\nmultiple tasks, including code completion, summarization, and question\\nanswering, show that LongCodeZip consistently outperforms baseline methods,\\nachieving up to a 5.6x compression ratio without degrading task performance. By\\neffectively reducing context size while preserving essential information,\\nLongCodeZip enables LLMs to better scale to real-world, large-scale code\\nscenarios, advancing the efficiency and capability of code intelligence\\napplications.&quot;,&quot;upvotes&quot;:81,&quot;discussionId&quot;:&quot;68ddef316024653e8a3ed0ee&quot;,&quot;githubRepo&quot;:&quot;https://github.com/YerbaPage/LongCodeZip&quot;,&quot;ai_summary&quot;:&quot;LongCodeZip is a code compression framework for LLMs that uses dual-stage compression to reduce context size without degrading performance, improving efficiency in code intelligence applications.&quot;,&quot;ai_keywords&quot;:[&quot;Large Language Models&quot;,&quot;code LLMs&quot;,&quot;context pruning&quot;,&quot;LLMLingua&quot;,&quot;conditional perplexity&quot;,&quot;function-level chunks&quot;,&quot;fine-grained compression&quot;,&quot;token budget&quot;,&quot;code completion&quot;,&quot;code summarization&quot;,&quot;question answering&quot;,&quot;code intelligence applications&quot;],&quot;githubStars&quot;:61,&quot;organization&quot;:{&quot;_id&quot;:&quot;6724d0b84c0a2bf36e39a226&quot;,&quot;name&quot;:&quot;Stanford-University&quot;,&quot;fullname&quot;:&quot;Stanford University&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/6724cfba409a4f96ce1d773b/BjPcf9AfmEU3WeSVb33s8.png&quot;}},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;645b0c3ec35da9c7afd95421&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuling&quot;,&quot;user&quot;:&quot;YerbaPage&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;678781fb6bc613abc37f5ad4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/678781fb6bc613abc37f5ad4/7Se4ieXuSG3Vac_m0REYs.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;YANG&quot;,&quot;user&quot;:&quot;YangLCC&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b1a165de36d6ef979f636f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b1a165de36d6ef979f636f/Vk6rmQFZ3FSzwOAk3SqBr.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;fanwanx&quot;,&quot;user&quot;:&quot;FANTKwan&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b1a0d17c5844c6bcf091db&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bf933b81eb35b529741c1704ad8ef73b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;benqi&quot;,&quot;user&quot;:&quot;Aidabenk&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b1a02c46b13200819aec08&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b1a02c46b13200819aec08/83apjllqYocftrXgeeSp9.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;fenchenw&quot;,&quot;user&quot;:&quot;Fanwenyoo&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b19f81615a3737b5772b3b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2ca4c53715e82a7e17c4b631eaf34042.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Fenzhif&quot;,&quot;user&quot;:&quot;FANCERTA&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b192229becd2d044116314&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b192229becd2d044116314/IXDR0p9_-PcgVxJOnQZY-.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;S.Dong&quot;,&quot;user&quot;:&quot;Shiny-JI&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b1937446b132008197d856&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b1937446b132008197d856/t9N1XCp7e2--P-xSa8pbk.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;KSBMyu05&quot;,&quot;user&quot;:&quot;KangSubMi&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b194a5ecf00cb7a519d4f1&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b194a5ecf00cb7a519d4f1/hLNuSoM1kSTp0GElLpF2P.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Sally&quot;,&quot;user&quot;:&quot;CArriy&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b1963c73b4976b632ab4c4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b1963c73b4976b632ab4c4/0mFN3OvlUO1NH9hKvOfZR.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Aki&quot;,&quot;user&quot;:&quot;Caleboo&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b198a3b25d54b45106b857&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b198a3b25d54b45106b857/y8pqqLDaOSIBM2fKHrePn.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;SongHye&quot;,&quot;user&quot;:&quot;ADidennn&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67b19a7d2a4cd18639e740fb&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/67b19a7d2a4cd18639e740fb/mJTfZWFJ6sPc7zNf2PmEM.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Elvisy&quot;,&quot;user&quot;:&quot;JEonColin&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:3,&quot;organization&quot;:{&quot;_id&quot;:&quot;6724d0b84c0a2bf36e39a226&quot;,&quot;name&quot;:&quot;Stanford-University&quot;,&quot;fullname&quot;:&quot;Stanford University&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/6724cfba409a4f96ce1d773b/BjPcf9AfmEU3WeSVb33s8.png&quot;}}\"> Papers arxiv:2510.00446","published":"2025-10-05","source":"huggingface","url":"https://huggingface.co/papers/2510.00446","analysis":{"introduction":"üöÄ Struggling with huge code contexts that spike API costs and slow down LLMs? LongCodeZip is a plug‚Äëand‚Äëplay framework that compresses long code context for code LLMs so they scale to real-world repos while preserving performance ‚Äî ideal for dev tools & code search.","challenges":"üéØ Problems tackled: - High API costs & generation latency when LLMs process long code contexts. - Generic pruning (e.g., LLMLingua) ignores code structure and dependencies ‚Üí suboptimal for programming tasks. - Difficulty scaling LLM reasoning across large, multi-file codebases.","innovations":"‚ú® Key innovations: - Coarse-grained stage: rank function-level chunks by conditional perplexity w.r.t. the instruction and retain the most relevant functions. - Fine-grained stage: segment retained functions into perplexity-based blocks and select an optimal subset under an adaptive token budget. - Novelty: code-structure-aware, dual-stage perplexity compression tailored for code LLMs.","experiments":"üìä Results: Evaluated on code completion, summarization, and QA ‚Äî LongCodeZip consistently outperforms baselines and achieves up to 5.6√ó compression without degrading task performance. Proof: substantial context reduction while preserving accuracy.","insights":"ü§î What's next? - Research idea: replace or augment perplexity ranking with learned relevance models fine-tuned on code signals. - Apply dynamic/online compression during editing or CI to reduce latency and cost in practice. Broader impact: cheaper, scalable code LLM APIs and more practical repo-level assistants. What apps become possible at scale?","keywords":["Large Language Models","code LLMs","context pruning","conditional perplexity","function-level chunks","fine-grained compression","adaptive token budget","code completion","code summarization","question answering"],"category":"machine_learning","relevance_score":9,"technical_depth":"intermediate","summary":"**Introduction:** üöÄ Struggling with huge code contexts that spike API costs and slow down LLMs? LongCodeZip is a plug‚Äëand‚Äëplay framework that compresses long code context for code LLMs so they scale to real-world repos while preserving performance ‚Äî ideal for dev tools & code search.\n\n**Challenges:** üéØ Problems tackled: - High API costs & generation latency when LLMs process long code contexts. - Generic pruning (e.g., LLMLingua) ...","analyzed_at":"2025-10-05T08:07:49.126Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-05T08:04:44.784Z","views":0,"archive_metadata":{"archived_at":"2025-10-05T08:08:43.268Z","original_id":"hf_longcodezip__compress_long_context_for_code_language_models_1759651484780","views_at_archive":0}},{"id":"arxiv_2510.02296v1","title":"Continual Personalization for Diffusion Models","authors":["Yu-Chien Liao","Jr-Jen Chen","Chi-Pin Huang","Ci-Siang Lin","Meng-Lin Wu","Yu-Chiang Frank Wang"],"abstract":"Updating diffusion models in an incremental setting would be practical in\nreal-world applications yet computationally challenging. We present a novel\nlearning strategy of Concept Neuron Selection (CNS), a simple yet effective\napproach to perform personalization in a continual learning scheme. CNS\nuniquely identifies neurons in diffusion models that are closely related to the\ntarget concepts. In order to mitigate catastrophic forgetting problems while\npreserving zero-shot text-to-image generation ability, CNS finetunes concept\nneurons in an incremental manner and jointly preserves knowledge learned of\nprevious concepts. Evaluation of real-world datasets demonstrates that CNS\nachieves state-of-the-art performance with minimal parameter adjustments,\noutperforming previous methods in both single and multi-concept personalization\nworks. CNS also achieves fusion-free operation, reducing memory storage and\nprocessing time for continual personalization.","published":"2025-10-02T17:58:56Z","source":"arxiv","url":"http://arxiv.org/abs/2510.02296v1","analysis":{"introduction":"üöÄ Want image models that learn new concepts without retraining everything? Concept Neuron Selection (CNS) personalizes diffusion models incrementally by identifying and finetuning only concept-related neurons. Practical for user-specific generators and continual updates.","challenges":"üéØ Key problems tackled: - Incrementally updating diffusion models is computationally challenging. - Catastrophic forgetting when adding new concepts. - Need to preserve zero-shot text-to-image capabilities while personalizing. - Reduce memory/storage and processing time for continual personalization.","innovations":"‚ú® Core innovations: - Concept Neuron Selection (CNS): identifies neurons in diffusion models tied to target concepts. - Incremental finetuning of selected concept neurons to add new concepts. - Joint preservation of previously learned concepts to mitigate forgetting. - Fusion-free operation (no costly model merging), minimizing extra storage and runtime. Novel twist: neuron-level selection inside diffusion models for continual personalization with minimal parameter changes.","experiments":"üìä Most compelling quantitative result: Not specified in the paper. Main experimental breakthrough: CNS is reported to achieve state-of-the-art performance in continual personalization, outperforming prior methods on both single- and multi-concept personalization while using minimal parameter adjustments and enabling fusion-free, lower-memory operation.","insights":"ü§î Potential next steps & applications: - Explore automated neuron selection policies or learnable selectors for broader model families (e.g., text encoders, other generative backbones). - Investigate on-device continual personalization and privacy-preserving updates using CNS-style sparse updates. Applications: personalized avatars, adaptive content creation, and efficient multi-user model deployments. Could neuron-level personalization become the standard for lightweight, continual model adaptation?","keywords":["Continual learning","Diffusion models","Personalization","Concept neuron","Catastrophic forgetting","Zero-shot","Model efficiency","Fusion-free"],"category":"computer_vision","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Want image models that learn new concepts without retraining everything? Concept Neuron Selection (CNS) personalizes diffusion models incrementally by identifying and finetuning only concept-related neurons. Practical for user-specific generators and continual updates.\n\n**Challenges:** üéØ Key problems tackled: - Incrementally updating diffusion models is computationally challenging. - Catastrophic forgetting whe...","analyzed_at":"2025-10-05T08:07:20.274Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-05T08:04:09.771Z","views":0,"archive_metadata":{"archived_at":"2025-10-05T08:08:43.268Z","original_id":"arxiv_2510.02296v1","views_at_archive":0}},{"id":"arxiv_2510.02300v1","title":"Equilibrium Matching: Generative Modeling with Implicit Energy-Based\n  Models","authors":["Runqian Wang","Yilun Du"],"abstract":"We introduce Equilibrium Matching (EqM), a generative modeling framework\nbuilt from an equilibrium dynamics perspective. EqM discards the\nnon-equilibrium, time-conditional dynamics in traditional diffusion and\nflow-based generative models and instead learns the equilibrium gradient of an\nimplicit energy landscape. Through this approach, we can adopt an\noptimization-based sampling process at inference time, where samples are\nobtained by gradient descent on the learned landscape with adjustable step\nsizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation\nperformance of diffusion/flow models empirically, achieving an FID of 1.90 on\nImageNet 256$\\times$256. EqM is also theoretically justified to learn and\nsample from the data manifold. Beyond generation, EqM is a flexible framework\nthat naturally handles tasks including partially noised image denoising, OOD\ndetection, and image composition. By replacing time-conditional velocities with\na unified equilibrium landscape, EqM offers a tighter bridge between flow and\nenergy-based models and a simple route to optimization-driven inference.","published":"2025-10-02T17:59:06Z","source":"arxiv","url":"http://arxiv.org/abs/2510.02300v1","analysis":{"introduction":"üöÄ What if generative models could ditch time-conditional chains and just optimize to a data landscape? Equilibrium Matching (EqM) learns an equilibrium gradient of an implicit energy and samples by optimization ‚Äî faster, adaptive inference that boosts image generation (and more).","challenges":"üéØ Problems tackled: - Diffusion/flow models rely on non-equilibrium, time-conditional dynamics that complicate training & sampling. - Fixed, sequential samplers limit adaptive compute and optimizer choices at inference. - Existing bridges between flows and EBMs are loose and inefficient.","innovations":"‚ú® Key ideas: - Equilibrium Matching (EqM): learn the equilibrium gradient of an implicit energy landscape. - Replace time-conditional velocities with a unified equilibrium landscape. - Optimization-based sampling: gradient descent on learned energy with adjustable step sizes, adaptive optimizers, and adaptive compute. - Theoretical justification for learning/sampling the data manifold.","experiments":"üìä Main result: EqM achieves FID 1.90 on ImageNet 256√ó256, outperforming prior diffusion/flow methods empirically. This demonstrates that optimization-driven sampling on an implicit equilibrium energy can reach SOTA generation quality.","insights":"ü§î Where to go next: - Explore learned optimizers or learned samplers on the EqM landscape for faster convergence. - Apply EqM to conditional generation, other modalities (audio/3D), or as a robust OOD detection backbone. Could optimization-first generative modeling reshape model-efficiency tradeoffs?","keywords":["Equilibrium Matching","EqM","implicit energy-based models","energy gradient","diffusion","flow models","optimization-based sampling","ImageNet","FID 1.90","denoising","OOD detection","image composition","data manifold"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ What if generative models could ditch time-conditional chains and just optimize to a data landscape? Equilibrium Matching (EqM) learns an equilibrium gradient of an implicit energy and samples by optimization ‚Äî faster, adaptive inference that boosts image generation (and more).\n\n**Challenges:** üéØ Problems tackled: - Diffusion/flow models rely on non-equilibrium, time-conditional dynamics that complicate trainin...","analyzed_at":"2025-10-05T08:06:44.025Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-05T08:04:09.771Z","views":0,"archive_metadata":{"archived_at":"2025-10-05T08:08:43.268Z","original_id":"arxiv_2510.02300v1","views_at_archive":0}},{"id":"arxiv_2510.02312v1","title":"KaVa: Latent Reasoning via Compressed KV-Cache Distillation","authors":["Anna Kuzina","Maciej Pioro","Paul N. Whatmough","Babak Ehteshami Bejnordi"],"abstract":"Large Language Models (LLMs) excel at multi-step reasoning problems with\nexplicit chain-of-thought (CoT), but verbose traces incur significant\ncomputational costs and memory overhead, and often carry redundant, stylistic\nartifacts. Latent reasoning has emerged as an efficient alternative that\ninternalizes the thought process, but it suffers from a critical lack of\nsupervision, limiting its effectiveness on complex, natural-language reasoning\ntraces. In this work, we propose KaVa, the first framework that bridges this\ngap by distilling knowledge directly from a compressed KV-cache of the teacher\ninto a latent-reasoning student via self-distillation, leveraging the\nrepresentational flexibility of continuous latent tokens to align stepwise KV\ntrajectories. We show that the abstract, unstructured knowledge within\ncompressed KV-cache, which lacks direct token correspondence, can serve as a\nrich supervisory signal for a latent reasoning student. Empirically, the\napproach consistently outperforms strong latent baselines, exhibits markedly\nsmaller degradation from equation-only to natural-language traces, and scales\nto larger backbones while preserving efficiency. These results establish\ncompressed KV-cache distillation as a scalable supervision signal for latent\nreasoning, combining the accuracy of CoT-trained teachers with the efficiency\nand deployability of latent inference.","published":"2025-10-02T17:59:51Z","source":"arxiv","url":"http://arxiv.org/abs/2510.02312v1","analysis":{"introduction":"üöÄ Want CoT-level reasoning without the huge compute and verbose traces? KaVa distills a teacher's compressed KV-cache into a latent-reasoning student, keeping CoT accuracy while enabling efficient, deployable latent inference for complex reasoning.","challenges":"üéØ Challenges: - Chain-of-thought (CoT) yields verbose, costly traces and memory overhead. - Latent reasoning is efficient but lacks supervision for complex, natural-language traces. - Compressed KV-caches contain useful but unaligned, abstract info.","innovations":"‚ú® Innovations: - Distill knowledge from a teacher's compressed KV-cache into a latent student via self-distillation. - Align stepwise KV trajectories using continuous latent tokens. - Novel use of compressed KV-cache as a supervisory signal for latent reasoning.","experiments":"üìä Experiments: Not specified in the paper. Qualitative findings: KaVa consistently outperforms strong latent baselines, shows much smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency.","insights":"ü§î Insights & next steps: - Explore applying KV-cache distillation to multimodal or retrieval-augmented models. - Investigate on-device/edge deployment for cost-sensitive apps and better latent interpretability. Could this make CoT-quality reasoning practical in low-cost settings?","keywords":["KaVa","latent reasoning","KV-cache","compressed KV-cache","distillation","self-distillation","chain-of-thought","LLMs"],"category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Want CoT-level reasoning without the huge compute and verbose traces? KaVa distills a teacher's compressed KV-cache into a latent-reasoning student, keeping CoT accuracy while enabling efficient, deployable latent inference for complex reasoning.\n\n**Challenges:** üéØ Challenges: - Chain-of-thought (CoT) yields verbose, costly traces and memory overhead. - Latent reasoning is efficient but lacks supervision for co...","analyzed_at":"2025-10-05T08:05:59.515Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-05T08:04:09.771Z","views":19,"archive_metadata":{"archived_at":"2025-10-05T08:08:43.268Z","original_id":"arxiv_2510.02312v1","views_at_archive":19}},{"id":"arxiv_2510.02298v1","title":"ARMADA: Autonomous Online Failure Detection and Human Shared Control\n  Empower Scalable Real-world Deployment and Adaptation","authors":["Wenye Yu","Jun Lv","Zixi Ying","Yang Jin","Chuan Wen","Cewu Lu"],"abstract":"Imitation learning has shown promise in learning from large-scale real-world\ndatasets. However, pretrained policies usually perform poorly without\nsufficient in-domain data. Besides, human-collected demonstrations entail\nsubstantial labour and tend to encompass mixed-quality data and redundant\ninformation. As a workaround, human-in-the-loop systems gather domain-specific\ndata for policy post-training, and exploit closed-loop policy feedback to offer\ninformative guidance, but usually require full-time human surveillance during\npolicy rollout. In this work, we devise ARMADA, a multi-robot deployment and\nadaptation system with human-in-the-loop shared control, featuring an\nautonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA\nenables paralleled policy rollout and requests human intervention only when\nnecessary, significantly reducing reliance on human supervision. Hence, ARMADA\nenables efficient acquisition of in-domain data, and leads to more scalable\ndeployment and faster adaptation to new scenarios. We evaluate the performance\nof ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on\naverage, surpassing prior state-of-the-art failure detection approaches by over\n20%. Besides, ARMADA manifests more than 4$\\times$ increase in success rate and\ngreater than 2$\\times$ reduction in human intervention rate over multiple\nrounds of policy rollout and post-training, compared to previous\nhuman-in-the-loop learning methods.","published":"2025-10-02T17:59:02Z","source":"arxiv","url":"http://arxiv.org/abs/2510.02298v1","analysis":{"introduction":"üöÄ Struggling to scale robot policies in the real world? ARMADA is a multi-robot, human-in-the-loop deployment + adaptation system that uses FLOAT, an autonomous online failure detector. It parallelizes rollouts and calls humans only when needed‚Äîcutting supervision and speeding adaptation.","challenges":"üéØ Key problems addressed: - Pretrained policies fail without sufficient in-domain data. - Human demonstrations are costly, mixed-quality, and redundant. - Human-in-the-loop rollouts usually demand full-time supervision, limiting scalability.","innovations":"‚ú® Core contributions: - ARMADA: multi-robot deployment & post-training pipeline with shared human control. - FLOAT: autonomous online failure detection that triggers human intervention only as needed. - Novelty: FLOAT enables parallel policy rollouts with sparse, targeted human input‚Äîscalable real-world adaptation.","experiments":"üìä Main results: FLOAT achieves nearly 95% average accuracy, outperforming prior failure-detection approaches by >20%. ARMADA delivers >4√ó increase in task success rate and >2√ó reduction in human intervention rate across multiple rollout/post-training rounds‚Äîshowing scalable, efficient adaptation.","insights":"ü§î Where to go next: - Research directions: extend FLOAT to heterogeneous fleets and integrate active learning to minimize queries; explore autonomous recovery actions when failures are detected. - Applications: warehouse robots, field robotics, assistive robots in care settings. Could sparse, confidence-driven intervention redefine fleet-scale deployment?","keywords":["ARMADA","FLOAT","failure detection","human-in-the-loop","shared control","imitation learning","online adaptation","multi-robot deployment","real-world datasets","policy post-training"],"category":"robotics","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Struggling to scale robot policies in the real world? ARMADA is a multi-robot, human-in-the-loop deployment + adaptation system that uses FLOAT, an autonomous online failure detector. It parallelizes rollouts and calls humans only when needed‚Äîcutting supervision and speeding adaptation.\n\n**Challenges:** üéØ Key problems addressed: - Pretrained policies fail without sufficient in-domain data. - Human demonstration...","analyzed_at":"2025-10-05T08:06:54.067Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-05T08:04:09.771Z","views":0,"archive_metadata":{"archived_at":"2025-10-05T08:08:43.268Z","original_id":"arxiv_2510.02298v1","views_at_archive":0}},{"id":"arxiv_2510.02313v1","title":"Clink! Chop! Thud! -- Learning Object Sounds from Real-World\n  Interactions","authors":["Mengyu Yang","Yiming Chen","Haozheng Pei","Siddhant Agarwal","Arun Balajee Vasudevan","James Hays"],"abstract":"Can a model distinguish between the sound of a spoon hitting a hardwood floor\nversus a carpeted one? Everyday object interactions produce sounds unique to\nthe objects involved. We introduce the sounding object detection task to\nevaluate a model's ability to link these sounds to the objects directly\ninvolved. Inspired by human perception, our multimodal object-aware framework\nlearns from in-the-wild egocentric videos. To encourage an object-centric\napproach, we first develop an automatic pipeline to compute segmentation masks\nof the objects involved to guide the model's focus during training towards the\nmost informative regions of the interaction. A slot attention visual encoder is\nused to further enforce an object prior. We demonstrate state of the art\nperformance on our new task along with existing multimodal action understanding\ntasks.","published":"2025-10-02T17:59:52Z","source":"arxiv","url":"http://arxiv.org/abs/2510.02313v1","analysis":{"introduction":"üöÄ Can a model tell a spoon clink on hardwood vs. carpet?  This paper introduces \"sounding object detection\": a multimodal, object-aware framework that learns from in-the-wild egocentric videos to link sounds to the actual objects involved.  Why it matters: improves object-level audio-visual understanding.","challenges":"üéØ Key problems tackled: - Models struggle to link specific sounds to the exact objects producing them. - Lack of object-centric supervision in audio-visual learning. - Need for methods that learn from in-the-wild egocentric interactions.","innovations":"‚ú® Core innovations: - Automatic pipeline to compute segmentation masks of interacting objects to guide training. - Slot-attention visual encoder to enforce an object prior. - Multimodal object-aware framework that trains on egocentric videos. Novelty: object-centric audio-visual learning guided by automatically extracted masks + slot attention.","experiments":"üìä Results & proof: - Demonstrated state-of-the-art performance on the new sounding object detection task and on existing multimodal action understanding benchmarks. - Specific numeric improvements: Not specified in the paper.","insights":"ü§î What's next? - Research directions: explore object-level audio-visual pretraining and self-supervised object-sound representations; adapt for real-time robotic perception during manipulation. - Applications: improved robot touch/audio feedback, richer AR/VR sound grounding. Could object-level sound grounding enable safer manipulation?","keywords":["sounding object detection","audio-visual learning","egocentric video","slot attention","segmentation masks","object-centric","multimodal","sound recognition"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","summary":"**Introduction:** üöÄ Can a model tell a spoon clink on hardwood vs. carpet?  This paper introduces \"sounding object detection\": a multimodal, object-aware framework that learns from in-the-wild egocentric videos to link sounds to the actual objects involved.  Why it matters: improves object-level audio-visual understanding.\n\n**Challenges:** üéØ Key problems tackled: - Models struggle to link specific sounds to the exact objects produ...","analyzed_at":"2025-10-05T08:05:40.546Z","model":"openai/gpt-5-mini"},"scraped_at":"2025-10-05T08:04:09.771Z","views":2,"archive_metadata":{"archived_at":"2025-10-05T08:08:43.268Z","original_id":"arxiv_2510.02313v1","views_at_archive":2}}],"metadata":{"total_papers":10,"categories":{"machine_learning":6,"natural_language_processing":2,"computer_vision":1,"robotics":1},"sources":{"huggingface":5,"arxiv":5},"average_score":9,"unique_keywords":["reinforcement learning","pretraining","next-token prediction","chain-of-thought","information gain","log-likelihood","dense reward","Qwen3-1.7B","NeMo-12B","reasoning","CLUE","hidden states","non-parametric verifier","nearest-centroid","hidden-state delta","LLM verification","AIME","GPQA","ModernVBERT","visual document retrieval","vision-language encoder","contrastive learning","late interaction","attention masking","image resolution","modality alignment","reinforcement learning from verifiable rewards","RLVR","on-policy training","rollout experiences","experience characteristics","rollout correctness","entropy","ExGRPO","Experiential Group Relative Policy Optimization","mixed-policy objective","exploration","experience exploitation","reasoning performance","mathematical benchmarks","general benchmarks","Large Language Models","code LLMs","context pruning","conditional perplexity","function-level chunks","fine-grained compression","adaptive token budget","code completion","code summarization"],"total_views":21,"created_at":"2025-10-05T08:08:43.268Z","source":"daily_update","auto_archived":true,"papers_archived":10,"total_papers_analyzed":23}}