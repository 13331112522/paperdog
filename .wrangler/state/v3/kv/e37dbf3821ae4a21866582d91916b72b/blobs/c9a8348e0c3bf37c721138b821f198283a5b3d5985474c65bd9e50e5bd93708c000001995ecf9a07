{"date":"2025-09-18","papers":[{"id":"arxiv_2509.14227v1","title":"Cinéaste: A Fine-grained Contextual Movie Question Answering\n  Benchmark","authors":["Nisarg A. Shah","Amir Ziai","Chaitanya Ekanadham","Vishal M. Patel"],"abstract":"While recent advancements in vision-language models have improved video\nunderstanding, diagnosing their capacity for deep, narrative comprehension\nremains a challenge. Existing benchmarks often test short-clip recognition or\nuse template-based questions, leaving a critical gap in evaluating fine-grained\nreasoning over long-form narrative content. To address these gaps, we introduce\n$\\mathsf{Cin\\acute{e}aste}$, a comprehensive benchmark for long-form movie\nunderstanding. Our dataset comprises 3,119 multiple-choice question-answer\npairs derived from 1,805 scenes across 200 diverse movies, spanning five novel\nfine-grained contextual reasoning categories. We use GPT-4o to generate\ndiverse, context-rich questions by integrating visual descriptions, captions,\nscene titles, and summaries, which require deep narrative understanding. To\nensure high-quality evaluation, our pipeline incorporates a two-stage filtering\nprocess: Context-Independence filtering ensures questions require video\ncontext, while Contextual Veracity filtering validates factual consistency\nagainst the movie content, mitigating hallucinations. Experiments show that\nexisting MLLMs struggle on $\\mathsf{Cin\\acute{e}aste}$; our analysis reveals\nthat long-range temporal reasoning is a primary bottleneck, with the top\nopen-source model achieving only 63.15\\% accuracy. This underscores significant\nchallenges in fine-grained contextual understanding and the need for\nadvancements in long-form movie comprehension.","published":"2025-09-17T17:58:06Z","source":"arxiv","url":"http://arxiv.org/abs/2509.14227v1","analysis":{"introduction":"The paper addresses the limitations of current vision-language models in understanding long-form narratives, particularly in the context of movies. Existing benchmarks primarily focus on short clips or template-based questions, failing to evaluate deep narrative comprehension. The authors introduce the Cinéaste benchmark, which aims to fill this gap by providing a comprehensive dataset for fine-grained contextual movie question answering.","challenges":"Key challenges include the need for deep narrative understanding and the ability to reason over long temporal sequences in video content. Existing approaches often lack the capacity to handle complex, context-dependent questions that require an understanding of the entire movie narrative, leading to significant performance limitations in current models.","innovations":"Cinéaste introduces a novel dataset comprising 3,119 question-answer pairs derived from 1,805 movie scenes across 200 films, categorized into five fine-grained contextual reasoning types. The authors employ GPT-4o for generating context-rich questions that integrate visual descriptions and narrative elements. A two-stage filtering process ensures that questions are context-dependent and factually consistent, addressing issues of hallucination in model responses. This methodological advancement represents a significant contribution to the field of movie comprehension.","experiments":"The experimental setup involved evaluating existing multi-modal language models (MLLMs) on the Cinéaste benchmark. The results showed that the best-performing open-source model achieved only 63.15% accuracy, highlighting the difficulty of the task. The authors analyzed the performance bottlenecks, identifying long-range temporal reasoning as a critical challenge that existing models struggle to address, thus demonstrating the benchmark's effectiveness in revealing model limitations.","insights":"Cinéaste has significant implications for advancing the field of video understanding and narrative comprehension. It opens avenues for developing more sophisticated models capable of handling complex reasoning tasks in long-form content. Future research could explore enhancements in model architectures and training methodologies to improve performance on such benchmarks.","keywords":["movie understanding","question answering","dataset","narrative comprehension","vision-language models","temporal reasoning","GPT-4o","fine-grained reasoning"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"尽管最近的视觉语言模型在视频理解方面取得了进展，但诊断它们在深层叙事理解方面的能力仍然是一项挑战。现有基准通常测试短片段识别或使用模板问题，留下了评估长篇叙事内容的细粒度推理的关键空白。为了解决这些问题，我们引入了Cinéaste，这是一个全面的长篇电影理解基准。我们的数据集由3,119个多项选择问题-答案对组成，来自200部多样化电影的1,805个场景，涵盖五个新颖的细粒度上下文推理类别。我们使用GPT-4o生成多样的、丰富上下文的问题，通过整合视觉描述、标题和摘要，这些问题需要深刻的叙事理解。为了确保高质量的评估，我们的流程包括一个两阶段过滤过程：上下文独立过滤确保问题需要视频上下文，而上下文真实性过滤验证与电影内容的一致性，从而减轻幻觉。实验表明，现有的MLLM在Cinéaste上表现不佳；我们的分析表明，长距离时间推理是主要瓶颈，顶级开源模型的准确率仅为63.15%。这突显了细粒度上下文理解的重大挑战，以及在长篇电影理解方面的进步需求。","chinese_introduction":"中文介绍：研究背景和动机","chinese_challenges":"中文挑战：主要技术挑战","chinese_innovations":"中文创新：新方法和贡献","chinese_experiments":"中文实验：实验设置和结果","chinese_insights":"中文见解：领域意义和未来方向","summary":"**Introduction:** The paper addresses the limitations of current vision-language models in understanding long-form narratives, particularly in the context of movies. Existing benchmarks primarily focus on short clips or template-based questions, failing to evaluate deep narrative comprehension. The authors introduce the Cinéaste benchmark, which aims to fill this gap by providing a comprehensive dataset for fine-grained contextual movie question answering.","analyzed_at":"2025-09-18T21:42:54.062Z","model":"openai/gpt-4o-mini"},"scraped_at":"2025-09-18T21:39:49.439Z","archive_metadata":{"archived_at":"2025-09-18T21:51:27.750Z","original_id":"arxiv_2509.14227v1"}},{"id":"arxiv_2509.14219v1","title":"Data Denoising and Derivative Estimation for Data-Driven Modeling of\n  Nonlinear Dynamical Systems","authors":["Jiaqi Yao","Lewis Mitchell","John Maclean","Hemanth Saratchandran"],"abstract":"Data-driven modeling of nonlinear dynamical systems is often hampered by\nmeasurement noise. We propose a denoising framework, called Runge-Kutta and\nTotal Variation Based Implicit Neural Representation (RKTV-INR), that\nrepresents the state trajectory with an implicit neural representation (INR)\nfitted directly to noisy observations. Runge-Kutta integration and total\nvariation are imposed as constraints to ensure that the reconstructed state is\na trajectory of a dynamical system that remains close to the original data. The\ntrained INR yields a clean, continuous trajectory and provides accurate\nfirst-order derivatives via automatic differentiation. These denoised states\nand derivatives are then supplied to Sparse Identification of Nonlinear\nDynamics (SINDy) to recover the governing equations. Experiments demonstrate\neffective noise suppression, precise derivative estimation, and reliable system\nidentification.","published":"2025-09-17T17:51:43Z","source":"arxiv","url":"http://arxiv.org/abs/2509.14219v1","analysis":{"introduction":"The research addresses the challenge of data-driven modeling of nonlinear dynamical systems, which is often impeded by measurement noise. Accurate modeling is crucial for predicting system behavior, yet noisy observations can lead to significant errors in the derived models. This paper proposes a novel framework that integrates denoising techniques with implicit neural representations to enhance the fidelity of state trajectory reconstruction from noisy data.","challenges":"Key challenges include effectively suppressing noise while preserving the underlying dynamics of the system. Existing methods often struggle with maintaining the continuity and differentiability of the reconstructed trajectories, leading to inaccuracies in derivative estimation. Additionally, traditional approaches may not adequately incorporate the constraints of dynamical systems, which can result in poor system identification.","innovations":"The proposed RKTV-INR framework introduces a unique combination of Runge-Kutta integration and total variation constraints within an implicit neural representation. This innovative approach allows for the direct fitting of the INR to noisy observations, ensuring that the reconstructed state adheres closely to the dynamics of the original system. The framework not only provides a continuous and clean trajectory but also enables accurate first-order derivative estimation through automatic differentiation. This dual capability enhances the Sparse Identification of Nonlinear Dynamics (SINDy) method for recovering governing equations, representing a significant advancement in the field.","experiments":"The experimental setup involved testing the RKTV-INR framework on synthetic datasets with varying levels of noise. Key metrics included noise suppression effectiveness, accuracy of derivative estimation, and reliability of system identification compared to baseline methods. Results demonstrated that the RKTV-INR significantly outperformed traditional denoising techniques, achieving superior noise reduction and more precise derivative estimates, which facilitated more accurate recovery of the governing equations through SINDy.","insights":"The implications of this research extend to various applications in engineering, physics, and biology, where accurate modeling of dynamical systems is essential. The RKTV-INR framework provides a robust tool for researchers and practitioners dealing with noisy data. Future research directions may include exploring the application of this framework to real-world datasets and extending its capabilities to higher-dimensional systems.","keywords":["data-driven modeling","nonlinear dynamical systems","implicit neural representation","Runge-Kutta","total variation","derivative estimation","SINDy","noise suppression"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"数据驱动的非线性动态系统建模通常受到测量噪声的困扰。我们提出了一种去噪框架，称为基于Runge-Kutta和全变差的隐式神经表示（RKTV-INR），该框架直接拟合到噪声观测值的隐式神经表示。施加Runge-Kutta积分和全变差作为约束，以确保重建的状态是接近原始数据的动态系统轨迹。训练后的INR产生干净、连续的轨迹，并通过自动微分提供准确的一阶导数。这些去噪状态和导数随后被提供给稀疏非线性动力学识别（SINDy）以恢复控制方程。实验表明有效的噪声抑制、精确的导数估计和可靠的系统识别。","chinese_introduction":"中文介绍：研究背景和动机","chinese_challenges":"中文挑战：主要技术挑战","chinese_innovations":"中文创新：新方法和贡献","chinese_experiments":"中文实验：实验设置和结果","chinese_insights":"中文见解：领域意义和未来方向","summary":"**Introduction:** The research addresses the challenge of data-driven modeling of nonlinear dynamical systems, which is often impeded by measurement noise. Accurate modeling is crucial for predicting system behavior, yet noisy observations can lead to significant errors in the derived models. This paper proposes a novel framework that integrates denoising techniques with implicit neural representations to enhance the fidelity of state trajectory reconstruction fro...","analyzed_at":"2025-09-18T21:43:42.858Z","model":"openai/gpt-4o-mini"},"scraped_at":"2025-09-18T21:39:49.439Z","archive_metadata":{"archived_at":"2025-09-18T21:51:27.750Z","original_id":"arxiv_2509.14219v1"}},{"id":"arxiv_2509.14128v1","title":"Canary-1B-v2 &amp; Parakeet-TDT-0.6B-v3: Efficient and High-Performance\n  Models for Multilingual ASR and AST","authors":["Monica Sekoyan","Nithin Rao Koluguri","Nune Tadevosyan","Piotr Zelasko","Travis Bartley","Nick Karpov","Jagadeesh Balam","Boris Ginsburg"],"abstract":"This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters.","published":"2025-09-17T16:08:46Z","source":"arxiv","url":"http://arxiv.org/abs/2509.14128v1","analysis":{"introduction":"The paper addresses the growing demand for efficient multilingual Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST) systems. With the increasing global need for language accessibility, the authors present Canary-1B-v2, a model designed to improve performance across 25 primarily European languages. The motivation stems from the limitations of existing ASR systems in handling diverse languages effectively and efficiently, particularly in terms of speed and accuracy.","challenges":"Key challenges include the need for robust multilingual support while maintaining high performance and low latency. Existing models often struggle with hallucinations in ASR and AST outputs, especially when dealing with non-speech audio. Additionally, the computational demands of larger models can hinder real-time applications, making it essential to balance model size and performance effectively.","innovations":"The paper introduces several innovations, including the FastConformer encoder combined with a Transformer decoder, which enhances the model's efficiency and performance. The two-stage pre-training and fine-tuning process, along with dynamic data balancing, represents a significant advancement in training methodologies. The use of the NeMo Forced Aligner (NFA) with an auxiliary CTC model for timestamping further contributes to the model's robustness. The introduction of Parakeet-TDT-0.6B-v3, a smaller model with competitive performance, showcases the authors' commitment to scalability and efficiency.","experiments":"The experimental setup involved training on 1.7M hours of diverse data, including Granary and NeMo ASR Set 3.0, with added non-speech audio to mitigate hallucinations. The results indicate that Canary-1B-v2 outperforms Whisper-large-v3 in English ASR tasks while being ten times faster. Additionally, it demonstrates competitive performance against larger models like Seamless-M4T-v2-large, highlighting its effectiveness in multilingual ASR and AST tasks.","insights":"The findings have significant implications for the field of multilingual ASR and AST, suggesting that efficiency can be achieved without sacrificing performance. Potential applications include real-time translation services, accessibility tools for diverse languages, and integration into voice-activated systems. Future research could explore further optimizations, the incorporation of more languages, and the application of these models in low-resource settings.","keywords":["Automatic Speech Recognition","Speech-to-Text Translation","FastConformer","Transformer","NeMo Forced Aligner","dynamic data balancing","multilingual models","nGPT"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"本报告介绍了Canary-1B-v2，这是一个快速、稳健的多语言自动语音识别（ASR）和语音到文本翻译（AST）模型。该模型采用FastConformer编码器和Transformer解码器，支持25种主要为欧洲的语言。模型在170万小时的总数据样本上训练，包括Granary和NeMo ASR Set 3.0，并添加了非语音音频以减少ASR和AST的幻觉。我们描述了其两阶段的预训练和微调过程，以及动态数据平衡的实验，nGPT编码器的实验结果表明，nGPT在大规模数据上表现良好，而FastConformer在微调后表现出色。对于时间戳，Canary-1B-v2使用NeMo强制对齐器（NFA）和辅助CTC模型，为ASR和AST提供可靠的段级时间戳。评估结果显示，Canary-1B-v2在英语ASR上优于Whisper-large-v3，同时速度快10倍，并且在与更大模型如Seamless-M4T-v2-large和基于LLM的系统的比较中，提供了竞争力的多语言ASR和AST性能。我们还发布了Parakeet-TDT-0.6B-v3，这是v2的继任者，提供相同25种语言的多语言ASR，参数仅为6亿。","chinese_introduction":"中文介绍：研究背景和动机","chinese_challenges":"中文挑战：主要技术挑战","chinese_innovations":"中文创新：新方法和贡献","chinese_experiments":"中文实验：实验设置和结果","chinese_insights":"中文见解：领域意义和未来方向","summary":"**Introduction:** The paper addresses the growing demand for efficient multilingual Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST) systems. With the increasing global need for language accessibility, the authors present Canary-1B-v2, a model designed to improve performance across 25 primarily European languages. The motivation stems from the limitations of existing ASR systems in handling diverse languages effectively and efficiently, part...","analyzed_at":"2025-09-18T21:48:40.741Z","model":"openai/gpt-4o-mini"},"scraped_at":"2025-09-18T21:39:49.439Z","archive_metadata":{"archived_at":"2025-09-18T21:51:27.750Z","original_id":"arxiv_2509.14128v1"}},{"id":"arxiv_2509.14084v1","title":"AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with\n  Anomaly-Aware Calibration","authors":["Jingyi Yuan","Jianxiong Ye","Wenkang Chen","Chenqiang Gao"],"abstract":"Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary\nnovel categories, offering a scalable and annotation-efficient solution.\nTraditionally, most ZSAD works have been based on the CLIP model, which\nperforms anomaly detection by calculating the similarity between visual and\ntext embeddings. Recently, vision foundation models such as DINOv3 have\ndemonstrated strong transferable representation capabilities. In this work, we\nare the first to adapt DINOv3 for ZSAD. However, this adaptation presents two\nkey challenges: (i) the domain bias between large-scale pretraining data and\nanomaly detection tasks leads to feature misalignment; and (ii) the inherent\nbias toward global semantics in pretrained representations often leads to\nsubtle anomalies being misinterpreted as part of the normal foreground objects,\nrather than being distinguished as abnormal regions. To overcome these\nchallenges, we introduce AD-DINOv3, a novel vision-language multimodal\nframework designed for ZSAD. Specifically, we formulate anomaly detection as a\nmultimodal contrastive learning problem, where DINOv3 is employed as the visual\nbackbone to extract patch tokens and a CLS token, and the CLIP text encoder\nprovides embeddings for both normal and abnormal prompts. To bridge the domain\ngap, lightweight adapters are introduced in both modalities, enabling their\nrepresentations to be recalibrated for the anomaly detection task. Beyond this\nbaseline alignment, we further design an Anomaly-Aware Calibration Module\n(AACM), which explicitly guides the CLS token to attend to anomalous regions\nrather than generic foreground semantics, thereby enhancing discriminability.\nExtensive experiments on eight industrial and medical benchmarks demonstrate\nthat AD-DINOv3 consistently matches or surpasses state-of-the-art methods,\nverifying its superiority as a general zero-shot anomaly detection framework.","published":"2025-09-17T15:29:25Z","source":"arxiv","url":"http://arxiv.org/abs/2509.14084v1","analysis":{"introduction":"Zero-Shot Anomaly Detection (ZSAD) is an emerging area that aims to identify anomalies across various novel categories without the need for extensive labeled data. The motivation behind this research is to leverage the capabilities of vision foundation models like DINOv3, which have shown promise in transferable representation learning. The paper addresses the challenge of adapting DINOv3 for ZSAD, focusing on the misalignment of features due to domain bias and the difficulty in distinguishing subtle anomalies from normal foreground objects.","challenges":"The authors identify two primary challenges in adapting DINOv3 for ZSAD: first, the domain bias arising from the disparity between large-scale pretraining data and the specific requirements of anomaly detection tasks, which leads to feature misalignment. Second, the pretrained representations often emphasize global semantics, causing subtle anomalies to be overlooked or misclassified as normal regions, complicating the detection process.","innovations":"The paper introduces AD-DINOv3, a novel multimodal framework that reformulates anomaly detection as a contrastive learning problem. Key innovations include the integration of lightweight adapters in both visual and textual modalities to recalibrate representations for anomaly detection. Additionally, the Anomaly-Aware Calibration Module (AACM) is designed to enhance the CLS token's focus on anomalous regions, improving the model's discriminability. These contributions represent a significant advancement in the application of DINOv3 for ZSAD, addressing the limitations of existing approaches.","experiments":"The experimental setup involves extensive testing on eight industrial and medical benchmarks to evaluate the performance of AD-DINOv3. The results indicate that the proposed method consistently matches or exceeds the performance of state-of-the-art ZSAD techniques, demonstrating its effectiveness. Key metrics used for evaluation include precision, recall, and F1-score, providing a comprehensive assessment of the model's anomaly detection capabilities compared to established baselines.","insights":"The findings of this research have significant implications for the field of anomaly detection, particularly in industrial and medical applications where labeled data is scarce. The AD-DINOv3 framework offers a scalable solution for detecting novel anomalies, paving the way for future research into enhancing model robustness and generalization. Potential applications include real-time monitoring systems and automated inspection processes in various domains.","keywords":["Zero-Shot Anomaly Detection","DINOv3","Anomaly-Aware Calibration","Multimodal Contrastive Learning","Vision-Language Models","Feature Misalignment","Industrial Benchmarks","Medical Benchmarks"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"零样本异常检测（ZSAD）旨在从任意新类别中识别异常，提供可扩展且高效的解决方案。传统上，大多数ZSAD工作基于CLIP模型，通过计算视觉和文本嵌入之间的相似性进行异常检测。最近，像DINOv3这样的视觉基础模型展示了强大的可转移表示能力。本文首次将DINOv3适应于ZSAD，但这一适应面临两个关键挑战：一是大规模预训练数据与异常检测任务之间的领域偏差导致特征不对齐；二是预训练表示对全局语义的固有偏向常常导致微妙的异常被误解为正常前景对象的一部分，而不是被区分为异常区域。为克服这些挑战，我们提出了AD-DINOv3，一个旨在ZSAD的多模态框架。具体而言，我们将异常检测形式化为多模态对比学习问题，其中DINOv3作为视觉骨干提取补丁令牌和CLS令牌，而CLIP文本编码器为正常和异常提示提供嵌入。为了弥合领域差距，我们在两个模态中引入轻量级适配器，使其表示能够重新校准以适应异常检测任务。除了这一基线对齐外，我们还设计了异常感知校准模块（AACM），该模块明确指导CLS令牌关注异常区域，而不是通用前景语义，从而增强可区分性。在八个工业和医疗基准上的大量实验表明，AD-DINOv3始终与或超过最先进的方法，验证了其作为通用零样本异常检测框架的优越性。","chinese_introduction":"中文介绍：研究背景和动机","chinese_challenges":"中文挑战：主要技术挑战","chinese_innovations":"中文创新：新方法和贡献","chinese_experiments":"中文实验：实验设置和结果","chinese_insights":"中文见解：领域意义和未来方向","summary":"**Introduction:** Zero-Shot Anomaly Detection (ZSAD) is an emerging area that aims to identify anomalies across various novel categories without the need for extensive labeled data. The motivation behind this research is to leverage the capabilities of vision foundation models like DINOv3, which have shown promise in transferable representation learning. The paper addresses the challenge of adapting DINOv3 for ZSAD, focusing on the misalignment of features due to ...","analyzed_at":"2025-09-18T21:50:22.108Z","model":"openai/gpt-4o-mini"},"scraped_at":"2025-09-18T21:39:49.439Z","archive_metadata":{"archived_at":"2025-09-18T21:51:27.750Z","original_id":"arxiv_2509.14084v1"}},{"id":"arxiv_2509.14230v1","title":"NIRVANA: Structured pruning reimagined for large language models\n  compression","authors":["Mengting Ai","Tianxin Wei","Sirui Chen","Jingrui He"],"abstract":"Structured pruning of large language models (LLMs) offers substantial\nefficiency improvements by removing entire hidden units, yet current approaches\noften suffer from significant performance degradation, particularly in\nzero-shot settings, and necessitate costly recovery techniques such as\nsupervised fine-tuning (SFT) or adapter insertion. To address these critical\nshortcomings, we introduce NIRVANA, a novel pruning method explicitly designed\nto balance immediate zero-shot accuracy preservation with robust fine-tuning\ncapability. Leveraging a first-order saliency criterion derived from the Neural\nTangent Kernel under Adam optimization dynamics, NIRVANA provides a\ntheoretically grounded pruning strategy that respects essential model training\nbehaviors. To further address the unique challenges posed by structured\npruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across\nlayers and modules (attention vs. MLP), which adjusts pruning intensity between\nmodules in a globally balanced manner. Additionally, to mitigate the high\nsensitivity of pruning decisions to calibration data quality, we propose a\nsimple yet effective KL divergence-based calibration data selection strategy,\nensuring more reliable and task-agnostic pruning outcomes. Comprehensive\nexperiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA\noutperforms existing structured pruning methods under equivalent sparsity\nconstraints, providing a theoretically sound and practical approach to LLM\ncompression. The code is available at\nhttps://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.","published":"2025-09-17T17:59:00Z","source":"arxiv","url":"http://arxiv.org/abs/2509.14230v1","analysis":{"introduction":"The paper addresses the pressing need for efficient compression techniques in large language models (LLMs) through structured pruning. Current methods often lead to performance degradation, especially in zero-shot scenarios, and require expensive recovery techniques like supervised fine-tuning. NIRVANA aims to overcome these issues by preserving zero-shot accuracy while enhancing fine-tuning capabilities.","challenges":"The primary challenges include the significant performance drop in zero-shot tasks due to structured pruning and the reliance on costly recovery methods. Existing approaches often fail to maintain model integrity post-pruning, leading to suboptimal performance and increased computational costs.","innovations":"NIRVANA introduces a first-order saliency criterion based on the Neural Tangent Kernel, providing a theoretically grounded pruning strategy that aligns with model training behaviors. It features an adaptive sparsity allocation mechanism that balances pruning intensity across different layers and modules. Additionally, a KL divergence-based calibration data selection strategy is proposed to enhance the reliability of pruning outcomes, making it task-agnostic.","experiments":"The authors conducted extensive experiments on Llama3, Qwen, and T5 models to evaluate NIRVANA's performance. The results indicate that NIRVANA outperforms existing structured pruning methods while maintaining equivalent sparsity levels. Key metrics include zero-shot accuracy and fine-tuning efficiency, demonstrating significant improvements over baseline methods.","insights":"NIRVANA's approach has important implications for the efficiency of LLMs, potentially enabling their deployment in resource-constrained environments. Future research could explore further refinements in pruning techniques and their applications across various NLP tasks, as well as the integration of NIRVANA with other model compression strategies.","keywords":["structured pruning","large language models","compression","Neural Tangent Kernel","adaptive sparsity","KL divergence","zero-shot learning","fine-tuning"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"结构化剪枝大型语言模型（LLMs）通过移除整个隐藏单元提供了显著的效率提升，但当前方法往往在零-shot 设置中遭受显著的性能下降，并且需要昂贵的恢复技术，如监督微调（SFT）或适配器插入。为了解决这些关键缺陷，我们引入了NIRVANA，一种新颖的剪枝方法，专门设计用于平衡即时零-shot 准确性保留与稳健的微调能力。利用基于Adam优化动态的神经切线核导出的一级显著性标准，NIRVANA提供了一种理论上合理的剪枝策略，尊重模型训练行为的本质。为了进一步解决结构化剪枝所带来的独特挑战，NIRVANA在层和模块（注意力与MLP）之间引入了一种自适应稀疏分配机制，以全球平衡的方式调整模块之间的剪枝强度。此外，为了减轻剪枝决策对校准数据质量的高度敏感性，我们提出了一种简单而有效的基于KL散度的校准数据选择策略，确保更可靠和任务无关的剪枝结果。在Llama3、Qwen和T5模型上进行的全面实验表明，NIRVANA在等效稀疏约束下优于现有的结构化剪枝方法，提供了一种理论上合理且实用的LLM压缩方法。代码可在https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA获取。","chinese_introduction":"中文介绍：研究背景和动机","chinese_challenges":"中文挑战：主要技术挑战","chinese_innovations":"中文创新：新方法和贡献","chinese_experiments":"中文实验：实验设置和结果","chinese_insights":"中文见解：领域意义和未来方向","summary":"**Introduction:** The paper addresses the pressing need for efficient compression techniques in large language models (LLMs) through structured pruning. Current methods often lead to performance degradation, especially in zero-shot scenarios, and require expensive recovery techniques like supervised fine-tuning. NIRVANA aims to overcome these issues by preserving zero-shot accuracy while enhancing fine-tuning capabilities.","analyzed_at":"2025-09-18T21:42:05.809Z","model":"openai/gpt-4o-mini"},"scraped_at":"2025-09-18T21:39:49.439Z","archive_metadata":{"archived_at":"2025-09-18T21:51:27.750Z","original_id":"arxiv_2509.14230v1"}},{"id":"arxiv_2509.14082v1","title":"FlightDiffusion: Revolutionising Autonomous Drone Training with\n  Diffusion Models Generating FPV Video","authors":["Valerii Serpiva","Artem Lykov","Faryal Batool","Vladislav Kozlovskiy","Miguel Altamirano Cabrera","Dzmitry Tsetserukou"],"abstract":"We present FlightDiffusion, a diffusion-model-based framework for training\nautonomous drones from first-person view (FPV) video. Our model generates\nrealistic video sequences from a single frame, enriched with corresponding\naction spaces to enable reasoning-driven navigation in dynamic environments.\nBeyond direct policy learning, FlightDiffusion leverages its generative\ncapabilities to synthesize diverse FPV trajectories and state-action pairs,\nfacilitating the creation of large-scale training datasets without the high\ncost of real-world data collection. Our evaluation demonstrates that the\ngenerated trajectories are physically plausible and executable, with a mean\nposition error of 0.25 m (RMSE 0.28 m) and a mean orientation error of 0.19 rad\n(RMSE 0.24 rad). This approach enables improved policy learning and dataset\nscalability, leading to superior performance in downstream navigation tasks.\nResults in simulated environments highlight enhanced robustness, smoother\ntrajectory planning, and adaptability to unseen conditions. An ANOVA revealed\nno statistically significant difference between performance in simulation and\nreality (F(1, 16) = 0.394, p = 0.541), with success rates of M = 0.628 (SD =\n0.162) and M = 0.617 (SD = 0.177), respectively, indicating strong sim-to-real\ntransfer. The generated datasets provide a valuable resource for future UAV\nresearch. This work introduces diffusion-based reasoning as a promising\nparadigm for unifying navigation, action generation, and data synthesis in\naerial robotics.","published":"2025-09-17T15:28:09Z","source":"arxiv","url":"http://arxiv.org/abs/2509.14082v1","analysis":{"introduction":"The paper introduces FlightDiffusion, a novel framework that utilizes diffusion models to train autonomous drones using first-person view (FPV) video. The motivation stems from the need for efficient training methods that can generate realistic video sequences and corresponding action spaces, addressing the challenges of real-world data collection in dynamic environments.","challenges":"Key technical challenges include generating physically plausible FPV video sequences from a single frame and ensuring that the synthesized trajectories are executable in real-world scenarios. Existing approaches often rely heavily on expensive real-world data collection, which limits scalability and adaptability.","innovations":"FlightDiffusion presents a unique approach by integrating diffusion models for generating FPV video, allowing for the synthesis of diverse trajectories and state-action pairs. This innovation not only enhances policy learning but also significantly reduces the reliance on real-world data. The framework's ability to unify navigation, action generation, and data synthesis marks a substantial theoretical advancement in aerial robotics.","experiments":"The experimental setup involved evaluating the generated trajectories in both simulated and real-world environments. Key metrics included mean position error (0.25 m) and mean orientation error (0.19 rad), demonstrating the physical plausibility of the generated data. Comparisons with baseline methods showed no statistically significant performance difference between simulation and reality, indicating strong sim-to-real transfer capabilities.","insights":"FlightDiffusion has significant implications for the field of UAV research, particularly in enhancing the scalability of training datasets and improving policy learning. Future applications could extend to various autonomous systems beyond drones. Future research directions may explore further refinements in generative modeling and the integration of more complex environmental interactions.","keywords":["FlightDiffusion","autonomous drones","diffusion models","FPV video","trajectory generation","policy learning","sim-to-real transfer","dataset synthesis"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"我们提出了FlightDiffusion，这是一个基于扩散模型的框架，用于从第一人称视角（FPV）视频训练自主无人机。我们的模型从单帧生成逼真的视频序列，并丰富相应的动作空间，以便在动态环境中进行推理驱动的导航。除了直接的策略学习，FlightDiffusion还利用其生成能力合成多样的FPV轨迹和状态-动作对，从而在不需要高成本的现实世界数据收集的情况下，促进大规模训练数据集的创建。我们的评估表明，生成的轨迹在物理上是合理且可执行的，平均位置误差为0.25米（RMSE 0.28米），平均方向误差为0.19弧度（RMSE 0.24弧度）。这种方法提高了策略学习和数据集的可扩展性，导致下游导航任务的性能优越。在模拟环境中的结果突出了增强的鲁棒性、更平滑的轨迹规划和对未见条件的适应性。ANOVA分析显示，模拟与现实之间的性能没有统计学显著差异（F(1, 16) = 0.394，p = 0.541），成功率分别为M = 0.628（SD = 0.162）和M = 0.617（SD = 0.177），表明强大的模拟到现实转移。生成的数据集为未来的无人机研究提供了宝贵的资源。这项工作引入了基于扩散的推理作为统一导航、动作生成和数据合成在空中机器人中的有前景的范式。","chinese_introduction":"中文介绍：研究背景和动机","chinese_challenges":"中文挑战：主要技术挑战","chinese_innovations":"中文创新：新方法和贡献","chinese_experiments":"中文实验：实验设置和结果","chinese_insights":"中文见解：领域意义和未来方向","summary":"**Introduction:** The paper introduces FlightDiffusion, a novel framework that utilizes diffusion models to train autonomous drones using first-person view (FPV) video. The motivation stems from the need for efficient training methods that can generate realistic video sequences and corresponding action spaces, addressing the challenges of real-world data collection in dynamic environments.","analyzed_at":"2025-09-18T21:50:19.906Z","model":"openai/gpt-4o-mini"},"scraped_at":"2025-09-18T21:39:49.439Z","archive_metadata":{"archived_at":"2025-09-18T21:51:27.750Z","original_id":"arxiv_2509.14082v1"}},{"id":"arxiv_2509.14119v1","title":"Generative AI for Misalignment-Resistant Virtual Staining to Accelerate\n  Histopathology Workflows","authors":["Jiabo MA","Wenqiang Li","Jinbang Li","Ziyi Liu","Linshan Wu","Fengtao Zhou","Li Liang","Ronald Cheong Kin Chan","Terence T. W. Wong","Hao Chen"],"abstract":"Accurate histopathological diagnosis often requires multiple differently\nstained tissue sections, a process that is time-consuming, labor-intensive, and\nenvironmentally taxing due to the use of multiple chemical stains. Recently,\nvirtual staining has emerged as a promising alternative that is faster,\ntissue-conserving, and environmentally friendly. However, existing virtual\nstaining methods face significant challenges in clinical applications,\nprimarily due to their reliance on well-aligned paired data. Obtaining such\ndata is inherently difficult because chemical staining processes can distort\ntissue structures, and a single tissue section cannot undergo multiple staining\nprocedures without damage or loss of information. As a result, most available\nvirtual staining datasets are either unpaired or roughly paired, making it\ndifficult for existing methods to achieve accurate pixel-level supervision. To\naddress this challenge, we propose a robust virtual staining framework\nfeaturing cascaded registration mechanisms to resolve spatial mismatches\nbetween generated outputs and their corresponding ground truth. Experimental\nresults demonstrate that our method significantly outperforms state-of-the-art\nmodels across five datasets, achieving an average improvement of 3.2% on\ninternal datasets and 10.1% on external datasets. Moreover, in datasets with\nsubstantial misalignment, our approach achieves a remarkable 23.8% improvement\nin peak signal-to-noise ratio compared to baseline models. The exceptional\nrobustness of the proposed method across diverse datasets simplifies the data\nacquisition process for virtual staining and offers new insights for advancing\nits development.","published":"2025-09-17T15:58:59Z","source":"arxiv","url":"http://arxiv.org/abs/2509.14119v1","analysis":{"introduction":"The paper addresses the challenges of histopathological diagnosis, which traditionally requires multiple stained tissue sections, a method that is both time-consuming and environmentally taxing. Virtual staining has emerged as a promising alternative, yet existing methods struggle due to the need for well-aligned paired data, which is difficult to obtain due to the distortions caused by chemical staining processes.","challenges":"The primary technical challenges include the reliance on accurately paired data for training virtual staining models, which is often unavailable due to the inherent distortions in tissue structures during chemical staining. Existing virtual staining methods are limited by their inability to handle unpaired or roughly paired datasets effectively, leading to inaccuracies in pixel-level supervision.","innovations":"The authors propose a novel virtual staining framework that incorporates cascaded registration mechanisms to address spatial mismatches between generated outputs and their corresponding ground truth. This innovative approach enhances the robustness of virtual staining across diverse datasets. The framework significantly improves the accuracy of virtual staining by achieving better alignment between the generated images and actual stained sections, thus offering a practical solution to the limitations of existing methods.","experiments":"The experimental setup involved testing the proposed method against state-of-the-art models across five datasets, both internal and external. Key metrics included peak signal-to-noise ratio (PSNR) and overall accuracy. The results indicated an average improvement of 3.2% on internal datasets and 10.1% on external datasets, with a remarkable 23.8% improvement in PSNR for datasets with significant misalignment, showcasing the effectiveness of the proposed approach.","insights":"The findings of this research have significant implications for the field of histopathology, as they simplify the data acquisition process for virtual staining and enhance the accuracy of diagnoses. Potential applications include faster and more efficient histopathological workflows. Future research could explore further enhancements in alignment techniques and the application of the proposed framework to other imaging modalities.","keywords":["virtual staining","histopathology","cascaded registration","pixel-level supervision","peak signal-to-noise ratio","machine learning","computer vision","data alignment"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"准确的组织病理学诊断通常需要多种不同染色的组织切片，这一过程耗时、劳动密集且由于使用多种化学染料而对环境造成负担。最近，虚拟染色作为一种更快、节省组织和环保的替代方案出现。然而，现有的虚拟染色方法在临床应用中面临重大挑战，主要是由于它们依赖于良好对齐的配对数据。获取这样的数据本质上是困难的，因为化学染色过程会扭曲组织结构，单个组织切片无法在不损坏或丢失信息的情况下进行多次染色。因此，现有的虚拟染色数据集大多是未配对或粗略配对的，这使得现有方法难以实现准确的像素级监督。为了解决这一挑战，我们提出了一种强大的虚拟染色框架，具有级联配准机制，以解决生成输出与相应真实值之间的空间不匹配。实验结果表明，我们的方法在五个数据集上显著优于最先进的模型，在内部数据集上平均提高了3.2%，在外部数据集上提高了10.1%。此外，在具有显著不对齐的数据集中，我们的方法相比基线模型在峰值信噪比上实现了显著的23.8%的提升。所提出方法在不同数据集上的卓越鲁棒性简化了虚拟染色的数据获取过程，并为其发展提供了新的见解。","chinese_introduction":"中文介绍：研究背景和动机","chinese_challenges":"中文挑战：主要技术挑战","chinese_innovations":"中文创新：新方法和贡献","chinese_experiments":"中文实验：实验设置和结果","chinese_insights":"中文见解：领域意义和未来方向","summary":"**Introduction:** The paper addresses the challenges of histopathological diagnosis, which traditionally requires multiple stained tissue sections, a method that is both time-consuming and environmentally taxing. Virtual staining has emerged as a promising alternative, yet existing methods struggle due to the need for well-aligned paired data, which is difficult to obtain due to the distortions caused by chemical staining processes.","analyzed_at":"2025-09-18T21:49:19.872Z","model":"openai/gpt-4o-mini"},"scraped_at":"2025-09-18T21:39:49.439Z","archive_metadata":{"archived_at":"2025-09-18T21:51:27.750Z","original_id":"arxiv_2509.14119v1"}},{"id":"arxiv_2509.14191v1","title":"MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for\n  High-Fidelity Mapping","authors":["Zhihao Cao","Hanyu Wu","Li Wa Tang","Zizhou Luo","Zihan Zhu","Wei Zhang","Marc Pollefeys","Martin R. Oswald"],"abstract":"Recent progress in dense SLAM has primarily targeted monocular setups, often\nat the expense of robustness and geometric coverage. We present MCGS-SLAM, the\nfirst purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting\n(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM\nfuses dense RGB inputs from multiple viewpoints into a unified, continuously\noptimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines\nposes and depths via dense photometric and geometric residuals, while a scale\nconsistency module enforces metric alignment across views using low-rank\npriors. The system supports RGB input and maintains real-time performance at\nlarge scale. Experiments on synthetic and real-world datasets show that\nMCGS-SLAM consistently yields accurate trajectories and photorealistic\nreconstructions, usually outperforming monocular baselines. Notably, the wide\nfield of view from multi-camera input enables reconstruction of side-view\nregions that monocular setups miss, critical for safe autonomous operation.\nThese results highlight the promise of multi-camera Gaussian Splatting SLAM for\nhigh-fidelity mapping in robotics and autonomous driving.","published":"2025-09-17T17:27:53Z","source":"arxiv","url":"http://arxiv.org/abs/2509.14191v1","analysis":{"introduction":"The paper addresses the limitations of existing dense SLAM systems, which predominantly focus on monocular setups, leading to issues in robustness and geometric coverage. The authors introduce MCGS-SLAM, a multi-camera SLAM framework that utilizes RGB inputs to create high-fidelity maps, thereby enhancing the performance and accuracy of SLAM in complex environments.","challenges":"Key technical challenges include the integration of dense RGB data from multiple cameras, ensuring real-time performance, and maintaining metric consistency across different views. Existing approaches often rely on sparse mapping or inertial data, which can compromise the quality of the reconstructed environment and the accuracy of trajectory estimation.","innovations":"MCGS-SLAM introduces a novel multi-camera bundle adjustment (MCBA) that jointly refines camera poses and depth estimates using dense photometric and geometric residuals. Additionally, it employs a scale consistency module that utilizes low-rank priors to enforce metric alignment across views. This framework is groundbreaking as it leverages Gaussian splatting for continuous optimization of a dense Gaussian map, significantly improving the fidelity of the mapping process compared to traditional methods.","experiments":"The experimental setup includes evaluations on both synthetic and real-world datasets, where MCGS-SLAM's performance is benchmarked against monocular SLAM systems. Key results indicate that MCGS-SLAM consistently achieves accurate trajectory estimation and photorealistic reconstructions, outperforming monocular baselines in terms of accuracy and coverage, particularly in areas that are typically missed by single-camera systems.","insights":"The findings underscore the potential of multi-camera systems in enhancing SLAM applications, particularly in robotics and autonomous driving, where comprehensive environmental understanding is crucial. Future research could explore the integration of additional sensor modalities and the scalability of the framework in more dynamic environments.","keywords":["multi-camera SLAM","Gaussian splatting","RGB input","bundle adjustment","photometric residuals","geometric residuals","real-time performance","autonomous driving"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"最近在密集SLAM方面的进展主要针对单目设置，往往以稳健性和几何覆盖率为代价。我们提出了MCGS-SLAM，这是第一个基于RGB的多摄像头SLAM系统，基于3D高斯点云（3DGS）。与依赖稀疏地图或惯性数据的先前方法不同，MCGS-SLAM将来自多个视角的密集RGB输入融合成一个统一的、持续优化的高斯地图。多摄像头束调整（MCBA）通过密集的光度和几何残差共同优化姿态和深度，而尺度一致性模块则利用低秩先验在视图之间强制执行度量对齐。该系统支持RGB输入，并在大规模下保持实时性能。在合成和真实世界数据集上的实验表明，MCGS-SLAM始终产生准确的轨迹和逼真的重建，通常优于单目基线。多摄像头输入的广阔视野使得重建单目设置错过的侧视区域成为可能，这对于安全的自主操作至关重要。这些结果突显了多摄像头高斯点云SLAM在机器人和自主驾驶中的高保真映射的潜力。","chinese_introduction":"中文介绍：研究背景和动机","chinese_challenges":"中文挑战：主要技术挑战","chinese_innovations":"中文创新：新方法和贡献","chinese_experiments":"中文实验：实验设置和结果","chinese_insights":"中文见解：领域意义和未来方向","summary":"**Introduction:** The paper addresses the limitations of existing dense SLAM systems, which predominantly focus on monocular setups, leading to issues in robustness and geometric coverage. The authors introduce MCGS-SLAM, a multi-camera SLAM framework that utilizes RGB inputs to create high-fidelity maps, thereby enhancing the performance and accuracy of SLAM in complex environments.","analyzed_at":"2025-09-18T21:45:02.093Z","model":"openai/gpt-4o-mini"},"scraped_at":"2025-09-18T21:39:49.439Z","archive_metadata":{"archived_at":"2025-09-18T21:51:27.750Z","original_id":"arxiv_2509.14191v1"}},{"id":"arxiv_2509.14233v1","title":"Apertus: Democratizing Open and Compliant LLMs for Global Language\n  Environments","authors":["Alejandro Hernández-Cano","Alexander Hägele","Allen Hao Huang","Angelika Romanou","Antoni-Joan Solergibert","Barna Pasztor","Bettina Messmer","Dhia Garbaya","Eduard Frank Ďurech","Ido Hakimi","Juan García Giraldo","Mete Ismayilzada","Negar Foroutan","Skander Moalla","Tiancheng Chen","Vinko Sabolčec","Yixuan Xu","Michael Aerni","Badr AlKhamissi","Ines Altemir Marinas","Mohammad Hossein Amani","Matin Ansaripour","Ilia Badanin","Harold Benoit","Emanuela Boros","Nicholas Browning","Fabian Bösch","Maximilian Böther","Niklas Canova","Camille Challier","Clement Charmillot","Jonathan Coles","Jan Deriu","Arnout Devos","Lukas Drescher","Daniil Dzenhaliou","Maud Ehrmann","Dongyang Fan","Simin Fan","Silin Gao","Miguel Gila","María Grandury","Diba Hashemi","Alexander Hoyle","Jiaming Jiang","Mark Klein","Andrei Kucharavy","Anastasiia Kucherenko","Frederike Lübeck","Roman Machacek","Theofilos Manitaras","Andreas Marfurt","Kyle Matoba","Simon Matrenok","Henrique Mendoncça","Fawzi Roberto Mohamed","Syrielle Montariol","Luca Mouchel","Sven Najem-Meyer","Jingwei Ni","Gennaro Oliva","Matteo Pagliardini","Elia Palme","Andrei Panferov","Léo Paoletti","Marco Passerini","Ivan Pavlov","Auguste Poiroux","Kaustubh Ponkshe","Nathan Ranchin","Javi Rando","Mathieu Sauser","Jakhongir Saydaliev","Muhammad Ali Sayfiddinov","Marian Schneider","Stefano Schuppli","Marco Scialanga","Andrei Semenov","Kumar Shridhar","Raghav Singhal","Anna Sotnikova","Alexander Sternfeld","Ayush Kumar Tarun","Paul Teiletche","Jannis Vamvas","Xiaozhe Yao","Hao Zhao Alexander Ilic","Ana Klimovic","Andreas Krause","Caglar Gulcehre","David Rosenthal","Elliott Ash","Florian Tramèr","Joost VandeVondele","Livio Veraldi","Martin Rajman","Thomas Schulthess","Torsten Hoefler","Antoine Bosselut","Martin Jaggi","Imanol Schlag"],"abstract":"We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.","published":"2025-09-17T17:59:21Z","source":"arxiv","url":"http://arxiv.org/abs/2509.14233v1","analysis":{"introduction":"The Apertus project aims to democratize access to large language models (LLMs) by addressing critical issues in the current open model ecosystem, particularly concerning data compliance and multilingual representation. The motivation stems from the need for models that respect content ownership rights and provide equitable representation across diverse languages, especially in non-English contexts.","challenges":"Key challenges include ensuring compliance with data usage rights while maintaining model performance and mitigating risks associated with data memorization. Existing models often overlook these aspects, leading to potential legal issues and biases in language representation, particularly for underrepresented languages.","innovations":"Apertus introduces several novel methods, including the Goldfish objective during pretraining, which effectively suppresses verbatim recall of training data while preserving performance on downstream tasks. The models are trained on a massive dataset of 15 trillion tokens from over 1800 languages, with a significant focus on non-English content. Additionally, the release of all scientific artifacts under a permissive license promotes transparency and reproducibility in the research community.","experiments":"The experimental setup involved training Apertus models at both 8B and 70B parameter scales, followed by rigorous evaluation on multilingual benchmarks. The results indicate that Apertus achieves state-of-the-art performance among fully open models, often surpassing existing open-weight counterparts. Metrics used for evaluation include accuracy and F1 scores across various language tasks, demonstrating the model's effectiveness in multilingual contexts.","insights":"Apertus has significant implications for the field of natural language processing, particularly in promoting ethical AI practices and enhancing multilingual capabilities. Potential applications include improved language translation services, content generation, and educational tools. Future research could explore further enhancements in data compliance and expanding the model's capabilities to additional languages and dialects.","keywords":["large language models","data compliance","multilingual representation","Goldfish objective","open-source","evaluation metrics","ethical AI","natural language processing"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"我们提出了Apertus，这是一个完全开放的大型语言模型(LLM)套件，旨在解决当前开放模型生态系统中的两个系统性缺陷：数据合规性和多语言表示。与许多先前的模型不同，Apertus模型仅在公开可用的数据上进行预训练，追溯性地尊重robots.txt排除条款，并过滤非许可、毒性和个人身份信息内容。为了减少记忆化的风险，我们在预训练期间采用了Goldfish目标，强烈抑制逐字回忆数据，同时保持下游任务性能。Apertus模型还扩大了多语言覆盖范围，基于来自1800多种语言的15T标记进行训练，约40%的预训练数据分配给非英语内容。以8B和70B规模发布，Apertus在多语言基准测试中接近或超过完全开放模型的最新成果。除了模型权重外，我们还以宽松许可证发布了开发周期中的所有科学文献，包括数据准备脚本、检查点、评估套件和训练代码，促进透明审计和扩展。","chinese_introduction":"中文介绍：Apertus项目旨在通过解决当前开放模型生态系统中的关键问题，特别是数据合规性和多语言表示，来实现大型语言模型(LLM)的民主化。其动机源于对尊重内容所有权和在多样语言中提供公平表示的模型的需求，尤其是在非英语背景下。","chinese_challenges":"中文挑战：主要挑战包括确保数据使用权的合规性，同时保持模型性能，并减轻与数据记忆化相关的风险。现有模型往往忽视这些方面，导致潜在的法律问题和语言表示中的偏见，特别是对于代表性不足的语言。","chinese_innovations":"中文创新：Apertus引入了几种新方法，包括在预训练期间采用的Goldfish目标，有效抑制逐字回忆训练数据，同时保持下游任务的性能。模型在来自1800多种语言的15万亿标记的大型数据集上进行训练，特别关注非英语内容。此外，所有科学文献以宽松许可证发布，促进研究社区的透明性和可重复性。","chinese_experiments":"中文实验：实验设置涉及在8B和70B参数规模下训练Apertus模型，随后在多语言基准上进行严格评估。结果表明，Apertus在完全开放模型中实现了最新的性能，通常超过现有的开放权重对手。评估中使用的指标包括各种语言任务的准确性和F1分数，展示了模型在多语言环境中的有效性。","chinese_insights":"中文见解：Apertus对自然语言处理领域具有重要意义，特别是在促进伦理AI实践和增强多语言能力方面。潜在应用包括改进语言翻译服务、内容生成和教育工具。未来的研究可以探索在数据合规性方面的进一步增强，以及将模型的能力扩展到更多语言和方言。","summary":"**Introduction:** The Apertus project aims to democratize access to large language models (LLMs) by addressing critical issues in the current open model ecosystem, particularly concerning data compliance and multilingual representation. The motivation stems from the need for models that respect content ownership rights and provide equitable representation across diverse languages, especially in non-English contexts.","analyzed_at":"2025-09-18T21:41:46.017Z","model":"openai/gpt-4o-mini"},"scraped_at":"2025-09-18T21:39:49.439Z","archive_metadata":{"archived_at":"2025-09-18T21:51:27.750Z","original_id":"arxiv_2509.14233v1"}},{"id":"arxiv_2509.14172v1","title":"TGPO: Tree-Guided Preference Optimization for Robust Web Agent\n  Reinforcement Learning","authors":["Ziyuan Chen","Zhenghui Zhao","Zhangye Han","Miancan Liu","Xianhang Ye","Yiqing Li","Hongbo Min","Jinkui Ren","Xiantao Zhang","Guitao Cao"],"abstract":"With the rapid advancement of large language models and vision-language\nmodels, employing large models as Web Agents has become essential for automated\nweb interaction. However, training Web Agents with reinforcement learning faces\ncritical challenges including credit assignment misallocation, prohibitively\nhigh annotation costs, and reward sparsity. To address these issues, we propose\nTree-Guided Preference Optimization (TGPO), an offline reinforcement learning\nframework that proposes a tree-structured trajectory representation merging\nsemantically identical states across trajectories to eliminate label conflicts.\nOur framework incorporates a Process Reward Model that automatically generates\nfine-grained rewards through subgoal progress, redundancy detection, and action\nverification. Additionally, a dynamic weighting mechanism prioritizes\nhigh-impact decision points during training. Experiments on Online-Mind2Web and\nour self-constructed C-WebShop datasets demonstrate that TGPO significantly\noutperforms existing methods, achieving higher success rates with fewer\nredundant steps.","published":"2025-09-17T16:58:44Z","source":"arxiv","url":"http://arxiv.org/abs/2509.14172v1","analysis":{"introduction":"The paper addresses the integration of large language models and vision-language models as Web Agents for automated web interactions. The motivation stems from the increasing necessity for efficient and robust web agents, particularly in the context of reinforcement learning. The authors highlight the challenges faced in training these agents, which include issues related to credit assignment, high annotation costs, and reward sparsity, necessitating innovative solutions to improve performance.","challenges":"The main technical challenges identified include credit assignment misallocation, which complicates the learning process, and the high costs associated with annotating data for training. Additionally, reward sparsity presents a significant barrier, making it difficult for agents to learn effectively from limited feedback. Existing approaches struggle to address these issues comprehensively, leading to inefficiencies in training and performance.","innovations":"The authors introduce Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning framework that employs a tree-structured trajectory representation to merge semantically identical states across different trajectories. This approach helps eliminate label conflicts. A key innovation is the Process Reward Model, which generates fine-grained rewards based on subgoal progress, redundancy detection, and action verification. Furthermore, a dynamic weighting mechanism is implemented to prioritize high-impact decision points during training, enhancing the learning process and overall agent performance.","experiments":"The experimental setup involved testing TGPO on two datasets: Online-Mind2Web and a self-constructed C-WebShop dataset. The evaluation metrics focused on success rates and the number of redundant steps taken by the agents. Results indicated that TGPO significantly outperformed existing methods, achieving higher success rates while minimizing redundant actions, thus demonstrating its effectiveness in improving the training of Web Agents in reinforcement learning scenarios.","insights":"The findings from this research have significant implications for the development of more efficient web agents, particularly in contexts requiring automated interactions. The innovations presented could be applied to various domains, including e-commerce and customer service automation. Future research directions may explore further enhancements to the reward mechanisms and the application of TGPO to other types of reinforcement learning tasks.","keywords":["Tree-Guided Preference Optimization","offline reinforcement learning","web agents","trajectory representation","reward model","dynamic weighting","Online-Mind2Web","C-WebShop"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"随着大型语言模型和视觉语言模型的快速发展，使用大型模型作为Web代理已成为自动化Web交互的必要条件。然而，使用强化学习训练Web代理面临着关键挑战，包括信用分配错误、过高的注释成本和稀疏奖励。为了解决这些问题，我们提出了树引导的偏好优化（TGPO），这是一种离线强化学习框架，提出了一种树结构的轨迹表示，合并跨轨迹的语义相同状态，以消除标签冲突。我们的框架结合了一个过程奖励模型，通过子目标进展、冗余检测和动作验证自动生成细粒度奖励。此外，动态加权机制在训练过程中优先考虑高影响决策点。在Online-Mind2Web和我们自构建的C-WebShop数据集上的实验表明，TGPO显著优于现有方法，在减少冗余步骤的同时实现更高的成功率。","chinese_introduction":"中文介绍：研究背景和动机","chinese_challenges":"中文挑战：主要技术挑战","chinese_innovations":"中文创新：新方法和贡献","chinese_experiments":"中文实验：实验设置和结果","chinese_insights":"中文见解：领域意义和未来方向","summary":"**Introduction:** The paper addresses the integration of large language models and vision-language models as Web Agents for automated web interactions. The motivation stems from the increasing necessity for efficient and robust web agents, particularly in the context of reinforcement learning. The authors highlight the challenges faced in training these agents, which include issues related to credit assignment, high annotation costs, and reward sparsity, necessita...","analyzed_at":"2025-09-18T21:45:26.439Z","model":"openai/gpt-4o-mini"},"scraped_at":"2025-09-18T21:39:49.439Z","archive_metadata":{"archived_at":"2025-09-18T21:51:27.750Z","original_id":"arxiv_2509.14172v1"}}],"metadata":{"total_papers":10,"categories":{"machine_learning":10},"sources":{"arxiv":10},"average_score":9,"unique_keywords":["movie understanding","question answering","dataset","narrative comprehension","vision-language models","temporal reasoning","GPT-4o","fine-grained reasoning","data-driven modeling","nonlinear dynamical systems","implicit neural representation","Runge-Kutta","total variation","derivative estimation","SINDy","noise suppression","Automatic Speech Recognition","Speech-to-Text Translation","FastConformer","Transformer","NeMo Forced Aligner","dynamic data balancing","multilingual models","nGPT","Zero-Shot Anomaly Detection","DINOv3","Anomaly-Aware Calibration","Multimodal Contrastive Learning","Vision-Language Models","Feature Misalignment","Industrial Benchmarks","Medical Benchmarks","structured pruning","large language models","compression","Neural Tangent Kernel","adaptive sparsity","KL divergence","zero-shot learning","fine-tuning","FlightDiffusion","autonomous drones","diffusion models","FPV video","trajectory generation","policy learning","sim-to-real transfer","dataset synthesis","virtual staining","histopathology"],"created_at":"2025-09-18T21:51:27.750Z","source":"daily_update","auto_archived":true,"papers_archived":10,"total_papers_analyzed":55}}