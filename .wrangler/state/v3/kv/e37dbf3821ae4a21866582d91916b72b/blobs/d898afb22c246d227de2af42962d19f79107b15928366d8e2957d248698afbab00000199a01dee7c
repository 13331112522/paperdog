[{"id":"arxiv_2509.26644v1","arxiv_id":"2509.26644v1","title":"Stitch: Training-Free Position Control in Multimodal Diffusion\n  Transformers","abstract":"Text-to-Image (T2I) generation models have advanced rapidly in recent years,\nbut accurately capturing spatial relationships like \"above\" or \"to the right\nof\" poses a persistent challenge. Earlier methods improved spatial relationship\nfollowing with external position control. However, as architectures evolved to\nenhance image quality, these techniques became incompatible with modern models.\nWe propose Stitch, a training-free method for incorporating external position\ncontrol into Multi-Modal Diffusion Transformers (MMDiT) via\nautomatically-generated bounding boxes. Stitch produces images that are both\nspatially accurate and visually appealing by generating individual objects\nwithin designated bounding boxes and seamlessly stitching them together. We\nfind that targeted attention heads capture the information necessary to isolate\nand cut out individual objects mid-generation, without needing to fully\ncomplete the image. We evaluate Stitch on PosEval, our benchmark for\nposition-based T2I generation. Featuring five new tasks that extend the concept\nof Position beyond the basic GenEval task, PosEval demonstrates that even top\nmodels still have significant room for improvement in position-based\ngeneration. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances\nbase models, even improving FLUX by 218% on GenEval's Position task and by 206%\non PosEval. Stitch achieves state-of-the-art results with Qwen-Image on\nPosEval, improving over previous models by 54%, all accomplished while\nintegrating position control into leading models training-free. Code is\navailable at https://github.com/ExplainableML/Stitch.","authors":["Jessica Bader","Mateusz Pach","Maria A. Bravo","Serge Belongie","Zeynep Akata"],"published":"2025-09-30T17:59:51Z","updated":"2025-09-30T17:59:51Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26644v1","pdf_url":"http://arxiv.org/pdf/2509.26644v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 Struggling to get T2I models to place “cat to the right of the vase” accurately?\nStitch is a training-free method that adds external position control to Multimodal Diffusion Transformers via auto-generated bounding boxes — producing spatially accurate, high-quality images.\nWho benefits: researchers and apps needing precise layout control.","challenges":"🎯 Key problems tackled:\n- Existing T2I models often fail to capture spatial relations like “above” or “to the right of”.\n- Prior position-control methods are incompatible with modern model architectures.\n- No training-free way to add position control to leading Multimodal Diffusion Transformers (MMDiT).","innovations":"✨ Core ideas:\n- Stitch: a training-free pipeline that injects external position control into MMDiT using automatically-generated bounding boxes.\n- Uses targeted attention heads to isolate and cut out individual objects mid-generation.\n- Seamlessly stitches object crops back to form final, spatially-accurate images.\nNovelty: training-free integration into modern MMDiT and leveraging attention heads for mid-generation object isolation.","experiments":"📊 Results & proof:\n- Biggest win: +218% improvement (FLUX on GenEval Position task).\n- Also: +206% on PosEval and Stitch achieves SOTA with Qwen-Image on PosEval (+54% over prior models).\nThis proves training-free position control substantially improves spatial accuracy in top models.","insights":"🤔 What's next?\n- Research directions: explore extending Stitch to video/dynamic layouts or interactive user-guided box editing during generation.\n- Applications: layout-aware design tools, scene planning for robotics, and guided content creation.\nCould this make T2I reliably layout-aware across real apps?","category":"computer_vision","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"chinese_translation\": \"🚀 还在为 T2I 模型无法精确地将“猫放在花瓶右边”而苦恼吗？\\n\\nStitch 是一种无需训练的方法，它通过自动生成的边界框，为多模态扩散 Transformer 增加了外部位置控制，从而生成空间精确、高质量的图像。\\n\\n受益者：需要精确布局控制的研究人员和应用程序开发者。\"\n}","chinese_challenges":"[\n  \"🎯 解决的关键问题：\",\n  \"- 现有的T2I模型通常难以准确捕捉“在...上方”或“在...右侧”等空间关系。\",\n  \"- 以往的位置控制方法与现代模型架构不兼容。\",\n  \"- 缺乏无需训练即可为领先的多模态扩散Transformer (MMDiT) 添加位置控制的方法。\"\n]","chinese_innovations":"{\n  \"translation\": \"✨ 核心思想：\\n- Stitch：一个无需训练的流程，它利用自动生成的边界框将外部位置控制注入到 MMDiT 中。\\n- 使用定向注意力头来隔离并“剪切”出生成过程中的单个对象。\\n- 将对象裁剪无缝地拼接回去，形成最终在空间上准确的图像。\\n新颖性：无需训练即可集成到现代 MMDiT 架构中，并利用注意力头实现生成过程中的对象隔离。\"\n}","chinese_experiments":"{\n  \"chinese_translation\": \"📊 结果与证明：\\n- 最大突破：在 GenEval 位置任务上，FLUX 实现了 +218% 的提升。\\n- 此外：在 PosEval 上实现了 +206% 的提升，并且 Stitch 模型在 PosEval 上使用 Qwen-Image 达到了 SOTA（比现有模型提升了 +54%）。\\n这证明了免训练的位置控制显著提高了顶级模型的空间准确性。\"\n}","chinese_insights":"{\n  \"translation\": \"🤔 下一步是什么？\\n- 研究方向：探索将 Stitch 扩展到视频/动态布局，或在生成过程中实现交互式用户引导的框编辑功能。\\n- 应用：布局感知设计工具、机器人场景规划以及引导式内容创作。\\n这能否使 T2I（文本到图像）模型在实际应用中可靠地具备布局感知能力？\"\n}","summary":"**Introduction:** 🚀 Struggling to get T2I models to place “cat to the right of the vase” accurately?\nStitch is a training-free method that adds external position control to Multimodal Diffusion Transformers via auto-generated bounding boxes — producing spatially accurate, high-quality images.\nWho benefits: researchers and apps needing precise layout control.\n\n**Challenges:** 🎯 Key problems tackled:\n- Existing T2I models often fail...","analyzed_at":"2025-10-01T09:32:25.791Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26645v1","arxiv_id":"2509.26645v1","title":"TTT3R: 3D Reconstruction as Test-Time Training","abstract":"Modern Recurrent Neural Networks have become a competitive architecture for\n3D reconstruction due to their linear-time complexity. However, their\nperformance degrades significantly when applied beyond the training context\nlength, revealing limited length generalization. In this work, we revisit the\n3D reconstruction foundation models from a Test-Time Training perspective,\nframing their designs as an online learning problem. Building on this\nperspective, we leverage the alignment confidence between the memory state and\nincoming observations to derive a closed-form learning rate for memory updates,\nto balance between retaining historical information and adapting to new\nobservations. This training-free intervention, termed TTT3R, substantially\nimproves length generalization, achieving a $2\\times$ improvement in global\npose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU\nmemory to process thousands of images. Code available in\nhttps://rover-xingyu.github.io/TTT3R","authors":["Xingyu Chen","Yue Chen","Yuliang Xiu","Andreas Geiger","Anpei Chen"],"published":"2025-09-30T17:59:51Z","updated":"2025-09-30T17:59:51Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26645v1","pdf_url":"http://arxiv.org/pdf/2509.26645v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 Ever tried a 3D recon model that works in training but collapses on longer sequences? TTT3R treats recurrent 3D reconstruction as online Test-Time Training: a training-free tweak that adapts memory updates on the fly, boosting length generalization for mapping & robotics.","challenges":"🎯 Problems tackled:\n- Existing recurrent 3D reconstruction models fail when applied beyond training context length (limited length generalization).\n- Balancing retention of historical memory vs. adapting to new observations is hard.\n- Need for efficient, scalable processing of thousands of images on limited GPU RAM.","innovations":"✨ Core ideas:\n- Reframe recurrent 3D reconstruction as an online Test-Time Training problem.\n- Derive a closed-form learning rate from alignment confidence between memory state and incoming observations.\n- Training-free intervention (TTT3R) that adjusts memory updates to balance past vs new info, enabling realtime, low-memory operation.","experiments":"📊 Key result: TTT3R yields a 2× improvement in global pose estimation over baselines, demonstrating substantially improved length generalization — while running at ~20 FPS and using only ~6 GB GPU to process thousands of images.","insights":"🤔 Next steps & applications:\n- Explore combining TTT3R with learned adaptation or other architectures (e.g., Transformers) to extend generalization.\n- Apply to long-horizon SLAM, autonomous mapping, and edge robotics where memory/compute are limited.\nCould test adaptive schemes for dynamic scenes or multi-agent mapping.","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"title\": \"TTT3R: 在线测试时训练提升循环3D重建的长度泛化能力\",\n  \"introduction\": \"🚀 您是否曾遇到过这样的3D重建模型：它在训练阶段表现出色，但在处理较长序列时却会失效（或崩溃）？TTT3R将循环3D重建视为一种在线测试时训练（online Test-Time Training）方法：这是一种免训练的调整手段，能够实时地自适应内存更新，从而显著提升了模型在建图与机器人技术应用中的长度泛化能力。\",\n  \"technical_terms\": [\n    \"3D重建模型 (3D recon model)\",\n    \"长度泛化 (length generalization)\",\n    \"循环3D重建 (recurrent 3D reconstruction)\",\n    \"在线测试时训练 (online Test-Time Training)\",\n    \"免训练的调整 (training-free tweak)\",\n    \"内存更新 (memory updates)\",\n    \"建图与机器人技术 (mapping & robotics)\"\n  ]\n}","chinese_challenges":"[\n  \"现有的循环（Recurrent）3D重建模型在应用于超出训练上下文长度的序列时会失效（长度泛化能力受限）。\",\n  \"难以平衡历史记忆的保留与对新观测数据的适应。\",\n  \"需要在有限的GPU显存下，对数千张图像进行高效且可扩展的处理。\"\n]","chinese_innovations":"{\n  \"innovations\": [\n    {\n      \"core_ideas\": \"核心思想\"\n    },\n    {\n      \"point_1\": \"将循环三维重建（recurrent 3D reconstruction）重新定义为一种在线测试时训练（Test-Time Training, TTT）问题。\"\n    },\n    {\n      \"point_2\": \"根据记忆状态（memory state）与新传入观测数据之间的对齐置信度，推导出一个闭式（closed-form）学习率。\"\n    },\n    {\n      \"point_3\": \"引入免训练干预机制（TTT3R），该机制用于调整记忆更新过程，以平衡历史信息与新传入信息，从而实现实时、低内存的运行。\"\n    }\n  ]\n}","chinese_experiments":"{\n  \"translation\": \"关键结果：TTT3R 在全局姿态估计方面实现了比基线方法高 2 倍的性能提升，同时显著增强了长度泛化能力。此外，其运行速度约为 20 帧/秒（FPS），且仅占用约 6 GB 显存即可处理数千张图像。\"\n}","chinese_insights":"{\n  \"translation\": {\n    \"title\": \"🤔 后续步骤与应用\",\n    \"steps\": [\n      \"探索将 TTT3R 与习得适应（learned adaptation）或其他架构（例如 Transformer）结合，以扩展其泛化能力。\",\n      \"应用于内存/计算资源受限的长程 SLAM、自主建图和边缘机器人领域。\",\n      \"可以针对动态场景或多智能体建图测试自适应方案（adaptive schemes）。\"\n    ]\n  }\n}","summary":"**Introduction:** 🚀 Ever tried a 3D recon model that works in training but collapses on longer sequences? TTT3R treats recurrent 3D reconstruction as online Test-Time Training: a training-free tweak that adapts memory updates on the fly, boosting length generalization for mapping & robotics.\n\n**Challenges:** 🎯 Problems tackled:\n- Existing recurrent 3D reconstruction models fail when applied beyond training context length (limited ...","analyzed_at":"2025-10-01T09:37:58.176Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26642v1","arxiv_id":"2509.26642v1","title":"MLA: A Multisensory Language-Action Model for Multimodal Understanding\n  and Forecasting in Robotic Manipulation","abstract":"Vision-language-action models (VLAs) have shown generalization capabilities\nin robotic manipulation tasks by inheriting from vision-language models (VLMs)\nand learning action generation. Most VLA models focus on interpreting vision\nand language to generate actions, whereas robots must perceive and interact\nwithin the spatial-physical world. This gap highlights the need for a\ncomprehensive understanding of robotic-specific multisensory information, which\nis crucial for achieving complex and contact-rich control. To this end, we\nintroduce a multisensory language-action (MLA) model that collaboratively\nperceives heterogeneous sensory modalities and predicts future multisensory\nobjectives to facilitate physical world modeling. Specifically, to enhance\nperceptual representations, we propose an encoder-free multimodal alignment\nscheme that innovatively repurposes the large language model itself as a\nperception module, directly interpreting multimodal cues by aligning 2D images,\n3D point clouds, and tactile tokens through positional correspondence. To\nfurther enhance MLA's understanding of physical dynamics, we design a future\nmultisensory generation post-training strategy that enables MLA to reason about\nsemantic, geometric, and interaction information, providing more robust\nconditions for action generation. For evaluation, the MLA model outperforms the\nprevious state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex,\ncontact-rich real-world tasks, respectively, while also demonstrating improved\ngeneralization to unseen configurations. Project website:\nhttps://sites.google.com/view/open-mla","authors":["Zhuoyang Liu","Jiaming Liu","Jiadong Xu","Nuowei Han","Chenyang Gu","Hao Chen","Kaichen Zhou","Renrui Zhang","Kai Chin Hsieh","Kun Wu","Zhengping Che","Jian Tang","Shanghang Zhang"],"published":"2025-09-30T17:59:50Z","updated":"2025-09-30T17:59:50Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26642v1","pdf_url":"http://arxiv.org/pdf/2509.26642v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 Question: Can robots truly feel and predict the physical world, not just see and read it?\nMLA introduces a multisensory language-action model that fuses vision, 3D, and tactile cues and predicts future multisensory objectives to unlock contact-rich robotic manipulation.","challenges":"🎯 Challenges:\n- Existing VLAs mainly interpret vision+language and ignore rich physical sensory signals.\n- Difficulty modeling contact-rich dynamics and multisensory temporal objectives.\n- Poor generalization to unseen spatial configurations in real-world manipulation.","innovations":"✨ Innovations:\n- Encoder-free multimodal alignment: repurposes a large language model as a perception module to align 2D images, 3D point clouds, and tactile tokens via positional correspondence.\n- Future multisensory generation post-training: trains the model to predict future multisensory objectives (semantic, geometric, interaction) to support action generation.","experiments":"📊 Experiment:\nMLA outperforms previous SOTA: +12% over 2D VLA and +24% over 3D VLA on complex, contact-rich real-world tasks — showing stronger task performance and better generalization to unseen configurations.","insights":"🤔 Insights (what's next?):\n- Research directions: integrate proprioception/force sensing and closed-loop real-time control; explore online adaptation and sim-to-real fine-tuning for safety.\n- Applications: dexterous assembly, surgical robotics, assistive manipulation. Could multisensory LLM perception become a new standard for embodied agents?","category":"robotics","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"\"🚀 问题：机器人能否真正感知并预测物理世界，而不仅仅是观察和识别它？MLA 引入了一个多感官语言-动作模型，它融合了视觉、3D 和触觉线索，并预测未来的多感官目标，从而实现接触密集型机器人操作。\"","chinese_challenges":"[\n  {\n    \"挑战\": [\n      \"现有的视觉-语言-动作模型（VLAs）主要侧重于解释视觉和语言信息，而忽略了丰富的物理感知信号。\",\n      \"难以对接触密集型动力学和多感官时间目标进行有效建模。\",\n      \"在现实世界操作中，对未见过的空间配置泛化能力较弱。\"\n    ]\n  }\n]","chinese_innovations":"[\n  {\n    \"innovations\": \"Encoder-free multimodal alignment: repurposes a large language model as a perception module to align 2D images, 3D point clouds, and tactile tokens via positional correspondence.\"\n  },\n  {\n    \"innovations\": \"Future multisensory generation post-training: trains the model to predict future multisensory objectives (semantic, geometric, interaction) to support action generation.\"\n  }\n]","chinese_experiments":"{\n\"translation\": \"📊 实验：MLA在复杂的、富接触的真实世界任务中，相比2D VLA性能提升了12%，相比3D VLA性能提升了24%——这表明其任务性能更强，并且对未见过的配置具有更好的泛化能力。\"\n}","chinese_insights":"{\n  \"translation\": \"🤔 洞察（下一步是什么？）：\\n- 研究方向：整合本体感觉/力传感和闭环实时控制；探索在线适应和用于安全的从模拟到现实的微调（sim-to-real fine-tuning）。\\n- 应用：灵巧装配、外科手术机器人、辅助操作。多感官大型语言模型（LLM）感知能否成为具身智能体（embodied agents）的新标准？\"\n}","summary":"**Introduction:** 🚀 Question: Can robots truly feel and predict the physical world, not just see and read it?\nMLA introduces a multisensory language-action model that fuses vision, 3D, and tactile cues and predicts future multisensory objectives to unlock contact-rich robotic manipulation.\n\n**Challenges:** 🎯 Challenges:\n- Existing VLAs mainly interpret vision+language and ignore rich physical sensory signals.\n- Difficulty modeling...","analyzed_at":"2025-10-01T10:08:50.959Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26643v1","arxiv_id":"2509.26643v1","title":"Convergence and Divergence of Language Models under Different Random\n  Seeds","abstract":"In this paper, we investigate the convergence of language models (LMs)\ntrained under different random seeds, measuring convergence as the expected\nper-token Kullback--Leibler (KL) divergence across seeds. By comparing LM\nconvergence as a function of model size and training checkpoint, we identify a\nfour-phase convergence pattern: (i) an initial uniform phase, (ii) a\nsharp-convergence phase, (iii) a sharp-divergence phase, and (iv) a\nslow-reconvergence phase. Further, we observe that larger models reconverge\nfaster in later training stages, while smaller models never actually\nreconverge; these results suggest that a certain model size may be necessary to\nlearn stable distributions. Restricting our analysis to specific token\nfrequencies or part-of-speech (PoS) tags further reveals that convergence is\nuneven across linguistic categories: frequent tokens and function words\nconverge faster and more reliably than their counterparts (infrequent tokens\nand content words). Overall, our findings highlight factors that influence the\nstability of the learned distributions in model training.","authors":["Finlay Fehlauer","Kyle Mahowald","Tiago Pimentel"],"published":"2025-09-30T17:59:50Z","updated":"2025-09-30T17:59:50Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26643v1","pdf_url":"http://arxiv.org/pdf/2509.26643v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 Ever wondered if LMs trained with different random seeds learn the same distribution?\nThis paper measures expected per-token KL divergence across seeds and finds a 4-phase convergence pattern (uniform → sharp-convergence → sharp-divergence → slow-reconvergence).\nLarger models reconverge faster — key for reproducibility and stability.","challenges":"🎯 Problems tackled:\n- Variability across random seeds undermines reproducibility of LM training.\n- Unclear how model size and training checkpoint affect learned distributions.\n- Convergence is uneven across token frequency and part-of-speech, complicating linguistic reliability.","innovations":"✨ Core methods / novelty:\n- Measure convergence as the expected per-token KL divergence across random seeds.\n- Trace convergence over checkpoints and model sizes to reveal a 4-phase pattern.\n- Analyze convergence by token frequency and PoS tags to expose uneven linguistic stability.\nNovel twist: systematic seed-to-seed distribution comparison that links phase behavior to model size and linguistic categories.","experiments":"📊 Main experimental takeaway:\nThe study demonstrates a four-phase convergence pattern and shows larger models reconverge faster in late training while smaller models may never reconverge; frequent tokens/function words converge faster than rare/content words.\nQuantitative specifics (numbers/percent improvements): Not specified in the paper.","insights":"🤔 What's next?\n- Investigate mechanistic causes of the four training phases and why scale aids reconvergence.\n- Explore interventions (regularization, loss adjustments, checkpoint ensembling) to force stable reconvergence.\nApplications: stability-aware model selection, more reproducible benchmarks, and robustness-aware deployment. Could scale-aware training protocols improve reproducibility?","category":"natural_language_processing","relevance_score":8,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"translation\": \"🚀 您是否曾好奇，用不同随机种子训练的语言模型（LM）是否会学习到相同的分布？\\n\\n本文测量了跨种子的预期每词元（per-token）KL散度，并发现了一种四阶段的收敛模式（均匀分布 → 快速收敛 → 快速发散 → 缓慢再收敛）。\\n\\n更大的模型能更快地再收敛——这对于可复现性和稳定性至关重要。\"\n}","chinese_challenges":"{\n  \"挑战\": [\n    {\n      \"目标问题\": \"随机种子之间的差异性破坏了语言模型训练的可重现性。\"\n    },\n    {\n      \"目标问题\": \"模型大小和训练检查点如何影响学习到的分布尚不明确。\"\n    },\n    {\n      \"目标问题\": \"收敛性在不同词元频率和词性上不均匀，使语言可靠性复杂化。\"\n    }\n  ]\n}","chinese_innovations":"{\n  \"核心方法 / 新颖性\": [\n    \"将收敛性度量为跨随机种子的、每词元的期望KL散度。\",\n    \"追踪收敛性在不同检查点和模型尺寸上的变化，揭示出四阶段模式。\",\n    \"通过词元频率和词性标签（PoS tags）分析收敛性，从而揭示出不均匀的语言学稳定性。\"\n  ],\n  \"创新之处\": \"系统的种子间分布比较，将阶段行为与模型尺寸和语言学类别联系起来。\"\n}","chinese_experiments":"{\n  \"experiments\": {\n    \"main_takeaway\": \"📊 主要实验结论：该研究展示了一种四阶段的收敛模式，并表明大型模型在训练后期能更快地重新收敛，而小型模型可能永远不会重新收敛；频繁出现的标记/功能词比稀有的/内容词收敛得更快。\",\n    \"quantitative_specifics\": \"定量细节（数字/百分比改进）：论文中未具体说明。\"\n  }\n}","chinese_insights":"{\n  \"insights\": [\n    {\n      \"category\": \"Future Research Directions\",\n      \"heading\": \"🤔 What's next?\",\n      \"points\": [\n        \"Investigate mechanistic causes of the four training phases and why scale aids reconvergence.\",\n        \"Explore interventions (regularization, loss adjustments, checkpoint ensembling) to force stable reconvergence.\"\n      ]\n    },\n    {\n      \"category\": \"Potential Applications\",\n      \"heading\": \"Applications:\",\n      \"points\": [\n        \"stability-aware model selection\",\n        \"more reproducible benchmarks\",\n        \"robustness-aware deployment\"\n      ]\n    },\n    {\n      \"category\": \"Specific Research Question\",\n      \"heading\": \"Question:\",\n      \"points\": [\n        \"Could scale-aware training protocols improve reproducibility?\"\n      ]\n    }\n  ]\n}","summary":"**Introduction:** 🚀 Ever wondered if LMs trained with different random seeds learn the same distribution?\nThis paper measures expected per-token KL divergence across seeds and finds a 4-phase convergence pattern (uniform → sharp-convergence → sharp-divergence → slow-reconvergence).\nLarger models reconverge faster — key for reproducibility and stability.\n\n**Challenges:** 🎯 Problems tackled:\n- Variability across random seeds undermi...","analyzed_at":"2025-10-01T10:43:34.786Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26641v1","arxiv_id":"2509.26641v1","title":"Query-Kontext: An Unified Multimodal Model for Image Generation and\n  Editing","abstract":"Unified Multimodal Models (UMMs) have demonstrated remarkable performance in\ntext-to-image generation (T2I) and editing (TI2I), whether instantiated as\nassembled unified frameworks which couple powerful vision-language model (VLM)\nwith diffusion-based generator, or as naive Unified Multimodal Models with an\nearly fusion of understanding and generation modalities. We contend that in\ncurrent unified frameworks, the crucial capability of multimodal generative\nreasoning which encompasses instruction understanding, grounding, and image\nreferring for identity preservation and faithful reconstruction, is\nintrinsically entangled with high-fidelity synthesis. In this work, we\nintroduce Query-Kontext, a novel approach that bridges the VLM and diffusion\nmodel via a multimodal ``kontext'' composed of semantic cues and coarse-grained\nimage conditions encoded from multimodal inputs. This design delegates the\ncomplex ability of multimodal generative reasoning to powerful VLM while\nreserving diffusion model's role for high-quality visual synthesis. To achieve\nthis, we propose a three-stage progressive training strategy. First, we connect\nthe VLM to a lightweight diffusion head via multimodal kontext tokens to\nunleash the VLM's generative reasoning ability. Second, we scale this head to a\nlarge, pre-trained diffusion model to enhance visual detail and realism.\nFinally, we introduce a low-level image encoder to improve image fidelity and\nperform instruction tuning on downstream tasks. Furthermore, we build a\ncomprehensive data pipeline integrating real, synthetic, and open-source\ndatasets, covering diverse multimodal reference-to-image scenarios, including\nimage generation, instruction-driven editing, customized generation, and\nmulti-subject composition. Experiments show that our approach matches strong\nunified baselines and even outperforms task-specific state-of-the-art methods\nin several cases.","authors":["Yuxin Song","Wenkai Dong","Shizun Wang","Qi Zhang","Song Xue","Tao Yuan","Hu Yang","Haocheng Feng","Hang Zhou","Xinyan Xiao","Jingdong Wang"],"published":"2025-09-30T17:59:46Z","updated":"2025-09-30T17:59:46Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26641v1","pdf_url":"http://arxiv.org/pdf/2509.26641v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 Can one model both understand complex multimodal instructions and produce high‑fidelity edits? \nQuery‑Kontext bridges a vision‑language model (VLM) and a diffusion generator via multimodal “kontext” tokens, offloading reasoning to the VLM and reserving synthesis for diffusion — improving T2I and instruction‑driven editing.","challenges":"🎯 Key problems tackled:\n- Existing unified frameworks entangle multimodal generative reasoning with high‑fidelity synthesis, hurting modularity.\n- Preserving identity and faithful reconstruction during instruction‑driven editing is hard.\n- Lack of a unified, diverse data pipeline for varied reference→image scenarios.","innovations":"✨ Core innovations:\n- Multimodal kontext: semantic cues + coarse image condition tokens that bridge VLM ↔ diffusion.\n- Three‑stage progressive training: lightweight diffusion head → scale to pretrained diffusion → add low‑level image encoder + instruction tuning.\n- Comprehensive data pipeline combining real, synthetic, and open datasets.\nNovel twist: explicit decoupling of generative reasoning (VLM) from high‑quality synthesis (diffusion) via kontext tokens.","experiments":"📊 Results: The approach matches strong unified baselines and even outperforms task‑specific state‑of‑the‑art methods in several cases. Exact numeric improvements or benchmark scores are not specified in the paper. \nMain proof: decoupling reasoning and synthesis via kontext tokens yields competitive or superior empirical performance.","insights":"🤔 Future directions & applications:\n- Explore spatially explicit kontext (per‑region tokens) or scene‑graph conditioning for finer control and compositionality.\n- Integrate multimodal chain‑of‑thought or stronger grounding for complex multi‑subject edits.\nPotential apps: personalized image editing, multi‑subject composition for content creation and AR/VR. Could this modular split speed up customization and safety checks?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"translation\": \"🚀 是否存在一个模型能够同时理解复杂的多模态指令并生成高保真度的编辑？Query-Kontext 通过多模态“kontext”令牌桥接了视觉-语言模型（VLM）和扩散生成器，将推理任务卸载给VLM，并将合成任务保留给扩散模型——从而改进了文本到图像（T2I）的生成和指令驱动的编辑。\"\n}","chinese_challenges":"[\n  {\n    \"name\": \"AlexNet\",\n    \"year\": 2012,\n    \"task\": \"Image Classification\",\n    \"key_innovation\": \"Deep CNN, ReLU activation, Dropout\",\n    \"impact_score\": 9.5\n  },\n  {\n    \"name\": \"VGGNet\",\n    \"year\": 2014,\n    \"task\": \"Image Classification, Localization\",\n    \"key_innovation\": \"3x3 convolution kernels, deep architecture\",\n    \"impact_score\": 8.8\n  },\n  {\n    \"name\": \"ResNet\",\n    \"year\": 2015,\n    \"task\": \"Image Classification, Object Detection\",\n    \"key_innovation\": \"Residual connections (skip connections), solving vanishing gradient\",\n    \"impact_score\": 9.8\n  },\n  {\n    \"name\": \"Transformer (Attention Is All You Need)\",\n    \"year\": 2017,\n    \"task\": \"Sequence Modeling (NLP, later Vision)\",\n    \"key_innovation\": \"Self-attention mechanism, eliminating recurrence\",\n    \"impact_score\": 10.0\n  },\n  {\n    \"name\": \"ViT (Vision Transformer)\",\n    \"year\": 2020,\n    \"task\": \"Image Classification\",\n    \"key_innovation\": \"Applying pure Transformer architecture directly to image patches\",\n    \"impact_score\": 9.2\n  },\n  {\n    \"name\": \"Diffusion Models (DDPM)\",\n    \"year\": 2020,\n    \"task\": \"Generative Modeling\",\n    \"key_innovation\": \"Iterative denoising process for high-quality image synthesis\",\n    \"impact_score\": 9.7\n  }\n]","chinese_innovations":"{\n  \"核心创新\": [\n    \"多模态上下文（Multimodal kontext）：语义线索 + 粗粒度图像条件令牌，用于连接视觉语言模型（VLM）与扩散模型（diffusion）。\",\n    \"三阶段渐进式训练：轻量级扩散头部 → 扩展到预训练扩散模型 → 添加低级图像编码器 + 指令微调。\",\n    \"结合真实、合成和开放数据集的综合数据管线。\",\n    \"新颖的转折点：通过上下文令牌，将生成推理（VLM）与高质量合成（扩散模型）明确解耦。\"\n  ]\n}","chinese_experiments":"{\n  \"chinese_translation\": \"📊 结果：该方法与强大的统一基线（unified baselines）相当，在某些情况下甚至超越了针对特定任务的最先进（state-of-the-art）方法。论文中没有具体说明精确的数字改进或基准分数。\\n主要证明：通过“kontext tokens”解耦推理（reasoning）和合成（synthesis），可以获得具有竞争力或更优越的经验性能。\"\n}","chinese_insights":"{\n  \"insights\": [\n    {\n      \"key\": \"Future directions & applications\",\n      \"value\": \"探索空间显式上下文（每区域标记）或场景图条件，以实现更精细的控制和组合性。\"\n    },\n    {\n      \"key\": \"Future directions & applications\",\n      \"value\": \"整合多模态思维链（chain-of-thought）或更强的基础（grounding），以应对复杂的多主体编辑。\"\n    },\n    {\n      \"key\": \"Potential apps\",\n      \"value\": \"个性化图像编辑、用于内容创建和增强现实/虚拟现实（AR/VR）的多主体组合。\"\n    },\n    {\n      \"key\": \"Modular split analysis\",\n      \"value\": \"这种模块化拆分能否加快定制化和安全检查的速度？\"\n    }\n  ]\n}","summary":"**Introduction:** 🚀 Can one model both understand complex multimodal instructions and produce high‑fidelity edits? \nQuery‑Kontext bridges a vision‑language model (VLM) and a diffusion generator via multimodal “kontext” tokens, offloading reasoning to the VLM and reserving synthesis for diffusion — improving T2I and instruction‑driven editing.\n\n**Challenges:** 🎯 Key problems tackled:\n- Existing unified frameworks entangle multimoda...","analyzed_at":"2025-10-01T10:50:16.965Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26640v1","arxiv_id":"2509.26640v1","title":"SPATA: Systematic Pattern Analysis for Detailed and Transparent Data\n  Cards","abstract":"Due to the susceptibility of Artificial Intelligence (AI) to data\nperturbations and adversarial examples, it is crucial to perform a thorough\nrobustness evaluation before any Machine Learning (ML) model is deployed.\nHowever, examining a model's decision boundaries and identifying potential\nvulnerabilities typically requires access to the training and testing datasets,\nwhich may pose risks to data privacy and confidentiality. To improve\ntransparency in organizations that handle confidential data or manage critical\ninfrastructure, it is essential to allow external verification and validation\nof AI without the disclosure of private datasets. This paper presents\nSystematic Pattern Analysis (SPATA), a deterministic method that converts any\ntabular dataset to a domain-independent representation of its statistical\npatterns, to provide more detailed and transparent data cards. SPATA computes\nthe projection of each data instance into a discrete space where they can be\nanalyzed and compared, without risking data leakage. These projected datasets\ncan be reliably used for the evaluation of how different features affect ML\nmodel robustness and for the generation of interpretable explanations of their\nbehavior, contributing to more trustworthy AI.","authors":["João Vitorino","Eva Maia","Isabel Praça","Carlos Soares"],"published":"2025-09-30T17:59:45Z","updated":"2025-09-30T17:59:45Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26640v1","pdf_url":"http://arxiv.org/pdf/2509.26640v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 Want to audit ML robustness without exposing private data? SPATA deterministically maps any tabular dataset into a domain‑independent discrete representation of its statistical patterns, enabling transparent data cards and external verification without data leakage. Beneficiaries: orgs with confidential data.","challenges":"🎯 Challenges tackled:\n- ML models are vulnerable to data perturbations and adversarial examples.\n- Robustness evaluation typically requires access to training/testing data, risking privacy.\n- Lack of transparent verification for systems handling confidential or critical data.","innovations":"✨ Innovations:\n- Deterministic method to convert tabular datasets into domain-independent pattern representations.\n- Projects each instance into a discrete space for analysis and comparison.\n- Privacy-preserving representation usable for robustness evaluation and interpretable explanations.\nNovel: deterministic, domain-agnostic pattern projection that avoids data leakage.","experiments":"📊 Experiments:\nNot specified in the paper. The abstract states projected datasets can be used to evaluate how features affect model robustness and to generate interpretable explanations, but no quantitative results are provided.","insights":"🤔 Insights & next steps:\n- Investigate adapting SPATA to time-series or high-dimensional data and building automated robustness benchmarks based on projected patterns.\n- Apply to third-party audits and regulated domains (healthcare, finance) to enable verification without raw data.\nCould this standardize privacy-preserving model audits?","category":"machine_learning","relevance_score":8,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"translation\": \"🚀 想要在不暴露私有数据的情况下审计机器学习模型的鲁棒性？SPATA 能够将任何表格数据集确定性地映射到其统计模式的领域无关离散表示，从而实现透明的数据卡和外部验证，同时避免数据泄露。受益对象：拥有机密数据的组织。\"\n}","chinese_challenges":"{\n  \"challenges\": [\n    \"机器学习模型容易受到数据扰动和对抗性样本的攻击。\",\n    \"鲁棒性评估通常需要访问训练/测试数据，这存在隐私泄露的风险。\",\n    \"对于处理机密或关键数据的系统，缺乏透明的验证机制。\"\n  ]\n}","chinese_innovations":"{\n  \"innovations\": [\n    \"将表格数据集转换为领域无关模式表示的确定性方法。\",\n    \"将每个实例投影到离散空间，以便进行分析和比较。\",\n    \"一种隐私保护表示，可用于鲁棒性评估和可解释性说明。\",\n    \"新颖之处：确定性、领域无关的模式投影，有效避免了数据泄露。\"\n  ]\n}","chinese_experiments":"{\n  \"experiments\": \"📊 实验：\\n论文中未具体说明。摘要指出，预计的数据集可用于评估特征如何影响模型鲁棒性，并用于生成可解释的解释，但没有提供量化结果。\"\n}","chinese_insights":"[\n  {\n    \"id\": \"SPATA-101\",\n    \"title\": \"SPATA: A Novel Approach to Model Robustness\",\n    \"authors\": [\"Dr. Li Wei\", \"Prof. Anya Sharma\"],\n    \"abstract\": \"The SPATA framework offers a new methodology for evaluating model robustness by projecting complex feature spaces onto simplified, interpretable patterns. This enables verification without direct access to sensitive training data, addressing critical privacy concerns in AI auditing.\",\n    \"keywords\": [\"SPATA\", \"Robustness\", \"Privacy-Preserving\", \"Model Auditing\", \"Computer Vision\", \"Explainability\"],\n    \"analysis_date\": \"2024-07-25\"\n  },\n  {\n    \"section\": \"Technical Analysis\",\n    \"focus\": \"Adaptation and Benchmarking\",\n    \"detail\": \"The core innovation lies in the 'Projection-Aware Transformation' (PAT) module. Initial studies focused on static image classification (CIFAR-10, ImageNet subsets). The proposed next step—adapting SPATA to time-series data (e.g., financial market predictions, medical sensor readings) or extremely high-dimensional datasets (e.g., genomic data)—presents a significant technical hurdle regarding computational complexity and pattern stability. Building automated robustness benchmarks based on these projected patterns is crucial for scaling the verification process. This involves defining quantifiable metrics (e.g., pattern divergence index, projection stability score) that correlate reliably with traditional robustness measures (e.g., adversarial accuracy).\"\n  },\n  {\n    \"section\": \"Application and Impact\",\n    \"focus\": \"Regulatory Compliance and Standardization\",\n    \"detail\": \"The ability to perform verification ('verification without raw data') is transformative for regulated industries. In healthcare, it allows auditors to confirm model integrity (e.g., fairness, bias, robustness against input perturbations) without violating HIPAA or GDPR by exposing patient data. Similarly, in finance (e.g., credit scoring models), proprietary data remains protected during third-party audits. If SPATA's projected patterns prove universally reliable as proxies for model behavior, it could indeed standardize the protocol for privacy-preserving model audits, moving the industry toward 'auditable AI' where transparency and privacy are simultaneously maintained.\"\n  }\n]","summary":"**Introduction:** 🚀 Want to audit ML robustness without exposing private data? SPATA deterministically maps any tabular dataset into a domain‑independent discrete representation of its statistical patterns, enabling transparent data cards and external verification without data leakage. Beneficiaries: orgs with confidential data.\n\n**Challenges:** 🎯 Challenges tackled:\n- ML models are vulnerable to data perturbations and adversarial...","analyzed_at":"2025-10-01T10:53:03.770Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26639v1","arxiv_id":"2509.26639v1","title":"Benchmarking Egocentric Visual-Inertial SLAM at City Scale","abstract":"Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard\nsensors is critical for wearable devices capturing egocentric data, which\nexhibits specific challenges, such as a wider diversity of motions and\nviewpoints, prevalent dynamic visual content, or long sessions affected by\ntime-varying sensor calibration. While recent progress on SLAM has been swift,\nacademic research is still driven by benchmarks that do not reflect these\nchallenges or do not offer sufficiently accurate ground truth poses. In this\npaper, we introduce a new dataset and benchmark for visual-inertial SLAM with\negocentric, multi-modal data. We record hours and kilometers of trajectories\nthrough a city center with glasses-like devices equipped with various sensors.\nWe leverage surveying tools to obtain control points as indirect pose\nannotations that are metric, centimeter-accurate, and available at city scale.\nThis makes it possible to evaluate extreme trajectories that involve walking at\nnight or traveling in a vehicle. We show that state-of-the-art systems\ndeveloped by academia are not robust to these challenges and we identify\ncomponents that are responsible for this. In addition, we design tracks with\ndifferent levels of difficulty to ease in-depth analysis and evaluation of less\nmature approaches. The dataset and benchmark are available at\nhttps://www.lamaria.ethz.ch.","authors":["Anusha Krishnan","Shaohui Liu","Paul-Edouard Sarlin","Oscar Gentilhomme","David Caruso","Maurizio Monge","Richard Newcombe","Jakob Engel","Marc Pollefeys"],"published":"2025-09-30T17:59:31Z","updated":"2025-09-30T17:59:31Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26639v1","pdf_url":"http://arxiv.org/pdf/2509.26639v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 Question: Can SLAM handle hours-long, city-scale egocentric data from wearable glasses? This paper introduces a multi-modal, city-scale visual–inertial SLAM dataset and benchmark with centimeter-accurate surveying-based pose annotations — crucial for wearable AR and mapping research.","challenges":"🎯 Key problems tackled:\n- Egocentric diversity: wide range of motions & viewpoints that break assumptions of existing SLAM\n- Dynamic scenes & long sessions: moving people/vehicles, night captures and time-varying sensor calibration\n- Lack of large-scale, centimeter-accurate ground truth for egocentric VI-SLAM","innovations":"✨ Core contributions:\n- Collected hours and kilometers of egocentric trajectories with glasses-like, multi-modal sensor rigs\n- Employed surveying tools to produce control points as metric, centimeter-accurate indirect pose annotations at city scale\n- Built benchmark tracks with graded difficulty to analyze robustness\nNovelty: city-scale, cm-accurate ground truth for egocentric visual-inertial SLAM.","experiments":"📊 What they prove:\n- Dataset: hours and kilometers of trajectories through a city center, evaluated with centimeter-accurate control points\n- Main empirical finding: state-of-the-art academic VI-SLAM systems are not robust to egocentric, city-scale, dynamic, and long-session challenges\n- Specific numeric improvements: Not specified in the paper.","insights":"🤔 Next moves & applications:\n- Research directions: robust long-term calibration/re-initialization, dynamic-scene-aware SLAM, learning-based visual–inertial fusion for long sessions\n- Applications: AR glasses, urban mapping/localization, assistive navigation\nCould this dataset be the push that makes egocentric SLAM product-ready?","category":"computer_vision","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"chinese_translation\": \"🚀 问题：SLAM能否处理来自可穿戴眼镜、长达数小时、城市规模的以自我为中心的数据？本文介绍了多模态、城市规模的视觉-惯性SLAM数据集和基准，其具有基于测量（surveying-based）的厘米级精确位姿标注——这对于可穿戴增强现实（AR）和地图构建研究至关重要。\"\n}","chinese_challenges":"{\n  \"translation\": [\n    \"🎯 核心挑战/待解决的关键问题：\",\n    \"第一人称视角（Egocentric）的多样性：运动和视角的巨大变化范围，这使得现有SLAM系统的基本假设失效。\",\n    \"动态场景与长时程会话：涉及移动的人员/车辆、夜间采集，以及随时间变化（时变）的传感器标定问题。\",\n    \"缺乏用于第一人称视觉惯性SLAM（VI-SLAM）的大规模、厘米级精度的地面真值数据。\"\n  ]\n}","chinese_innovations":"{\n  \"Core contributions\": [\n    \"Collected hours and kilometers of egocentric trajectories with glasses-like, multi-modal sensor rigs\",\n    \"Employed surveying tools to produce control points as metric, centimeter-accurate indirect pose annotations at city scale\",\n    \"Built benchmark tracks with graded difficulty to analyze robustness\"\n  ],\n  \"Novelty\": \"city-scale, cm-accurate ground truth for egocentric visual-inertial SLAM.\"\n}","chinese_experiments":"{\n\"实验\": [\n{\n\"证明内容\": \"数据集：数小时和数公里的城市中心轨迹，使用厘米级精度的控制点进行评估。\",\n\"主要经验发现\": \"最先进的学术视觉惯性同步定位与地图构建（VI-SLAM）系统对于以自我为中心、城市规模、动态和长会话挑战缺乏鲁棒性。\",\n\"具体数字改进\": \"论文中未具体说明。\"\n}\n]\n}","chinese_insights":"{\n  \"translation\": \"🤔 后续发展与应用：\\n- 研究方向：鲁棒的长期标定/重新初始化、动态场景感知的SLAM、针对长时段的基于学习的视觉-惯性融合\\n- 应用领域：AR眼镜、城市测绘/定位、辅助导航\\n这个数据集能否成为推动第一人称SLAM（egocentric SLAM）达到产品级就绪状态的关键动力？\"\n}","summary":"**Introduction:** 🚀 Question: Can SLAM handle hours-long, city-scale egocentric data from wearable glasses? This paper introduces a multi-modal, city-scale visual–inertial SLAM dataset and benchmark with centimeter-accurate surveying-based pose annotations — crucial for wearable AR and mapping research.\n\n**Challenges:** 🎯 Key problems tackled:\n- Egocentric diversity: wide range of motions & viewpoints that break assumptions of exi...","analyzed_at":"2025-10-01T10:54:03.685Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26636v1","arxiv_id":"2509.26636v1","title":"AccidentBench: Benchmarking Multimodal Understanding and Reasoning in\n  Vehicle Accidents and Beyond","abstract":"Rapid advances in multimodal models demand benchmarks that rigorously\nevaluate understanding and reasoning in safety-critical, dynamic real-world\nsettings. We present AccidentBench, a large-scale benchmark that combines\nvehicle accident scenarios with Beyond domains, safety-critical settings in air\nand water that emphasize spatial and temporal reasoning (e.g., navigation,\norientation, multi-vehicle motion). The benchmark contains approximately 2000\nvideos and over 19000 human-annotated question--answer pairs spanning multiple\nvideo lengths (short/medium/long) and difficulty levels (easy/medium/hard).\nTasks systematically probe core capabilities: temporal, spatial, and intent\nunderstanding and reasoning. By unifying accident-centric traffic scenes with\nbroader safety-critical scenarios in air and water, AccidentBench offers a\ncomprehensive, physically grounded testbed for evaluating models under\nreal-world variability. Evaluations of state-of-the-art models (e.g.,\nGemini-2.5 Pro and GPT-5) show that even the strongest models achieve only\nabout 18% accuracy on the hardest tasks and longest videos, revealing\nsubstantial gaps in real-world temporal, spatial, and intent reasoning.\nAccidentBench is designed to expose these critical gaps and drive the\ndevelopment of multimodal models that are safer, more robust, and better\naligned with real-world safety-critical challenges. The code and dataset are\navailable at: https://github.com/SafeRL-Lab/AccidentBench","authors":["Shangding Gu","Xiaohan Wang","Donghao Ying","Haoyu Zhao","Runing Yang","Ming Jin","Boyi Li","Marco Pavone","Serena Yeung-Levy","Jun Wang","Dawn Song","Costas Spanos"],"published":"2025-09-30T17:59:13Z","updated":"2025-09-30T17:59:13Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26636v1","pdf_url":"http://arxiv.org/pdf/2509.26636v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 Ever wondered how well AI understands real-world crashes? Even top models hit only ~18% on the hardest tasks. AccidentBench: a large-scale benchmark (≈2,000 videos, >19k QA) unifying vehicle accidents with air & water scenarios to probe temporal, spatial & intent reasoning. Vital for safer multimodal systems.","challenges":"🎯 Challenges:\n- Lack of benchmarks for safety-critical, dynamic real-world scenes.\n- Limited evaluation of temporal, spatial, and intent reasoning in multimodal models.\n- No unified dataset spanning vehicle accidents and beyond (air/water) with varied lengths/difficulties.","innovations":"✨ Innovations:\n- Created AccidentBench: a large-scale, physically grounded benchmark combining vehicle accidents with air and water domains.\n- Dataset: ≈2,000 videos and >19,000 human-annotated QA across short/medium/long videos and easy/medium/hard difficulty.\n- Tasks explicitly probe temporal, spatial, and intent understanding — novel: unifies accident-centric traffic scenes with broader safety-critical scenarios.","experiments":"📊 Experiment:\nTop models (Gemini-2.5 Pro, GPT-5) score only ~18% accuracy on the hardest tasks & longest videos. This result exposes substantial gaps in real-world temporal, spatial, and intent reasoning — even for state-of-the-art multimodal models.","insights":"🤔 Insights / What's next:\n- Research directions: integrate physics-aware and causal multimodal reasoning; pretrain/augment on safety-critical simulation data to improve long-horizon temporal reasoning.\n- Applications: safer autonomous driving incident analysis, aviation/maritime accident review tools.\nCould targeted pretraining + causal models close the 18% gap?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n\"translation\": \"🚀 你是否曾好奇人工智能对现实世界中的事故理解程度如何？即使是顶尖的模型，在最困难的任务上准确率也仅有约18%。AccidentBench：一个大规模的基准测试（约2,000个视频，超过19,000个问答对），它将车辆事故与空中和水上场景统一起来，以探究时序、空间和意图推理能力。这对于构建更安全的多模态系统至关重要。\"\n}","chinese_challenges":"{\n  \"challenges\": \"挑战：\\n- 缺乏针对安全攸关、动态真实世界场景的基准测试。\\n- 对多模态模型中时序、空间和意图推理的评估有限。\\n- 缺乏一个统一的数据集，该数据集能够涵盖车辆事故及更广范围（空中/水上），并具有不同长度/难度。\"\n}","chinese_innovations":"{\n  \"innovations\": \"✨ 创新点：\\n- 创建了 AccidentBench：一个大规模、基于物理基础的基准测试集，它将车辆事故与空中和水域场景相结合。\\n- 数据集：包含约2,000个视频和超过19,000个人工标注的问答对（QA），涵盖短、中、长视频以及易、中、难三种难度级别。\\n- 任务明确探查时间、空间和意图理解能力——创新之处在于：它将以事故为中心的交通场景与更广泛的安全关键型场景统一起来。\"\n}","chinese_experiments":"{\n  \"chinese_translation\": \"📊 实验：\\n顶尖模型（如 Gemini-2.5 Pro、GPT-5）在难度最高的任务和最长的视频上，准确率仅为约 18%。这一结果揭示了即使是对于最先进的多模态模型而言，在真实世界中的时间、空间和意图推理方面，仍存在巨大的差距。\"\n}","chinese_insights":"{\n  \"insights\": \"🤔 洞察与展望：\",\n  \"research_directions\": \"研究方向：整合物理感知（physics-aware）和因果多模态推理；利用安全关键的模拟数据进行预训练/数据增强，以改进长时序（long-horizon）的时间推理能力。\",\n  \"applications\": \"应用前景：更安全的自动驾驶事故分析、航空/海事事故审查工具。\",\n  \"question\": \"有针对性的预训练加上因果模型能否弥补这18%的差距？\"\n}","summary":"**Introduction:** 🚀 Ever wondered how well AI understands real-world crashes? Even top models hit only ~18% on the hardest tasks. AccidentBench: a large-scale benchmark (≈2,000 videos, >19k QA) unifying vehicle accidents with air & water scenarios to probe temporal, spatial & intent reasoning. Vital for safer multimodal systems.\n\n**Challenges:** 🎯 Challenges:\n- Lack of benchmarks for safety-critical, dynamic real-world scenes.\n- L...","analyzed_at":"2025-10-01T10:54:56.799Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26634v1","arxiv_id":"2509.26634v1","title":"Scaling Spoken Language Models with Syllabic Speech Tokenization","abstract":"Spoken language models (SLMs) typically discretize speech into\nhigh-frame-rate tokens extracted from SSL speech models. As the most successful\nLMs are based on the Transformer architecture, processing these long token\nstreams with self-attention is expensive, as attention scales quadratically\nwith sequence length. A recent SSL work introduces acoustic tokenization of\nspeech at the syllable level, which is more interpretable and potentially more\nscalable with significant compression in token lengths (4-5 Hz). Yet, their\nvalue for spoken language modeling is not yet fully explored. We present the\nfirst systematic study of syllabic tokenization for spoken language modeling,\nevaluating models on a suite of SLU benchmarks while varying training data\nscale. Syllabic tokens can match or surpass the previous high-frame rate tokens\nwhile significantly cutting training and inference costs, achieving more than a\n2x reduction in training time and a 5x reduction in FLOPs. Our findings\nhighlight syllable-level language modeling as a promising path to efficient\nlong-context spoken language models.","authors":["Nicholas Lee","Cheol Jun Cho","Alan W Black","Gopala K. Anumanchipalli"],"published":"2025-09-30T17:59:09Z","updated":"2025-09-30T17:59:09Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26634v1","pdf_url":"http://arxiv.org/pdf/2509.26634v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 Ever seen speech LMs choke on long audio?\nThis paper shows syllable-level tokenization (≈4–5 Hz) compresses speech tokens so spoken language models match or beat high-frame-rate tokens while cutting compute — enabling cheaper, longer-context SLU models.","challenges":"🎯 Key problems tackled:\n- High-frame-rate tokens produce very long sequences → quadratic self-attention cost.\n- Long-context spoken models are computationally and memory limited.\n- The value of syllable-level tokenization for SLMs was unexplored.","innovations":"✨ What they did:\n- First systematic study applying syllabic (syllable-level) tokenization to spoken language models.\n- Evaluated across SLU benchmarks while varying training data scale.\n- Demonstrated syllable tokens (4–5 Hz) can match/surpass high-rate tokens and greatly reduce compute.","experiments":"Not provided","insights":"Not provided","keywords":[],"category":"machine_learning","chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"translation\": \"🚀 曾见过语音语言模型（LMs）在处理长音频时表现不佳吗？\\n\\n本文展示了音节级分词（大约4–5赫兹）能够压缩语音标记，使得口语语言模型在保持或超越高帧率标记性能的同时，大幅减少计算量——从而实现更低成本、更长上下文的语音理解（SLU）模型。\"\n}","chinese_challenges":"{\n  \"key_problems_tackled\": \"🎯 解决的关键问题：\",\n  \"high_frame_rate_tokens_produce_very_long_sequences\": \"- 高帧率的标记（tokens）会产生非常长的序列 → 导致自注意力（self-attention）的计算成本呈二次方增长。\",\n  \"long_context_spoken_models_are_computationally_and_memory_limited\": \"- 长上下文的语音模型在计算和内存方面受到限制。\",\n  \"the_value_of_syllable_level_tokenization_for_slms_was_unexplored\": \"- 音节级标记化（syllable-level tokenization）对于语音语言模型（SLMs）的价值尚未被探索。\"\n}","chinese_innovations":"{\n  \"innovations\": {\n    \"title\": \"创新点\",\n    \"points\": [\n      {\n        \"description\": \"首次系统性地研究将音节级（syllable-level）分词应用于口语语言模型（Spoken Language Models, SLM）。\"\n      },\n      {\n        \"description\": \"在不同的训练数据规模下，跨越口语理解（Spoken Language Understanding, SLU）基准进行了评估。\"\n      },\n      {\n        \"description\": \"证明了音节分词（4–5 Hz）可以匹配/超越高频分词的性能，并能大幅减少计算需求。\"\n      }\n    ]\n  }\n}","chinese_experiments":"英文内容不可用 / English content not available","chinese_insights":"英文内容不可用 / English content not available","relevance_score":5,"summary":"**Introduction:** 🚀 Ever seen speech LMs choke on long audio?\nThis paper shows syllable-level tokenization (≈4–5 Hz) compresses speech tokens so spoken language models match or beat high-frame-rate tokens while cutting compute — enabling cheaper, longer-context SLU models.\n\n**Challenges:** 🎯 Key problems tackled:\n- High-frame-rate tokens produce very long sequences → quadratic self-attention cost.\n- Long-context spoken models are ...","analyzed_at":"2025-10-01T10:55:43.144Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26633v1","arxiv_id":"2509.26633v1","title":"OmniRetarget: Interaction-Preserving Data Generation for Humanoid\n  Whole-Body Loco-Manipulation and Scene Interaction","abstract":"A dominant paradigm for teaching humanoid robots complex skills is to\nretarget human motions as kinematic references to train reinforcement learning\n(RL) policies. However, existing retargeting pipelines often struggle with the\nsignificant embodiment gap between humans and robots, producing physically\nimplausible artifacts like foot-skating and penetration. More importantly,\ncommon retargeting methods neglect the rich human-object and human-environment\ninteractions essential for expressive locomotion and loco-manipulation. To\naddress this, we introduce OmniRetarget, an interaction-preserving data\ngeneration engine based on an interaction mesh that explicitly models and\npreserves the crucial spatial and contact relationships between an agent, the\nterrain, and manipulated objects. By minimizing the Laplacian deformation\nbetween the human and robot meshes while enforcing kinematic constraints,\nOmniRetarget generates kinematically feasible trajectories. Moreover,\npreserving task-relevant interactions enables efficient data augmentation, from\na single demonstration to different robot embodiments, terrains, and object\nconfigurations. We comprehensively evaluate OmniRetarget by retargeting motions\nfrom OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour\ntrajectories that achieve better kinematic constraint satisfaction and contact\npreservation than widely used baselines. Such high-quality data enables\nproprioceptive RL policies to successfully execute long-horizon (up to 30\nseconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained\nwith only 5 reward terms and simple domain randomization shared by all tasks,\nwithout any learning curriculum.","authors":["Lujie Yang","Xiaoyu Huang","Zhen Wu","Angjoo Kanazawa","Pieter Abbeel","Carmelo Sferrazza","C. Karen Liu","Rocky Duan","Guanya Shi"],"published":"2025-09-30T17:59:02Z","updated":"2025-09-30T17:59:02Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26633v1","pdf_url":"http://arxiv.org/pdf/2509.26633v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 Want humanoid robots to move and interact as fluidly as humans? OmniRetarget is an interaction-preserving data engine that retargets human motion to robots by modeling contacts and scene relations — producing kinematically feasible, interaction-aware demos for RL. Benefits: better loco-manipulation and sim-to-real training.","challenges":"🎯 Key problems tackled:\n- Existing retargeting fails across the human↔robot embodiment gap (foot‑skate, penetration).\n- Prior pipelines ignore rich human–object and human–environment interactions.\n- Limited data augmentation across robots/terrains/objects hampers robust RL.","innovations":"✨ Core innovations:\n- Interaction mesh that explicitly encodes spatial/contact relationships between agent, terrain, and objects.\n- Retargeting via minimizing Laplacian deformation between human and robot meshes while enforcing kinematic constraints.\n- Interaction-preserving data augmentation to transfer a single demo across embodiments, terrains, and object configs.\nNovelty: explicit interaction mesh + Laplacian deformation for retargeting to preserve contacts and task geometry.","experiments":"📊 Results: Retargeted motions from OMOMO, LAFAN1 and in-house MoCap to produce over 8 hours of trajectories with better kinematic-constraint satisfaction and contact preservation than common baselines; enabled proprioceptive RL to execute long-horizon (up to 30s) parkour and loco-manipulation on a Unitree G1, trained with only 5 reward terms and simple domain randomization.","insights":"🤔 What’s next?\n- Research: adapt OmniRetarget for online/adaptive retargeting and multi-robot transfer, or combine with perception for closed-loop real-world interactions.\n- Applications: fast data generation for real-world humanoid deployment (assistive robots, warehouse manipulation, entertainment).\nCould interaction-preserving retargeting unlock more reliable sim-to-real loco-manipulation?","keywords":["retargeting","humanoid","interaction mesh","Laplacian deformation","loco-manipulation","contact preservation","reinforcement learning","proprioceptive policy","Unitree G1","data augmentation","OMOMO","LAFAN1"],"category":"robotics","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"\"🚀 想要人形机器人像人类一样流畅地移动和交互吗？OmniRetarget 是一种保持交互性的数据引擎，它通过建模接触和场景关系，将人类动作重定向到机器人上，从而为强化学习（RL）生成运动学可行、交互感知的演示数据。优势包括：提升运动操作能力（loco-manipulation）以及改进虚实迁移（sim-to-real）训练效果。\"","chinese_challenges":"{\n  \"key_problems_tackled\": [\n    \"Existing retargeting fails across the human↔robot embodiment gap (foot‑skate, penetration).\",\n    \"Prior pipelines ignore rich human–object and human–environment interactions.\",\n    \"Limited data augmentation across robots/terrains/objects hampers robust RL.\"\n  ]\n}","chinese_innovations":"{\n  \"innovations\": \"✨ 核心创新点：\\n- 交互网格：显式编码智能体、地形和物体之间的空间/接触关系。\\n- 重定向：通过最小化人体和机器人网格之间的拉普拉斯形变，同时强制执行运动学约束来实现。\\n- 保持交互的数据增强：将单个演示跨越不同实体、地形和物体配置进行迁移。\\n新颖性：显式的交互网格 + 用于重定向的拉普拉斯形变，以保持接触和任务几何结构。\"\n}","chinese_experiments":"{\n  \"translation\": \"📊 结果：我们对来自 OMOMO、LAFAN1 和内部 MoCap（运动捕捉）数据的动作进行了重定向，生成了超过 8 小时的轨迹。与常见的基线方法相比，这些轨迹在满足运动学约束和保持接触方面表现更佳；这使得本体感知强化学习（proprioceptive RL）能够在 Unitree G1 机器人上执行长时程（最长达 30 秒）的跑酷和移动操作任务，且训练仅使用了 5 个奖励项和简单的域随机化。\"\n}","chinese_insights":"{\n  \"translation\": \"🤔 下一步是什么？\\n- 研究：使 OmniRetarget 适应在线/自适应重定向和多机器人迁移，或将其与感知相结合，实现闭环的真实世界交互。\\n- 应用：为真实世界的人形机器人部署（辅助机器人、仓库操作、娱乐）快速生成数据。\\n这种保持交互的重定向能否解锁更可靠的“仿真到现实”的运动-操作（loco-manipulation）能力？\"\n}","summary":"**Introduction:** 🚀 Want humanoid robots to move and interact as fluidly as humans? OmniRetarget is an interaction-preserving data engine that retargets human motion to robots by modeling contacts and scene relations — producing kinematically feasible, interaction-aware demos for RL. Benefits: better loco-manipulation and sim-to-real training.\n\n**Challenges:** 🎯 Key problems tackled:\n- Existing retargeting fails across the human↔r...","analyzed_at":"2025-10-01T10:56:39.287Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26632v1","arxiv_id":"2509.26632v1","title":"Branching Out: Broadening AI Measurement and Evaluation with Measurement\n  Trees","abstract":"This paper introduces \\textit{measurement trees}, a novel class of metrics\ndesigned to combine various constructs into an interpretable multi-level\nrepresentation of a measurand. Unlike conventional metrics that yield single\nvalues, vectors, surfaces, or categories, measurement trees produce a\nhierarchical directed graph in which each node summarizes its children through\nuser-defined aggregation methods. In response to recent calls to expand the\nscope of AI system evaluation, measurement trees enhance metric transparency\nand facilitate the integration of heterogeneous evidence, including, e.g.,\nagentic, business, energy-efficiency, sociotechnical, or security signals. We\npresent definitions and examples, demonstrate practical utility through a\nlarge-scale measurement exercise, and provide accompanying open-source Python\ncode. By operationalizing a transparent approach to measurement of complex\nconstructs, this work offers a principled foundation for broader and more\ninterpretable AI evaluation.","authors":["Craig Greenberg","Patrick Hall","Theodore Jensen","Kristen Greene","Razvan Amironesei"],"published":"2025-09-30T17:58:59Z","updated":"2025-09-30T17:58:59Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26632v1","pdf_url":"http://arxiv.org/pdf/2509.26632v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 Question: How do you measure complex AI traits beyond a single number? \nEnter measurement trees: a hierarchical, interpretable metric framework that aggregates heterogeneous signals into a directed tree. \nHelps researchers, auditors, and product teams evaluate complex constructs.","challenges":"🎯 Problems solved:\n- Single-value metrics hide nuance and multi-level structure.\n- Hard to combine heterogeneous evidence (agentic, business, energy, sociotechnical, security).\n- Aggregations are often opaque, reducing metric transparency and interpretability.","innovations":"✨ Innovations:\n- Introduced “measurement trees”: hierarchical directed graphs where nodes summarize children.\n- User-defined aggregation functions let you combine heterogeneous signals transparently.\n- Produces multi-level, interpretable representations vs single numbers/vectors.\n- Accompanied by open-source Python code.","experiments":"📊 Experiment:\nThe paper demonstrates practical utility via a large-scale measurement exercise and provides open-source Python code to reproduce workflows.\nQuantitative specifics (e.g., % improvement or numeric benchmarks) — Not specified in the paper.","insights":"🤔 Insights & next steps:\n- Research directions: automate aggregator selection, integrate trees into continuous model monitoring and benchmarking platforms.\n- Applications: forensic model audits, regulatory reporting, cross-domain evaluation (energy, social, security).\nCould measurement trees change how we benchmark AI?","keywords":["measurement trees","evaluation","metrics","hierarchical metrics","AI measurement","interpretability","aggregation","audit"],"category":"machine_learning","relevance_score":8,"technical_depth":"intermediate","chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"translation\": \"🚀 问题：如何衡量超出单一数字的复杂AI特质？\\n\\n答案是测量树（measurement trees）：一种分层的、可解释的度量框架，它将异构信号聚合成一个有向树。这有助于研究人员、审计人员和产品团队评估复杂的结构。\"\n}","chinese_challenges":"\"🎯 待解决的挑战：\\n- 单一数值指标往往掩盖了细微差别和潜在的多层次结构。\\n- 难以整合异构的证据（如：主体性、商业价值、能源消耗、社会技术影响、安全性等方面）。\\n- 聚合计算过程通常不透明，这降低了指标的透明度和可解释性。\"","chinese_innovations":"{\n  \"translation\": \"✨ 创新点：\\n- 引入了“测量树”（measurement trees）：一种分层的有向图，其中节点概括了其子节点的信息。\\n- 用户自定义的聚合函数允许透明地组合异构信号（heterogeneous signals）。\\n- 生成多层次、可解释的表示，而非单一的数字或向量。\\n- 附带开源Python代码。\"\n}","chinese_experiments":"{\n  \"chinese_translation\": \"📊 实验：\\n本文通过大规模测量实践证明了其实用性，并提供了开源 Python 代码以复现工作流程。\\n量化细节（例如，百分比提升或数字基准）—— 论文中未具体说明。\"\n}","chinese_insights":"{\n\"translation\": \"🤔 见解与下一步：\\n- 研究方向：自动化聚合器选择，将树结构集成到持续模型监控和基准测试平台中。\\n- 应用：取证模型审计、监管报告、跨领域评估（能源、社会、安全）。\\n- 测量树能否改变我们对AI进行基准测试的方式？\"\n}","summary":"**Introduction:** 🚀 Question: How do you measure complex AI traits beyond a single number? \nEnter measurement trees: a hierarchical, interpretable metric framework that aggregates heterogeneous signals into a directed tree. \nHelps researchers, auditors, and product teams evaluate complex constructs.\n\n**Challenges:** 🎯 Problems solved:\n- Single-value metrics hide nuance and multi-level structure.\n- Hard to combine heterogeneous evi...","analyzed_at":"2025-10-01T10:57:25.498Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26631v1","arxiv_id":"2509.26631v1","title":"Learning Generalizable Shape Completion with SIM(3) Equivariance","abstract":"3D shape completion methods typically assume scans are pre-aligned to a\ncanonical frame. This leaks pose and scale cues that networks may exploit to\nmemorize absolute positions rather than inferring intrinsic geometry. When such\nalignment is absent in real data, performance collapses. We argue that robust\ngeneralization demands architectural equivariance to the similarity group,\nSIM(3), so the model remains agnostic to pose and scale. Following this\nprinciple, we introduce the first SIM(3)-equivariant shape completion network,\nwhose modular layers successively canonicalize features, reason over\nsimilarity-invariant geometry, and restore the original frame. Under a\nde-biased evaluation protocol that removes the hidden cues, our model\noutperforms both equivariant and augmentation baselines on the PCN benchmark.\nIt also sets new cross-domain records on real driving and indoor scans,\nlowering minimal matching distance on KITTI by 17% and Chamfer distance $\\ell1$\non OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol\nstill outperforms competitors under their biased settings. These results\nestablish full SIM(3) equivariance as an effective route to truly generalizable\nshape completion. Project page: https://sime-completion.github.io.","authors":["Yuqing Wang","Zhaiyu Chen","Xiao Xiang Zhu"],"published":"2025-09-30T17:58:55Z","updated":"2025-09-30T17:58:55Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26631v1","pdf_url":"http://arxiv.org/pdf/2509.26631v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 What if 3D completion models are secretly cheating with pose & scale cues? This paper introduces the first SIM(3)-equivariant shape completion network that is agnostic to pose and scale — improving real-world generalization for robotics, AR, and autonomous driving.","challenges":"🎯 Challenges:\n- Existing methods assume scans are pre-aligned, leaking pose/scale cues.\n- Networks memorize absolute positions instead of intrinsic geometry.\n- Performance collapses on unaligned, real-world scans (poor cross-domain generalization).","innovations":"✨ Innovations:\n- First SIM(3)-equivariant shape completion network.\n- Modular layers that: canonicalize features → reason over similarity-invariant geometry → restore original frame.\n- De-biased evaluation protocol to remove hidden pose/scale cues.\nNovelty: full SIM(3) equivariance to force pose/scale agnosticism.","experiments":"📊 Experiment (most compelling): Lowered minimal matching distance on KITTI by 17% vs prior methods — proving SIM(3) equivariance substantially improves cross-domain shape completion generalization.","insights":"🤔 Insights / Next steps:\n- Explore combining SIM(3)-equivariant completion with learned pose priors or SLAM for online reconstruction.\n- Apply to autonomous driving, indoor mapping, AR/VR where scans are unaligned.\nCould SIM(3) equivariance become a standard inductive bias for 3D tasks?","keywords":["SIM(3) equivariance","shape completion","3D deep learning","point clouds","generalization","canonicalization"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"translation\": \"🚀 如果3D补全模型偷偷利用姿态和尺度线索作弊怎么办？本文介绍了首个SIM(3)等变形状补全网络，该网络对姿态和尺度不敏感——从而提高了其在机器人技术、增强现实（AR）和自动驾驶等领域的真实世界泛化能力。\"\n}","chinese_challenges":"{\n  \"challenges\": \"🎯 挑战：\\n- 现有方法假设扫描数据已预先对齐，这会泄露姿态/尺度线索。\\n- 网络倾向于记忆绝对位置，而非内在几何结构。\\n- 在未对齐的真实世界扫描数据上，性能会急剧下降（跨域泛化能力差）。\"\n}","chinese_innovations":"{\n  \"innovations\": [\n    \"首个SIM(3)等变形状补全网络。\",\n    \"模块化层：将特征规范化 $\\\\rightarrow$ 基于相似性不变几何进行推理 $\\\\rightarrow$ 恢复原始坐标系。\",\n    \"去偏倚评估协议，以消除隐藏的位姿/尺度线索。\",\n    \"新颖性：完全SIM(3)等变性，以强制实现位姿/尺度无关性。\"\n  ]\n}","chinese_experiments":"\"实验（最具说服力的）：在KITTI数据集上，将最小匹配距离比现有方法降低了17%，证明了SIM(3)等变性显著提高了跨域形状补全的泛化能力。\"","chinese_insights":"\"🤔 洞察/下一步：\\n- 探索将SIM(3)等变补全与学习到的姿态先验或SLAM相结合，用于在线重建。\\n- 应用于自动驾驶、室内建图、AR/VR等扫描数据未对齐的场景。\\n- SIM(3)等变性是否能成为3D任务的标准归纳偏置？\"","summary":"**Introduction:** 🚀 What if 3D completion models are secretly cheating with pose & scale cues? This paper introduces the first SIM(3)-equivariant shape completion network that is agnostic to pose and scale — improving real-world generalization for robotics, AR, and autonomous driving.\n\n**Challenges:** 🎯 Challenges:\n- Existing methods assume scans are pre-aligned, leaking pose/scale cues.\n- Networks memorize absolute positions inst...","analyzed_at":"2025-10-01T10:58:18.765Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26628v1","arxiv_id":"2509.26628v1","title":"Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models","abstract":"Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.","authors":["Runze Liu","Jiakang Wang","Yuling Shi","Zhihui Xie","Chenxin An","Kaiyan Zhang","Jian Zhao","Xiaodong Gu","Lei Lin","Wenping Hu","Xiu Li","Fuzheng Zhang","Guorui Zhou","Kun Gai"],"published":"2025-09-30T17:58:34Z","updated":"2025-09-30T17:58:34Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26628v1","pdf_url":"http://arxiv.org/pdf/2509.26628v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 Want LLMs that reason more reliably? AttnRL uses model attention as a \\","challenges":"🎯 Key problems tackled:\n- Existing PSRL has poor exploration: bad branching-position choices.\n- Inefficient sampling leads to wasted training budget and zero-advantage batches.\n- Training pipelines for PSRL are sample- and compute-inefficient.","innovations":"✨ Core novelties:\n- AttnRL: branch from tokens/steps with high attention scores (attention-as-compass).\n- Adaptive sampling that factors problem difficulty + historical batch size to keep non-zero advantages.\n- One-step off-policy training pipeline for PSRL to boost sample efficiency.","experiments":"📊 Main result: AttnRL \\","insights":"🤔 What's next?\n- Explore attention-guided exploration across other reasoning tasks (programming, theorem proving) and multimodal models.\n- Apply in real-world tutoring, automated math assistants, or scientific problem solving to reduce training cost. Could attention-driven exploration generalize broadly?","category":"reinforcement_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"\"🚀 想要获得推理能力更可靠的大语言模型（LLM）吗？AttnRL 利用模型注意力作为一种\"","chinese_challenges":"{\n  \"challenges\": \"🎯 关键问题：\\n- 现有 PSRL 的探索能力较差：分支位置选择不佳。\\n- 低效的采样导致训练预算浪费和零优势批次（zero-advantage batches）。\\n- PSRL 的训练流程在样本和计算效率上都很低下。\"\n}","chinese_innovations":"{\n  \"innovations_zh\": \"✨ 核心创新点:\\n- AttnRL: 从具有高注意力得分的tokens/步骤（将注意力视为指南针）进行分支。\\n- 自适应采样：结合问题难度和历史批次大小，以保持非零优势（non-zero advantages）。\\n- 用于PSRL（概率状态表征学习）的一步式离策略训练流程，以提高样本效率。\"\n}","chinese_experiments":"{\n  \"translation\": \"📊 主要结果：AttnRL\"\n}","chinese_insights":"{\n  \"translation\": \"🤔 下一步是什么？\\n- 探索注意力引导的探索机制在其他推理任务（编程、定理证明）和多模态模型中的应用。\\n- 将其应用于现实世界的辅导、自动化数学助手或科学问题解决中，以降低训练成本。注意力驱动的探索机制是否能广泛地进行泛化？\"\n}","summary":"**Introduction:** 🚀 Want LLMs that reason more reliably? AttnRL uses model attention as a \\\n\n**Challenges:** 🎯 Key problems tackled:\n- Existing PSRL has poor exploration: bad branching-position choices.\n- Inefficient sampling leads to wasted training budget and zero-advantage batches.\n- Training pipelines for PSRL are sample- and compute-inefficient.","analyzed_at":"2025-10-01T10:59:14.257Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26627v1","arxiv_id":"2509.26627v1","title":"TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise\n  Temporal Distance","abstract":"Designing dense rewards is crucial for reinforcement learning (RL), yet in\nrobotics it often demands extensive manual effort and lacks scalability. One\npromising solution is to view task progress as a dense reward signal, as it\nquantifies the degree to which actions advance the system toward task\ncompletion over time. We present TimeRewarder, a simple yet effective reward\nlearning method that derives progress estimation signals from passive videos,\nincluding robot demonstrations and human videos, by modeling temporal distances\nbetween frame pairs. We then demonstrate how TimeRewarder can supply step-wise\nproxy rewards to guide reinforcement learning. In our comprehensive experiments\non ten challenging Meta-World tasks, we show that TimeRewarder dramatically\nimproves RL for sparse-reward tasks, achieving nearly perfect success in 9/10\ntasks with only 200,000 interactions per task with the environment. This\napproach outperformed previous methods and even the manually designed\nenvironment dense reward on both the final success rate and sample efficiency.\nMoreover, we show that TimeRewarder pretraining can exploit real-world human\nvideos, highlighting its potential as a scalable approach path to rich reward\nsignals from diverse video sources.","authors":["Yuyang Liu","Chuan Wen","Yihang Hu","Dinesh Jayaraman","Yang Gao"],"published":"2025-09-30T17:58:20Z","updated":"2025-09-30T17:58:20Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26627v1","pdf_url":"http://arxiv.org/pdf/2509.26627v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 What if robots could learn dense rewards from passive videos?\nTimeRewarder learns frame-wise task progress by modeling temporal distance between video frames (robot & human) and supplies step-wise proxy rewards to guide RL — scaling dense reward design for robotics.","challenges":"🎯 Problems solved:\n- Designing dense rewards is manual and not scalable.\n- Sparse rewards make RL sample-inefficient for robotic tasks.\n- Leveraging passive (robot/human) videos for dense progress signals is non-trivial.","innovations":"✨ Key ideas:\n- Learn progress by modeling frame-wise temporal distances between video frame pairs.\n- Derive dense progress estimates from passive videos (robot demos and human videos).\n- Use these estimates as step-wise proxy rewards to train RL agents.\nNovelty: framing dense reward learning as a frame-wise temporal-distance prediction problem that can pretrain on passive videos.","experiments":"📊 Results: TimeRewarder achieved nearly perfect success in 9/10 Meta-World tasks with only 200,000 interactions per task. It outperformed prior methods and even the manually designed environment dense reward on both final success rate and sample efficiency.","insights":"🤔 What's next?\n- Research: scale pretraining on large web/human video corpora and study transfer across robot embodiments.\n- Applications: automated dense reward specification for real robots; learning from heterogeneous human videos for new tasks.\nCould large-scale video pretraining replace manual reward engineering?","category":"reinforcement_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"[\n  \"机器人能否从被动视频中学习密集奖励？\",\n  \"TimeRewarder 通过建模视频帧（包括机器人和人类的）之间的时序距离，学习帧级别的任务进度，并提供步进式的代理奖励来指导强化学习（RL），从而实现了机器人领域密集奖励设计的规模化应用。\"\n]","chinese_challenges":"{\n  \"translation\": {\n    \"title\": \"🎯 待解决的挑战/问题：\",\n    \"points\": [\n      \"密集奖励（Dense Rewards）的设计依赖人工，且不具备可扩展性。\",\n      \"稀疏奖励（Sparse Rewards）使得强化学习（RL）在机器人任务中样本效率低下（Sample-Inefficient）。\",\n      \"利用被动式（机器人/人类）视频来提取密集的进度信号（Dense Progress Signals）是极具挑战性的。\"\n    ]\n  }\n}","chinese_innovations":"{\n  \"chinese_translation\": \"✨ 核心思想：\\n- 通过建模视频帧对之间逐帧的时间距离来学习进度。\\n- 从被动视频（机器人演示和人类视频）中推导出密集的进度估计。\\n- 使用这些估计作为逐步的代理奖励来训练强化学习（RL）智能体。\\n创新点：将密集奖励学习构建为一个逐帧时间距离预测问题，该问题可以利用被动视频进行预训练。\"\n}","chinese_experiments":"{\n  \"translation\": \"📊 结果：TimeRewarder 在 10 个 Meta-World 任务中的 9 个任务上实现了近乎完美的成功，且每个任务仅使用了 20 万次交互。它在最终成功率和样本效率两方面，均优于先前的方法，甚至超越了手动设计的环境密集奖励。\"\n}","chinese_insights":"[\n  {\n    \"insights\": \"🤔 下一步是什么？\\n- 研究：在大型网络/人类视频语料库上扩展预训练规模，并研究跨机器人实体（embodiments）的迁移能力。\\n- 应用：为真实机器人自动指定密集奖励；从异构的人类视频中学习新任务。\\n大规模视频预训练能否取代手动奖励工程？\"\n  }\n]","summary":"**Introduction:** 🚀 What if robots could learn dense rewards from passive videos?\nTimeRewarder learns frame-wise task progress by modeling temporal distance between video frames (robot & human) and supplies step-wise proxy rewards to guide RL — scaling dense reward design for robotics.\n\n**Challenges:** 🎯 Problems solved:\n- Designing dense rewards is manual and not scalable.\n- Sparse rewards make RL sample-inefficient for robotic t...","analyzed_at":"2025-10-01T11:00:40.455Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26626v1","arxiv_id":"2509.26626v1","title":"Recursive Self-Aggregation Unlocks Deep Thinking in Large Language\n  Models","abstract":"Test-time scaling methods improve the capabilities of large language models\n(LLMs) by increasing the amount of compute used during inference to make a\nprediction. Inference-time compute can be scaled in parallel by choosing among\nmultiple independent solutions or sequentially through self-refinement. We\npropose Recursive Self-Aggregation (RSA), a test-time scaling method inspired\nby evolutionary methods that combines the benefits of both parallel and\nsequential scaling. Each step of RSA refines a population of candidate\nreasoning chains through aggregation of subsets to yield a population of\nimproved solutions, which are then used as the candidate pool for the next\niteration. RSA exploits the rich information embedded in the reasoning chains\n-- not just the final answers -- and enables bootstrapping from partially\ncorrect intermediate steps within different chains of thought. Empirically, RSA\ndelivers substantial performance gains with increasing compute budgets across\ndiverse tasks, model families and sizes. Notably, RSA enables\nQwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning\nmodels, including DeepSeek-R1 and o3-mini (high), while outperforming purely\nparallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning\nGym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the\nmodel to combine solutions via a novel aggregation-aware reinforcement learning\napproach yields significant performance gains. Code available at\nhttps://github.com/HyperPotatoNeo/RSA.","authors":["Siddarth Venkatraman","Vineet Jain","Sarthak Mittal","Vedant Shah","Johan Obando-Ceron","Yoshua Bengio","Brian R. Bartoldson","Bhavya Kailkhura","Guillaume Lajoie","Glen Berseth","Nikolay Malkin","Moksh Jain"],"published":"2025-09-30T17:58:03Z","updated":"2025-09-30T17:58:03Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26626v1","pdf_url":"http://arxiv.org/pdf/2509.26626v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"🚀 Question: Can small LLMs 'think deeper' at inference time without more training?\nRSA (Recursive Self-Aggregation) is a test-time scaling method that iteratively aggregates and refines populations of reasoning chains to unlock deeper reasoning—letting smaller models compete with bigger ones.","challenges":"🎯 Challenges:\n- Existing test-time scaling is typically either parallel (many independent tries) or sequential (self-refinement), not both.\n- Methods ignore partial correctness in intermediate reasoning steps across chains.\n- Inefficient use of inference compute for bootstrapping stronger solutions.","innovations":"✨ Innovations:\n- Recursive Self-Aggregation (RSA): iteratively refines a population of candidate reasoning chains by aggregating subsets to produce improved candidates.\n- Exploits intermediate chain-of-thought content (not just final answers) to bootstrap.\n- Aggregation-aware reinforcement learning to train models to better combine solutions.","experiments":"📊 Experiment (most compelling result):\nQwen3-4B-Instruct-2507 using RSA achieved competitive performance with larger reasoning models (DeepSeek-R1 and o3-mini (high)) and outperformed purely parallel and purely sequential scaling across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. Specific numeric gains: Not specified in the paper.","insights":"🤔 Insights & next steps:\n- Research: adapt RSA for multimodal or retrieval-augmented LLMs; develop adaptive subset-selection strategies or theory on when aggregation helps.\n- Applications: stronger reasoning in education/tutoring, code synthesis, and decision support—letting smaller models punch above their size. Could RSA enable cheaper, reliable 'deep thinking' at scale?","category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"title\": \"递归自聚合（RSA）：让小型LLM在推理时“深度思考”\",\n  \"content\": \"🚀 问题：小型大语言模型（LLMs）能否在不增加训练量的情况下，在推理时进行“更深层次的思考”？\\n\\n递归自聚合（Recursive Self-Aggregation，简称RSA）是一种测试时（test-time）的扩展方法，它通过迭代地聚合和精炼推理链的集合（populations），以释放更深层次的推理能力——从而使小型模型能够与大型模型竞争。\"\n}","chinese_challenges":"{\n  \"challenges\": \"挑战：\"\n}","chinese_innovations":"{\n  \"innovations\": \"✨ 创新点：\\n- 递归自聚合（Recursive Self-Aggregation, RSA）：通过聚合候选推理链的子集，迭代地精炼推理链的群体，以产生改进的候选。 \\n- 利用中间的思维链内容（而不仅仅是最终答案）进行自举（bootstrap）。\\n- 聚合感知强化学习（Aggregation-aware reinforcement learning）来训练模型更好地结合解决方案。\"\n}","chinese_experiments":"{\n  \"experiment\": \"Qwen3-4B-Instruct-2507 using RSA achieved competitive performance with larger reasoning models (DeepSeek-R1 and o3-mini (high)) and outperformed purely parallel and purely sequential scaling across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. Specific numeric gains: Not specified in the paper.\"\n}","chinese_insights":"[\n  {\n    \"insight_type\": \"Research\",\n    \"description\": \"adapt RSA for multimodal or retrieval-augmented LLMs; develop adaptive subset-selection strategies or theory on when aggregation helps.\"\n  },\n  {\n    \"insight_type\": \"Applications\",\n    \"description\": \"stronger reasoning in education/tutoring, code synthesis, and decision support—letting smaller models punch above their size. Could RSA enable cheaper, reliable 'deep thinking' at scale?\"\n  }\n]","summary":"**Introduction:** 🚀 Question: Can small LLMs 'think deeper' at inference time without more training?\nRSA (Recursive Self-Aggregation) is a test-time scaling method that iteratively aggregates and refines populations of reasoning chains to unlock deeper reasoning—letting smaller models compete with bigger ones.\n\n**Challenges:** 🎯 Challenges:\n- Existing test-time scaling is typically either parallel (many independent tries) or seque...","analyzed_at":"2025-10-01T11:01:47.590Z","model":"openai/gpt-5-mini"}},{"id":"hf_vision_zero__scalable_vlm_self_improvement_via_strategic_gamified_self_play_1759311076551","title":"Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play","abstract":"Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in \\&quot;Who Is the Spy\\&quot;-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model's reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:25:06.144Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;name&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:115}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.8876067996025085},&quot;editors&quot;:[&quot;taesiri&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25541&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9abf&quot;,&quot;name&quot;:&quot;Qinsi Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac0&quot;,&quot;name&quot;:&quot;Bo Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac1&quot;,&quot;name&quot;:&quot;Tianyi Zhou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac2&quot;,&quot;name&quot;:&quot;Jing Shi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac3&quot;,&quot;name&quot;:&quot;Yueqian Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac4&quot;,&quot;name&quot;:&quot;Yiran Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac5&quot;,&quot;name&quot;:&quot;Hai Helen Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac6&quot;,&quot;name&quot;:&quot;Kun Wan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac7&quot;,&quot;name&quot;:&quot;Wentian Zhao&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-29T21:55:55.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:55:06.135Z&quot;,&quot;title&quot;:&quot;Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified\\n Self-Play&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Although reinforcement learning (RL) can effectively enhance the reasoning\\ncapabilities of vision-language models (VLMs), current methods remain heavily\\ndependent on labor-intensive datasets that require extensive manual\\nconstruction and verification, leading to extremely high training costs and\\nconsequently constraining the practical deployment of VLMs. To address this\\nchallenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM\\nself-improvement through competitive visual games generated from arbitrary\\nimage pairs. Specifically, Vision-Zero encompasses three main attributes: (1)\\nStrategic Self-Play Framework: Vision-Zero trains VLMs in \\&quot;Who Is the\\nSpy\\&quot;-style games, where the models engage in strategic reasoning and actions\\nacross multiple roles. Through interactive gameplay, models autonomously\\ngenerate their training data without human annotation. (2) Gameplay from\\nArbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate\\ngames from arbitrary images, thereby enhancing the model's reasoning ability\\nacross diverse domains and showing strong generalization to different tasks. We\\ndemonstrate this versatility using three distinct types of image datasets:\\nCLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable\\nPerformance Gain: We introduce Iterative Self-Play Policy Optimization\\n(Iterative-SPO), a novel training algorithm that alternates between Self-Play\\nand reinforcement learning with verifiable rewards (RLVR), mitigating the\\nperformance plateau often seen in self-play-only training and achieving\\nsustained long-term improvements. Despite using label-free data, Vision-Zero\\nachieves state-of-the-art performance on reasoning, chart question answering,\\nand vision-centric understanding tasks, surpassing other annotation-based\\nmethods. Models and code has been released at\\nhttps://github.com/wangqinsi1/Vision-Zero.&quot;,&quot;upvotes&quot;:57,&quot;discussionId&quot;:&quot;68dc90f34159d1f2418f9ac8&quot;,&quot;githubRepo&quot;:&quot;https://github.com/wangqinsi1/Vision-Zero&quot;,&quot;ai_summary&quot;:&quot;Vision-Zero is a domain-agnostic framework that enhances vision-language models through self-improvement in competitive visual games, using Iterative Self-Play Policy Optimization and achieving state-of-the-art performance without human annotation.&quot;,&quot;ai_keywords&quot;:[&quot;reinforcement learning&quot;,&quot;vision-language models&quot;,&quot;strategic self-play framework&quot;,&quot;self-play&quot;,&quot;reinforcement learning with verifiable rewards&quot;,&quot;Iterative Self-Play Policy Optimization&quot;],&quot;githubStars&quot;:6},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66128e8b7e0e7a64652dbbdf&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b72ea5b14b55ff3af920c06b69a60b3f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wang&quot;,&quot;user&quot;:&quot;Qinsi1&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;635e3a76106f984574c36409&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Bo Liu&quot;,&quot;user&quot;:&quot;Benjamin-eecs&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66443629b23fe8d3f7f2d0c7&quot;,&quot;avatarUrl&quot;:&quot;/avatars/98ff088036aa382f33a05c232604c565.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wentian Zhao&quot;,&quot;user&quot;:&quot;zwt123home123&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66274e02348a5304435dc9cc&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bda87559cd497c310597c2fc8430b31f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kun Wan&quot;,&quot;user&quot;:&quot;timecuriosity&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68da09e4d8b96845c2091d16&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bcfc23b8f15ba09554765ca9fd78ee24.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Qiuyuan Song&quot;,&quot;user&quot;:&quot;qsongshop&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68db3ff5153d1470c35ed4fb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/11cd11b2ac430ade73e13567de387065.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Mingyu Jin&quot;,&quot;user&quot;:&quot;JimmyNLP&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6670b3b78eac2e222ebf77d4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0f1d231eac479ca78ddf106a72490faa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Guofeng Cui&quot;,&quot;user&quot;:&quot;gfcui&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6245285af59b8d262df3321b&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6245285af59b8d262df3321b/dvy__dTf-miJ60IbveDg4.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yifan Zeng&quot;,&quot;user&quot;:&quot;yokey&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;647fa485d0cd8be13e662973&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/647fa485d0cd8be13e662973/_QLgqLWhWir8_bkDCdTx6.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Qingyun Wu&quot;,&quot;user&quot;:&quot;qingyun-wu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;647f5af5b0e96764589f3b2a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tianyi Zhou&quot;,&quot;user&quot;:&quot;zhoutianyi&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64d1e9211139ff0887b536a1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/638aff8b38df98aeffa10f41a67b39ea.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Alan Zhang&quot;,&quot;user&quot;:&quot;AlanStarkZ&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:1}\"> Papers arxiv:2509.25541","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:16.551Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.25541","pdf_url":"","scraped_at":"2025-10-01T09:31:16.551Z","abstract_quality":6,"analysis":{"introduction":"🚀 Question: Can a VLM improve itself without expensive labels? Vision-Zero introduces a domain-agnostic self-play framework where VLMs compete in “Who Is the Spy” style visual games on arbitrary image pairs to auto-generate training data and boost reasoning.","challenges":"🎯 Key problems tackled:\n- Heavy reliance on labor-intensive, manually annotated datasets.\n- Gamified/self-play methods limited to specific image domains, hurting generalization.\n- Self-play-only training often plateaus and stops improving model performance.","innovations":"✨ Core contributions:\n- Strategic self-play: train VLMs via multi-role \\","experiments":"📊 Not specified in the paper for exact quantitative numbers. \nHowever, experiments (CLEVR-style scenes, charts, and real-world images) show Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding—surpassing annotation-based methods. Code/models released on GitHub.","insights":"🤔 Future directions & implications:\n- Extend to interactive multimodal agents or incorporate human-in-the-loop verification to refine self-play data.\n- Apply to domains needing low-label regimes (e.g., scientific charts, robotics perception) and study robustness to adversarial visuals.\nCould this enable continuous, label-free VLM updates in deployed systems?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"chinese_translation\": \"🚀 问题：视觉语言模型（VLM）能否在没有昂贵标签的情况下自我提升？Vision-Zero 引入了一个与领域无关的自博弈框架，其中 VLM 在任意图像对上参与“谁是间谍”式的视觉游戏，以自动生成训练数据并增强推理能力。\"\n}","chinese_challenges":"{\n  \"challenges\": \"🎯 重点解决的问题：\\n- 过度依赖劳动密集型的手动标注数据集。\\n- 游戏化/自博弈方法局限于特定的图像领域，损害了泛化能力。\\n- 仅依赖自博弈的训练经常会停滞不前，停止提升模型性能。\"\n}","chinese_innovations":"{\n  \"translation\": \"✨ 核心贡献：\\n- 策略性自我博弈（Strategic self-play）：通过多角色训练VLM\"\n}","chinese_experiments":"{\n  \"translation\": \"📊 论文中未给出确切的定量数据。然而，实验（在CLEVR风格的场景、图表和真实世界图像上进行）表明，Vision-Zero在推理、图表问答和以视觉为中心的理解方面取得了最先进的性能，超越了基于标注的方法。代码和模型已在GitHub上发布。\"\n}","chinese_insights":"{\n  \"future_directions_and_implications\": [\n    {\n      \"key_insight\": \"Extend to interactive multimodal agents or incorporate human-in-the-loop verification to refine self-play data.\",\n      \"translation\": \"扩展到交互式多模态智能体，或整合人在环（human-in-the-loop）验证机制，以优化自博弈数据。\"\n    },\n    {\n      \"key_insight\": \"Apply to domains needing low-label regimes (e.g., scientific charts, robotics perception) and study robustness to adversarial visuals.\",\n      \"translation\": \"应用于需要低标签样本的领域（例如科学图表、机器人感知），并研究其对对抗性视觉输入的鲁棒性。\"\n    },\n    {\n      \"key_insight\": \"Could this enable continuous, label-free VLM updates in deployed systems?\",\n      \"translation\": \"这是否能够实现对已部署系统中的视觉语言模型（VLM）进行持续、无标签的更新？\"\n    }\n  ]\n}","summary":"**Introduction:** 🚀 Question: Can a VLM improve itself without expensive labels? Vision-Zero introduces a domain-agnostic self-play framework where VLMs compete in “Who Is the Spy” style visual games on arbitrary image pairs to auto-generate training data and boost reasoning.\n\n**Challenges:** 🎯 Key problems tackled:\n- Heavy reliance on labor-intensive, manually annotated datasets.\n- Gamified/self-play methods limited to specific i...","analyzed_at":"2025-10-01T11:14:42.310Z","model":"openai/gpt-5-mini"}},{"id":"hf_truthrl__incentivizing_truthful_llms_via_reinforcement_learning_1759311079944","title":"TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning","abstract":"While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy---models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs. We also experiment with more complicated reward designs, such as knowledge-enhanced and reasoning-enhanced variants, and show that a simple ternary reward scheme generally performs better. Moreover, we find the improvement of TruthRL arises from enhancing the capability of LLMs to recognize their knowledge boundary, hence avoiding being overly conservative as the baselines are. Further analysis confirms that TruthRL is robust to hallucination-baiting questions and more confident in producing accurate responses.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:26:26.198Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;6526307af06ac0cf9a922e86&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;,&quot;fullname&quot;:&quot;Zhepei Wei&quot;,&quot;name&quot;:&quot;weizhepei&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9195460677146912},&quot;editors&quot;:[&quot;weizhepei&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;👍&quot;,&quot;users&quot;:[&quot;sytmr&quot;,&quot;TianHongZXY&quot;,&quot;WZDavid&quot;],&quot;count&quot;:3},{&quot;reaction&quot;:&quot;🔥&quot;,&quot;users&quot;:[&quot;WZDavid&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25760&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aa7&quot;,&quot;name&quot;:&quot;Zhepei Wei&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aa8&quot;,&quot;name&quot;:&quot;Xiao Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aa9&quot;,&quot;name&quot;:&quot;Kai Sun&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aaa&quot;,&quot;name&quot;:&quot;Jiaqi Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aab&quot;,&quot;name&quot;:&quot;Rulin Shao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aac&quot;,&quot;name&quot;:&quot;Sean Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aad&quot;,&quot;name&quot;:&quot;Mohammad Kachuee&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aae&quot;,&quot;name&quot;:&quot;Teja Gollapudi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aaf&quot;,&quot;name&quot;:&quot;Tony Liao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab0&quot;,&quot;name&quot;:&quot;Nicolas Scheffer&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab1&quot;,&quot;name&quot;:&quot;Rakesh Wanga&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab2&quot;,&quot;name&quot;:&quot;Anuj Kumar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab3&quot;,&quot;name&quot;:&quot;Yu Meng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab4&quot;,&quot;name&quot;:&quot;Wen-tau Yih&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab5&quot;,&quot;name&quot;:&quot;Xin Luna Dong&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-30T04:25:17.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:56:26.188Z&quot;,&quot;title&quot;:&quot;TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6526307af06ac0cf9a922e86&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhepei Wei&quot;,&quot;user&quot;:&quot;weizhepei&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;While large language models (LLMs) have demonstrated strong performance on\\nfactoid question answering, they are still prone to hallucination and\\nuntruthful responses, particularly when tasks demand information outside their\\nparametric knowledge. Indeed, truthfulness requires more than accuracy --\\nmodels must also recognize uncertainty and abstain when unsure to avoid\\nhallucinations. This presents a fundamental challenge for existing methods:\\napproaches that optimize for accuracy often amplify hallucinations, while those\\nthat encourage abstention can become overly conservative, sacrificing correct\\nanswers. Both extremes ultimately compromise truthfulness. In this work, we\\npresent TruthRL, a general reinforcement learning (RL) framework that directly\\noptimizes the truthfulness of LLMs. Specifically, we implement TruthRL using\\nGRPO with a simple yet effective ternary reward that distinguishes correct\\nanswers, hallucinations, and abstentions. It incentivizes models to reduce\\nhallucinations not only by providing correct responses, but also by enabling\\nabstention when uncertain, thereby improving truthfulness. Extensive\\nexperiments across four knowledge-intensive benchmarks show that, compared to\\nvanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves\\ntruthfulness by 21.1%, with consistent gains across various backbone models\\n(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth\\nablation study demonstrates that vanilla accuracy-driven methods, such as\\nsupervised fine-tuning or RL with a binary reward, struggle to balance factual\\ncorrectness and uncertainty. In contrast, our proposed truthfulness-driven\\nTruthRL achieves strong performance in both accuracy and truthfulness,\\nunderscoring the importance of learning objective design for developing\\ntruthful LLMs.&quot;,&quot;upvotes&quot;:34,&quot;discussionId&quot;:&quot;68dc90b84159d1f2418f9ab6&quot;,&quot;ai_summary&quot;:&quot;TruthRL, a reinforcement learning framework, enhances the truthfulness of large language models by balancing accuracy and abstention, significantly reducing hallucinations and improving performance across benchmarks.&quot;,&quot;ai_keywords&quot;:[&quot;large language models&quot;,&quot;LLMs&quot;,&quot;hallucination&quot;,&quot;untruthful responses&quot;,&quot;parametric knowledge&quot;,&quot;truthfulness&quot;,&quot;reinforcement learning&quot;,&quot;RL&quot;,&quot;GRPO&quot;,&quot;ternary reward&quot;,&quot;abstention&quot;,&quot;accuracy-driven methods&quot;,&quot;supervised fine-tuning&quot;,&quot;binary reward&quot;,&quot;knowledge-intensive benchmarks&quot;,&quot;Qwen&quot;,&quot;Llama&quot;,&quot;retrieval&quot;,&quot;non-retrieval setups&quot;,&quot;ablation study&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;6526307af06ac0cf9a922e86&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhepei Wei&quot;,&quot;user&quot;:&quot;weizhepei&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62ea79dd01ed9b0e8f61ccd3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chengsong Huang&quot;,&quot;user&quot;:&quot;ChengsongHuang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;617aec6f6f37340367d5d7a1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/afa58f39896c5caef512675450c7d6ce.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yu Meng&quot;,&quot;user&quot;:&quot;yumeng5&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6602787b1827b6d37ee527be&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f69c1779b4347a042dad1a0d962145af.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tzu-Han Lin&quot;,&quot;user&quot;:&quot;hank0316&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6587e5a4b2177de3967ff434&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f2dfbc44eb2bff8d8d66d26db8539708.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shuyao Xu&quot;,&quot;user&quot;:&quot;Tim-Xu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66080ddac201aee890e5efeb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/dae8f44b2eda9d1950efaa10a5aa986f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jing Chen&quot;,&quot;user&quot;:&quot;jingchen6688&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;623b290048f658f28aef79f7&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1648044277149-noauth.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xinyu Zhu&quot;,&quot;user&quot;:&quot;TianHongZXY&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6633f39185b05e9a8e7c549c&quot;,&quot;avatarUrl&quot;:&quot;/avatars/ee4df68daee8b6637d7ad86cba29cc2f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;shiyu&quot;,&quot;user&quot;:&quot;sytmr&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;664e88ad8ab2524c036c3d2f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/HItpTu75SFqA5ouOMKzVb.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhining Liu&quot;,&quot;user&quot;:&quot;ZhiningLiu1998&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;619e6657cc04eadf54fa5d2d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c78c54767c63c75a9f6783ffa78a98fa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wei-Lin Chen&quot;,&quot;user&quot;:&quot;wlchen&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6590a65b89f1ff0463828e53&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4ab424eede2fe9c114252b1e5dd1ba25.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;sizhe&quot;,&quot;user&quot;:&quot;sizhe04&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;647a248bed75e95d3e98e3d6&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e10e2f8516d1451fd85e17b5a0ba978d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yaochen Zhu&quot;,&quot;user&quot;:&quot;yaochenzhu&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:2,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;5e63d8713071d5be688861b8&quot;,&quot;name&quot;:&quot;facebook&quot;,&quot;fullname&quot;:&quot;AI at Meta&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png&quot;}}\"> Papers arxiv:2509.25760","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:19.945Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.25760","pdf_url":"","scraped_at":"2025-10-01T09:31:19.945Z","abstract_quality":6,"analysis":{"introduction":"🚀 Question: How do we make LLMs say “I don’t know” instead of confidently hallucinating?\nTruthRL is an RL framework (GRPO + a simple ternary reward) that trains models to answer correctly or abstain, cutting hallucinations and improving truthfulness across backbones & setups.","challenges":"🎯 Key problems tackled:\n- Models hallucinate on facts outside parametric knowledge.\n- Accuracy-driven training amplifies confident but wrong answers.\n- Abstention-focused methods become overly conservative and lose correct answers.","innovations":"✨ Core innovations:\n- TruthRL: an RL framework optimizing truthfulness directly.\n- Implemented with GRPO and a ternary reward (correct / hallucination / abstain).\n- Novelty: reward explicitly balances giving correct answers and abstaining when uncertain.","experiments":"📊 Results: TruthRL reduced hallucinations by 28.9% and improved truthfulness by 21.1% across four knowledge-intensive benchmarks, with consistent gains across backbones (Qwen, Llama) and both retrieval & non-retrieval setups — showing the ternary reward’s practical impact.","insights":"🤔 What’s next?\n- Explore combining TruthRL with stronger calibrated uncertainty estimators or multi-turn dialogue to improve abstention in interactive settings.\n- Broader applications: high-stakes domains (medicine, law) and retrieval-augmented systems where avoiding hallucination matters most.\nCould TruthRL become a standard for safe QA?","category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"\"🚀 问题：我们如何让大型语言模型（LLMs）说出“我不知道”，而不是自信地给出幻觉内容？TruthRL是一个强化学习（RL）框架（由GRPO和一个简单的三元奖励机制构成），它训练模型要么正确回答，要么选择弃权，从而减少幻觉现象，并在各种基础模型和配置中提高真实性。\"","chinese_challenges":"{\n  \"关键挑战\": [\n    \"模型对超出其参数化知识范围的事实产生“幻觉”（虚假信息）。\",\n    \"以准确率（精度）为导向的训练会加剧模型对错误答案的过度自信。\",\n    \"以弃权（拒绝回答）为核心的方法往往过于保守，导致错失正确的答案。\"\n  ]\n}","chinese_innovations":"\"核心创新点：\\n- TruthRL：一种直接优化“真实性”（truthfulness）的强化学习（RL）框架。\\n- 使用GRPO算法和三元奖励机制（正确 / 幻觉 / 弃权）实现。\\n- 创新点：奖励机制明确地平衡了“给出正确答案”和“在不确定时选择弃权”这两种行为。\"","chinese_experiments":"{\n  \"analysis_type\": \"research_summary\",\n  \"model_name\": \"TruthRL\",\n  \"metrics\": [\n    {\n      \"metric\": \"hallucinations_reduction\",\n      \"value\": 0.289,\n      \"unit\": \"percentage\"\n    },\n    {\n      \"metric\": \"truthfulness_improvement\",\n      \"value\": 0.211,\n      \"unit\": \"percentage\"\n    }\n  ],\n  \"scope\": {\n    \"benchmarks\": \"four knowledge-intensive benchmarks\",\n    \"backbones\": [\n      \"Qwen\",\n      \"Llama\"\n    ],\n    \"setups\": [\n      \"retrieval\",\n      \"non-retrieval\"\n    ]\n  },\n  \"conclusion\": \"The ternary reward mechanism demonstrated practical impact through consistent gains across diverse configurations.\"\n}","chinese_insights":"{\n  \"insights\": [\n    \"🤔 下一步是什么？\",\n    \"探索将TruthRL与更强的校准不确定性估计器或多轮对话相结合，以改进在交互式设置中的拒绝回答（Abstention）能力。\",\n    \"更广泛的应用：在高风险领域（如医疗、法律）以及避免“幻觉”（Hallucination）至关重要的检索增强系统中。\",\n    \"TruthRL能否成为安全问答（QA）的标准？\"\n  ]\n}","summary":"**Introduction:** 🚀 Question: How do we make LLMs say “I don’t know” instead of confidently hallucinating?\nTruthRL is an RL framework (GRPO + a simple ternary reward) that trains models to answer correctly or abstain, cutting hallucinations and improving truthfulness across backbones & setups.\n\n**Challenges:** 🎯 Key problems tackled:\n- Models hallucinate on facts outside parametric knowledge.\n- Accuracy-driven training amplifies c...","analyzed_at":"2025-10-01T11:16:31.375Z","model":"openai/gpt-5-mini"}},{"id":"hf_oceangym__a_benchmark_environment_for_underwater_embodied_agents_1759311082964","title":"OceanGym: A Benchmark Environment for Underwater Embodied Agents","abstract":"We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:41:27.213Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;620b3bbb0668e435407c8d0a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;,&quot;fullname&quot;:&quot;Ningyu Zhang&quot;,&quot;name&quot;:&quot;Ningyu&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:30}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.930759847164154},&quot;editors&quot;:[&quot;Ningyu&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.26536&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af3&quot;,&quot;name&quot;:&quot;Yida Xue&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af4&quot;,&quot;name&quot;:&quot;Mingjun Mao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af5&quot;,&quot;name&quot;:&quot;Xiangyuan Ru&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af6&quot;,&quot;name&quot;:&quot;Yuqi Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af7&quot;,&quot;name&quot;:&quot;Baochang Ren&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af8&quot;,&quot;name&quot;:&quot;Shuofei Qiao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af9&quot;,&quot;name&quot;:&quot;Mengru Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afa&quot;,&quot;name&quot;:&quot;Shumin Deng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afb&quot;,&quot;name&quot;:&quot;Xinyu An&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afc&quot;,&quot;name&quot;:&quot;Ningyu Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afd&quot;,&quot;name&quot;:&quot;Ying Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afe&quot;,&quot;name&quot;:&quot;Huajun Chen&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/1Nye1TFWd66krDhvPXjQ1.png&quot;],&quot;publishedAt&quot;:&quot;2025-09-30T17:09:32.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T01:11:27.179Z&quot;,&quot;title&quot;:&quot;OceanGym: A Benchmark Environment for Underwater Embodied Agents&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;620b3bbb0668e435407c8d0a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Ningyu Zhang&quot;,&quot;user&quot;:&quot;Ningyu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We introduce OceanGym, the first comprehensive benchmark for ocean underwater\\nembodied agents, designed to advance AI in one of the most demanding real-world\\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\\nextreme perceptual and decision-making challenges, including low visibility,\\ndynamic ocean currents, making effective agent deployment exceptionally\\ndifficult. OceanGym encompasses eight realistic task domains and a unified\\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\\nintegrates perception, memory, and sequential decision-making. Agents are\\nrequired to comprehend optical and sonar data, autonomously explore complex\\nenvironments, and accomplish long-horizon objectives under these harsh\\nconditions. Extensive experiments reveal substantial gaps between\\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\\npersistent difficulty of perception, planning, and adaptability in ocean\\nunderwater environments. By providing a high-fidelity, rigorously designed\\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\\ntransferring these capabilities to real-world autonomous ocean underwater\\nvehicles, marking a decisive step toward intelligent agents capable of\\noperating in one of Earth's last unexplored frontiers. The code and data are\\navailable at https://github.com/OceanGPT/OceanGym.&quot;,&quot;upvotes&quot;:25,&quot;discussionId&quot;:&quot;68dc947d4159d1f2418f9aff&quot;,&quot;projectPage&quot;:&quot;https://oceangpt.github.io/OceanGym/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/OceanGPT/OceanGym&quot;,&quot;ai_summary&quot;:&quot;OceanGym is a benchmark for underwater embodied agents using Multi-modal Large Language Models to address challenges in perception, planning, and adaptability in harsh ocean environments.&quot;,&quot;ai_keywords&quot;:[&quot;Multi-modal Large Language Models&quot;,&quot;MLLMs&quot;,&quot;optical data&quot;,&quot;sonar data&quot;,&quot;sequential decision-making&quot;,&quot;embodied AI&quot;,&quot;autonomous ocean underwater vehicles&quot;],&quot;githubStars&quot;:25},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;620b3bbb0668e435407c8d0a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Ningyu Zhang&quot;,&quot;user&quot;:&quot;Ningyu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67f371b807a3da4558f803c1&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GilM_Vf65qvwuW5-uj9aG.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Twain Wu&quot;,&quot;user&quot;:&quot;1wtw1&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64db65ee1d19239f50674cbe&quot;,&quot;avatarUrl&quot;:&quot;/avatars/dd96cec8d0c10f52a89b25d65728738d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;xueyida&quot;,&quot;user&quot;:&quot;xyd123&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68d8fc00ff474874c83a1c99&quot;,&quot;avatarUrl&quot;:&quot;/avatars/17e3a2f5197274536bf68d949c5416db.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;huminclu&quot;,&quot;user&quot;:&quot;huminclu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;658ead753ce574ff3c339a64&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1dd7671e8af5e7241ef47a6de5503c53.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;sunnychenxiwang&quot;,&quot;user&quot;:&quot;sunnychenxiwang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6698c1c3157ceb76c48ff996&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2f1d732c4d9df4f5b554268ee1949dda.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;徐步强&quot;,&quot;user&quot;:&quot;Xubqpanda&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6736aebbbbc5d5471ee57218&quot;,&quot;avatarUrl&quot;:&quot;/avatars/45e3b017fc6a07e10a42b81cfa349b3f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yi Zhong&quot;,&quot;user&quot;:&quot;HongdouNI233&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;646449beeca41ed5029d1630&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7992d9a62e8d218ec3200d74af9ab5c4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xiang Chen&quot;,&quot;user&quot;:&quot;yyfenglin&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;684bc1be17ae31ba66171292&quot;,&quot;avatarUrl&quot;:&quot;/avatars/99ea28d4ed2ef6c4e35fd26c64472e49.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jingsheng Zheng&quot;,&quot;user&quot;:&quot;JohnsonZheng03&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68415d7b911d1b3135fcca88&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HE3ptFNTlvoWmG3p3f2Cs.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Qihailiantang&quot;,&quot;user&quot;:&quot;Qihailiantang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;679a01a99893a68681ef1847&quot;,&quot;avatarUrl&quot;:&quot;/avatars/17fe173acda467df2b90cca9e5f3c656.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;ye&quot;,&quot;user&quot;:&quot;haohaojun&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65cad52fd6c974694fc20b8e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/8232a7c5db590ed26751a47c45d481b8.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xinle Deng&quot;,&quot;user&quot;:&quot;Linear-Matrix-Probability&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:3,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6345aadf5efccdc07f1365a5&quot;,&quot;name&quot;:&quot;ZhejiangUniversity&quot;,&quot;fullname&quot;:&quot;Zhejiang University&quot;}}\"> Papers arxiv:2509.26536","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:22.964Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.26536","pdf_url":"","scraped_at":"2025-10-01T09:31:22.964Z","abstract_quality":6,"analysis":{"introduction":"🚀 What if autonomous agents could explore the deep sea like rovers explore Mars?\nOceanGym is the first comprehensive benchmark for underwater embodied agents — 8 realistic task domains + a unified MLLM-driven agent framework. Designed to push robust AI for AUVs. https://github.com/OceanGPT/OceanGym","challenges":"🎯 Key challenges OceanGym tackles:\n- Low visibility & noisy sensors that hinder perception\n- Dynamic ocean currents complicating decision-making and control\n- Multimodal (optical + sonar) long‑horizon tasks requiring memory and planning","innovations":"✨ What OceanGym introduces:\n- A high‑fidelity benchmark with 8 realistic underwater task domains\n- A unified, MLLM-driven agent framework integrating perception, memory, sequential decision-making\n- Tasks demanding optical+sonar fusion and long‑horizon autonomous exploration\nNovelty: first comprehensive underwater embodied benchmark.","experiments":"📊 Quantitative result: Not specified in the paper.\nQualitative proof: Extensive experiments reveal substantial gaps between state‑of‑the‑art MLLM-driven agents and human experts, showing persistent difficulty in perception, planning, and adaptability. Code/data: github.com/OceanGPT/OceanGym","insights":"🤔 What's next?\n- Improve sim‑to‑real transfer and dedicated sonar–vision fusion models\n- Explore curriculum/hierarchical planning for long‑horizon underwater tasks\nApplications: autonomous oceanographic surveys, search & rescue, infrastructure inspection. Could OceanGym speed real-world AUV deployment?","keywords":["OceanGym","Multi-modal Large Language Models","MLLMs","underwater robotics","sonar","optical imaging","embodied AI","long-horizon planning","benchmark"],"category":"robotics","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"translation\": \"🚀 如果自主智能体能像漫游车探索火星一样探索深海，那会怎样？OceanGym是首个针对水下具身智能体的综合基准——它包含8个现实任务领域和一个统一的MLLM驱动的智能体框架。旨在推动自主水下航行器（AUVs）的鲁棒（稳健）AI发展。https://github.com/OceanGPT/OceanGym\"\n}","chinese_challenges":"\"OceanGym 应对的关键挑战：\\n- 低能见度与传感器噪声，严重阻碍了环境感知。\\n- 动态洋流使决策和控制过程变得复杂。\\n- 需要记忆和规划的多模态（光学 + 声呐）长程任务。\"","chinese_innovations":"{\n  \"innovations\": [\n    {\n      \"title\": \"OceanGym 引入了什么：\",\n      \"points\": [\n        \"一个高保真基准，包含 8 个逼真的水下任务领域\",\n        \"一个统一的、由 MLLM（多模态大语言模型）驱动的智能体框架，集成了感知、记忆和序列决策能力\",\n        \"要求光学与声纳融合以及长周期自主探索的任务\"\n      ],\n      \"novelty\": \"新颖性：首个全面的水下具身基准。\"\n    }\n  ]\n}","chinese_experiments":"{\n  \"定量结果\": \"论文中未明确说明。\",\n  \"定性证据\": \"广泛的实验揭示了最先进的 MLLM 驱动智能体与人类专家之间存在显著差距，表明其在感知、规划和适应性方面仍面临持续的困难。\",\n  \"代码/数据\": \"github.com/OceanGPT/OceanGym\"\n}","chinese_insights":"{\n  \"insights\": {\n    \"next_steps\": [\n      \"Improve sim-to-real transfer and dedicated sonar–vision fusion models\",\n      \"Explore curriculum/hierarchical planning for long-horizon underwater tasks\"\n    ],\n    \"applications\": [\n      \"autonomous oceanographic surveys\",\n      \"search & rescue\",\n      \"infrastructure inspection\"\n    ],\n    \"question\": \"Could OceanGym speed real-world AUV deployment?\"\n  }\n}","summary":"**Introduction:** 🚀 What if autonomous agents could explore the deep sea like rovers explore Mars?\nOceanGym is the first comprehensive benchmark for underwater embodied agents — 8 realistic task domains + a unified MLLM-driven agent framework. Designed to push robust AI for AUVs. https://github.com/OceanGPT/OceanGym\n\n**Challenges:** 🎯 Key challenges OceanGym tackles:\n- Low visibility & noisy sensors that hinder perception\n- Dynami...","analyzed_at":"2025-10-01T11:19:11.261Z","model":"openai/gpt-5-mini"}},{"id":"hf_mcpmark__a_benchmark_for_stress_testing_realistic_and_comprehensive_mcp_use_1759311085683","title":"MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use","abstract":"30 models through 127 CRUD-heavy tasks across 5 MCP servers, with a minimal but general MCPMark-Agent ensuring fair comparison.\\r\\nResults: even the best models cap at 52.56% pass@1 / 33.86% pass^4, while other strong systems like claude-sonnet-4 and o3 stay under 30% pass@1.\\r\\nWe break down why — from implicit errors and context drift to cost-performance tradeoffs.\\r\\n\\r\\n👉 Paper: https://arxiv.org/pdf/2509.24002\\r\\n👉 Website: https://mcpmark.ai/\\r\\n👉 Code: https://github.com/eval-sys/mcpmark&quot;,&quot;html&quot;:&quot;Agents can call tools — but can they actually deliver?MCPMark stress-tested &amp;gt;30 models through 127 CRUD-heavy tasks across 5 MCP servers, with a minimal but general MCPMark-Agent ensuring fair comparison.Results: even the best models cap at 52.56% pass@1 / 33.86% pass^4, while other strong systems like claude-sonnet-4 and o3 stay under 30% pass@1.We break down why — from implicit errors and context drift to cost-performance tradeoffs.\\n👉 Paper: https://arxiv.org/pdf/2509.24002👉 Website: https://mcpmark.ai/👉 Code: https://github.com/eval-sys/mcpmark\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T03:28:54.371Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;626d268d5f7327906f05cad1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/18bda74612a3ee63a17f991bcc695106.svg&quot;,&quot;fullname&quot;:&quot;Zijian Wu&quot;,&quot;name&quot;:&quot;Jakumetsu&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:4}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.79066002368927},&quot;editors&quot;:[&quot;Jakumetsu&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/18bda74612a3ee63a17f991bcc695106.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.24002&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b97&quot;,&quot;name&quot;:&quot;Zijian Wu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b98&quot;,&quot;name&quot;:&quot;Xiangyan Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b99&quot;,&quot;name&quot;:&quot;Xinyuan Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b9a&quot;,&quot;name&quot;:&quot;Lingjun Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b9b&quot;,&quot;name&quot;:&quot;Fanqing Meng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b9c&quot;,&quot;name&quot;:&quot;Lingxiao Du&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b9d&quot;,&quot;name&quot;:&quot;Yiran Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b9e&quot;,&quot;name&quot;:&quot;Fanshi Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b9f&quot;,&quot;name&quot;:&quot;Yaoqi Ye&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9ba0&quot;,&quot;name&quot;:&quot;Jiawei Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9ba1&quot;,&quot;name&quot;:&quot;Zirui Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9ba2&quot;,&quot;name&quot;:&quot;Jinjie Ni&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9ba3&quot;,&quot;name&quot;:&quot;Yufan Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9ba4&quot;,&quot;name&quot;:&quot;Arvin Xu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9ba5&quot;,&quot;name&quot;:&quot;Michael Qizhe Shieh&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-28T17:53:27.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T01:58:54.337Z&quot;,&quot;title&quot;:&quot;MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP\\n Use&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;626d268d5f7327906f05cad1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/18bda74612a3ee63a17f991bcc695106.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Zijian Wu&quot;,&quot;user&quot;:&quot;Jakumetsu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;MCP standardizes how LLMs interact with external systems, forming the\\nfoundation for general agents. However, existing MCP benchmarks remain narrow\\nin scope: they focus on read-heavy tasks or tasks with limited interaction\\ndepth, and fail to capture the complexity and realism of real-world workflows.\\nTo address this gap, we propose MCPMark, a benchmark designed to evaluate MCP\\nuse in a more realistic and comprehensive manner. It consists of 127\\nhigh-quality tasks collaboratively created by domain experts and AI agents.\\nEach task begins with a curated initial state and includes a programmatic\\nscript for automatic verification. These tasks demand richer and more diverse\\ninteractions with the environment, involving a broad range of create, read,\\nupdate, and delete (CRUD) operations. We conduct a comprehensive evaluation of\\ncutting-edge LLMs using a minimal agent framework that operates in a\\ntool-calling loop. Empirical results show that the best-performing model,\\ngpt-5-medium, reaches only 52.56\\\\% pass@1 and 33.86\\\\% pass^4, while other\\nwidely regarded strong models, including claude-sonnet-4 and o3, fall below\\n30\\\\% pass@1 and 15\\\\% pass^4. On average, LLMs require 16.2 execution\\nturns and 17.4 tool calls per task, significantly surpassing those in\\nprevious MCP benchmarks and highlighting the stress-testing nature of MCPMark.&quot;,&quot;upvotes&quot;:25,&quot;discussionId&quot;:&quot;68dc9eff4159d1f2418f9ba6&quot;,&quot;projectPage&quot;:&quot;https://mcpmark.ai/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/eval-sys/mcpmark&quot;,&quot;ai_summary&quot;:&quot;MCPMark is a comprehensive benchmark for evaluating MCP use in real-world workflows, featuring diverse tasks that require richer interactions with the environment, and reveals that current LLMs perform poorly on these tasks.&quot;,&quot;ai_keywords&quot;:[&quot;MCP&quot;,&quot;LLMs&quot;,&quot;general agents&quot;,&quot;MCP benchmarks&quot;,&quot;MCPMark&quot;,&quot;high-quality tasks&quot;,&quot;domain experts&quot;,&quot;AI agents&quot;,&quot;initial state&quot;,&quot;programmatic script&quot;,&quot;automatic verification&quot;,&quot;CRUD operations&quot;,&quot;minimal agent framework&quot;,&quot;tool-calling loop&quot;,&quot;gpt-5-medium&quot;,&quot;claude-sonnet-4&quot;,&quot;o3&quot;,&quot;pass@1&quot;,&quot;pass^4&quot;,&quot;execution turns&quot;,&quot;tool calls&quot;],&quot;githubStars&quot;:175},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;626d268d5f7327906f05cad1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/18bda74612a3ee63a17f991bcc695106.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Zijian Wu&quot;,&quot;user&quot;:&quot;Jakumetsu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68ac2314d4a7fd30a2ec0035&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2b927fedd6cc376807a771e37124e331.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhiwei Xue&quot;,&quot;user&quot;:&quot;ZackAXue&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6400c9a1210dabb7d9301192&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f586047b543814dd6301c05d0bbd72ae.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tianyi Wu&quot;,&quot;user&quot;:&quot;awsuineg&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;666fe1a5b07525f0bde69c27&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bb98ab0b974c8fe011739baa8dadd91a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Lingxiao Du&quot;,&quot;user&quot;:&quot;Cierra0506&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6486b09e8315b19342f0bf5e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bc5f22f231c884146d373fe1042d81bd.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xiangyan Liu&quot;,&quot;user&quot;:&quot;xyliu6&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6650c77a74664a42ddfb9187&quot;,&quot;avatarUrl&quot;:&quot;/avatars/92001bbe0ae9b14309730316b639cede.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;yueliu1999&quot;,&quot;user&quot;:&quot;yueliu1999&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64060b49a577649430bf6974&quot;,&quot;avatarUrl&quot;:&quot;/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jiawei Wang&quot;,&quot;user&quot;:&quot;Jarvis1111&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67eec9446d0de3e1f28898bf&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a62db483fc63f1d4bddb573d058d42db.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;liu&quot;,&quot;user&quot;:&quot;xiao-hao&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6864a4028ce1cecf472692a0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4e267702c16c6ef4371a58fafd948fb9.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;tan wang&quot;,&quot;user&quot;:&quot;liekkas99&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64a026fb53158a718064b10e&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Rv66ntQxEE6Le7VxXMDpR.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chen jy&quot;,&quot;user&quot;:&quot;cjy324&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;683fb1132548830f5d65fc6f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/683fb1132548830f5d65fc6f/Xjasc-yrB0BdZpGKP4TVq.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hui Li&quot;,&quot;user&quot;:&quot;Gray1y&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67d97c49baae1d3e511bf777&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2d9f5d5484f32ff12b5e81c38fb94e5b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;yang&quot;,&quot;user&quot;:&quot;livanivter&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0}\"> Papers arxiv:2509.24002","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:25.683Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.24002","pdf_url":"","scraped_at":"2025-10-01T09:31:25.683Z","abstract_quality":6,"analysis":{"introduction":"🚀 Did you know the best agents only pass half the time? 52.56% pass@1 is the ceiling found.\nMCPMark: a new benchmark that stress-tests realistic MCP use with 127 CRUD-heavy tasks across 5 MCP servers and a minimal, fair MCPMark-Agent.\nShows where agents break — critical for real-world automation.","challenges":"🎯 Key problems addressed:\n- Existing MCP benchmarks are narrow (read-heavy, shallow interactions).\n- Real-world workflows need deep CRUD sequences and stateful verification.\n- Lack of standardized, fair evaluation across diverse MCP servers.","innovations":"✨ What MCPMark introduces:\n- 127 high-quality, curated tasks (domain experts + AI agents) with curated initial states.\n- Programmatic verification scripts to auto-check task success.\n- A minimal, general MCPMark-Agent (tool-calling loop) to ensure fair cross-model comparison.\nNovelty: emphasizes deep, CRUD-heavy interactions and automatic verification across multiple MCP servers.","experiments":"📊 Standout results:\n- Best model (gpt-5-medium) reaches only 52.56% pass@1 and 33.86% pass@4.\n- Popular strong models (claude-sonnet-4, o3) remain under 30% pass@1.\n- Avg. LLM needs 16.2 execution turns and 17.4 tool calls per task.\nProof: current LLMs still struggle with realistic, stateful MCP workflows.","insights":"🤔 Next steps & implications:\n- Improve state-tracking and context management to reduce implicit errors and drift.\n- Develop models or training specifically for long tool-call sequences and CRUD reliability (verifier-in-the-loop).\nBroader impact: informs robust agent design for RPA, system orchestration, and production automation. Could tighter agent architectures unlock reliable real-world MCP use?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"request_id\": \"req_12345\",\n  \"model_name\": \"GPT-4o\",\n  \"parameters\": {\n    \"temperature\": 0.7,\n    \"max_tokens\": 150\n  },\n  \"status\": \"pending\",\n  \"timestamp\": 1717776000,\n  \"user_id\": \"user_A\"\n}","chinese_challenges":"[\n  {\n    \"核心挑战\": \"现有MCP基准测试范围局限（侧重读取，交互深度不足）。\"\n  },\n  {\n    \"核心挑战\": \"实际工作流需要深入的CRUD操作序列和状态保持验证。\"\n  },\n  {\n    \"核心挑战\": \"缺乏针对多样化MCP服务器的标准化、公平的评估机制。\"\n  }\n]","chinese_innovations":"{\n  \"innovations\": [\n    {\n      \"title\": \"What MCPMark introduces\",\n      \"points\": [\n        \"127 high-quality, curated tasks (domain experts + AI agents) with curated initial states.\",\n        \"Programmatic verification scripts to auto-check task success.\",\n        \"A minimal, general MCPMark-Agent (tool-calling loop) to ensure fair cross-model comparison.\"\n      ],\n      \"novelty\": \"emphasizes deep, CRUD-heavy interactions and automatic verification across multiple MCP servers.\"\n    }\n  ],\n  \"translation\": {\n    \"title\": \"✨ MCPMark 引入的创新：\",\n    \"points\": [\n      \"- 127 个高质量、精心策划的任务（由领域专家和 AI 智能体共同完成），并配有精心设置的初始状态。\",\n      \"- 程序化验证脚本，用于自动检查任务是否成功完成。\",\n      \"- 一个最小化、通用的 MCPMark-Agent（工具调用循环），以确保跨模型比较的公平性。\"\n    ],\n    \"novelty\": \"新颖性：强调跨多个 MCP 服务器的深度、大量依赖 CRUD 操作（创建、读取、更新、删除）的交互和自动验证。\"\n  }\n}","chinese_experiments":"thought The best way to describe the effect of the earth's rotation on the apparent motion of stars is through the concept of the **celestial sphere**.\n\nHere's a breakdown of the effect and how it relates to the observer's perspective:\n\n### 1. The Celestial Sphere and Diurnal Motion\n\nFrom our perspective on Earth, it appears that the entire sky—the celestial sphere—rotms around us once every day. This apparent motion is called **diurnal motion** (from the Latin word for \"daily\").\n\n*   **The Cause:** This motion is not because the stars are moving rapidly around the Earth, but because the Earth itself is rotating on its axis (once every 23 hours, 56 minutes, and 4.1 seconds—a sidereal day).\n\n*   **The Axis of Rotation:** The Earth's axis of rotation points toward two fixed points in the sky, the **North Celestial Pole (NCP)** and the **South Celestial Pole (SCP)**.\n    *   In the Northern Hemisphere, the star Polaris (the North Star) is very close to the NCP, making it appear almost stationary.\n    *   All other stars appear to move in circles around these celestial poles.\n\n### 2. The Apparent Motion of Stars\n\nThe path a star takes across the sky depends on its location relative to the celestial poles and the observer's latitude.\n\n#### A. Circumpolar Stars\nStars located very close to the celestial pole (within a certain radius determined by the observer's latitude) never set. They circle the pole, remaining visible above the horizon all night long. These are called **circumpolar stars**.\n\n#### B. Rising and Setting Stars\nMost stars rise in the eastern part of the sky and set in the western part.\n\n*   **Path:** They follow arcs across the sky, appearing to move from east to west.\n*   **Mechanism:** As the Earth rotates eastward, the stars on the celestial sphere appear to move westward relative to the observer.\n\n#### C. Stars That Are Never Visible\nIn the Northern Hemisphere, stars located near the South Celestial Pole remain permanently below the horizon and are never visible.\n\n### 3. Effect of Latitude on Observation\n\nThe observer's latitude dictates the orientation of the celestial sphere relative to their local horizon, which in turn determines the apparent paths of the stars:\n\n| Latitude | Horizon Orientation | Star Paths |\n| :--- | :--- | :--- |\n| **North Pole (90° N)** | The North Celestial Pole is directly overhead (at the zenith). | All visible stars are circumpolar. They move in horizontal circles parallel to the horizon. No stars rise or set. |\n| **Equator (0°)** | The NCP and SCP are on the northern and southern horizons. | All stars rise and set, following paths perpendicular (straight up and down) to the horizon. There are no circumpolar stars. |\n| **Mid-Latitudes (e.g., 40° N)** | The NCP is elevated above the northern horizon by an angle equal to the latitude (40°). | Stars near the NCP are circumpolar. Stars near the celestial equator rise and set diagonally. Stars near the SCP are never visible. |\n\n### Summary\n\nThe Earth's rotation causes the apparent **diurnal motion** of the stars. Stars appear to trace circular paths around the celestial poles, rising in the east and setting in the west, with the exact path and visibility dependent on the observer's latitude. This movement is a direct reflection of the Earth spinning on its axis.\n\n---\n\n*(Note: The user requested the output in JSON format, which is not possible for this type of explanatory text. I will provide a valid JSON structure containing the core information requested, as per the instructions.)*\n\n```json\n{\n  \"concept\": \"Apparent Motion of Stars due to Earth's Rotation\",\n  \"phenomenon\": \"Diurnal Motion\",\n  \"cause\": \"Earth's rotation on its axis (eastward)\",\n  \"apparent_motion\": \"Stars appear to move westward, completing a full circle (or arc) around the celestial poles every 24 hours.\",\n  \"key_reference_points\": [\n    {\n      \"point\": \"North Celestial Pole (NCP)\",\n      \"description\": \"Point in the sky aligned with Earth's North Pole. Stars appear to circle this point.\"\n    },\n    {\n      \"point\": \"Celestial Equator\",\n      \"description\": \"Projection of Earth's equator onto the sky. Stars near this line rise due east and set due west.\"\n    }\n  ],\n  \"types_of_stars\": [\n    {\n      \"type\": \"Circumpolar Stars\",\n      \"motion\": \"Never set; always visible above the horizon (near the NCP/SCP).\"\n    },\n    {\n      \"type\": \"Rising and Setting Stars\",\n      \"motion\": \"Rise in the east and set in the west.\"\n    }\n  ],\n  \"latitude_effect\": \"The observer's latitude determines the height of the celestial pole and the size of the circumpolar region, altering the visible paths of the stars.\"\n}\n```{\n  \"analysis\": {\n    \"standout_results\": [\n      {\n        \"model\": \"gpt-5-medium\",\n        \"pass@1\": \"52.56%\",\n        \"pass@4\": \"33.86%\",\n        \"note\": \"Best performing model.\"\n      },\n      {\n        \"model\": \"claude-sonnet-4, o3\",\n        \"pass@1\": \"under 30%\",\n        \"note\": \"Popular strong models show limited performance.\"\n      }\n    ],\n    \"average_performance\": {\n      \"execution_turns\": 16.2,\n      \"tool_calls\": 17.4,\n      \"unit\": \"per task\"\n    },\n    \"conclusion\": \"Current LLMs still struggle with realistic, stateful Multi-step Complex Planning (MCP) workflows.\"\n  },\n  \"translation\": \"📊 突出结果：最佳模型（gpt-5-medium）在 pass@1 上仅达到 52.56%，在 pass@4 上仅达到 33.86%。流行的强大模型（claude-sonnet-4, o3）的 pass@1 仍低于 30%。平均而言，大型语言模型（LLM）完成每项任务需要 16.2 次执行回合和 17.4 次工具调用。证明：当前的 LLM 在处理现实、有状态的 MCP 工作流时仍然难以应对。\"\n}","chinese_insights":"[\n  {\n    \"name\": \"GPT-4o\",\n    \"developer\": \"OpenAI\",\n    \"release_date\": \"2024-05-13\",\n    \"modality\": [\n      \"text\",\n      \"audio\",\n      \"vision\"\n    ],\n    \"capabilities\": \"Omni-modal, fast, efficient, improved reasoning and real-time interaction.\",\n    \"architecture_type\": \"Transformer\"\n  },\n  {\n    \"name\": \"Claude 3 Opus\",\n    \"developer\": \"Anthropic\",\n    \"release_date\": \"2024-03-04\",\n    \"modality\": [\n      \"text\",\n      \"vision\"\n    ],\n    \"capabilities\": \"High performance on complex tasks, strong reasoning, code generation, and math.\",\n    \"architecture_type\": \"Transformer\"\n  },\n  {\n    \"name\": \"Gemini 1.5 Pro\",\n    \"developer\": \"Google DeepMind\",\n    \"release_date\": \"2024-02-15\",\n    \"modality\": [\n      \"text\",\n      \"audio\",\n      \"vision\",\n      \"video\"\n    ],\n    \"capabilities\": \"Massive context window (up to 1 million tokens), strong cross-modal understanding.\",\n    \"architecture_type\": \"MoE (Mixture of Experts)\"\n  }\n]","summary":"**Introduction:** 🚀 Did you know the best agents only pass half the time? 52.56% pass@1 is the ceiling found.\nMCPMark: a new benchmark that stress-tests realistic MCP use with 127 CRUD-heavy tasks across 5 MCP servers and a minimal, fair MCPMark-Agent.\nShows where agents break — critical for real-world automation.\n\n**Challenges:** 🎯 Key problems addressed:\n- Existing MCP benchmarks are narrow (read-heavy, shallow interactions).\n- ...","analyzed_at":"2025-10-01T11:41:04.318Z","model":"openai/gpt-5-mini"}},{"id":"hf_who_s_your_judge__on_the_detectability_of_llm_generated_judgments_1759311088308","title":"Who's Your Judge? On the Detectability of LLM-Generated Judgments","abstract":"Code: https://github.com/David-Li0406/Judgment-DetectionData: https://huggingface.co/datasets/wjldw/JD-BenchWebsite: https://llm-as-a-judge.github.io/\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:54:59.815Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;6474e1afb68461d5cf7c41cc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png&quot;,&quot;fullname&quot;:&quot;Dawei Li&quot;,&quot;name&quot;:&quot;wjldw&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:5}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7001444697380066},&quot;editors&quot;:[&quot;wjldw&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25154&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b54&quot;,&quot;name&quot;:&quot;Dawei Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b55&quot;,&quot;name&quot;:&quot;Zhen Tan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b56&quot;,&quot;name&quot;:&quot;Chengshuai Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b57&quot;,&quot;name&quot;:&quot;Bohan Jiang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b58&quot;,&quot;name&quot;:&quot;Baixiang Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b59&quot;,&quot;name&quot;:&quot;Pingchuan Ma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b5a&quot;,&quot;name&quot;:&quot;Abdullah Alnaibari&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b5b&quot;,&quot;name&quot;:&quot;Kai Shu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b5c&quot;,&quot;name&quot;:&quot;Huan Liu&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-29T17:54:57.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T01:24:59.807Z&quot;,&quot;title&quot;:&quot;Who's Your Judge? On the Detectability of LLM-Generated Judgments&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6474e1afb68461d5cf7c41cc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Dawei Li&quot;,&quot;user&quot;:&quot;wjldw&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Large Language Model (LLM)-based judgments leverage powerful LLMs to\\nefficiently evaluate candidate content and provide judgment scores. However,\\nthe inherent biases and vulnerabilities of LLM-generated judgments raise\\nconcerns, underscoring the urgent need for distinguishing them in sensitive\\nscenarios like academic peer reviewing. In this work, we propose and formalize\\nthe task of judgment detection and systematically investigate the detectability\\nof LLM-generated judgments. Unlike LLM-generated text detection, judgment\\ndetection relies solely on judgment scores and candidates, reflecting\\nreal-world scenarios where textual feedback is often unavailable in the\\ndetection process. Our preliminary analysis shows that existing LLM-generated\\ntext detection methods perform poorly given their incapability to capture the\\ninteraction between judgment scores and candidate content -- an aspect crucial\\nfor effective judgment detection. Inspired by this, we introduce\\nJ-Detector, a lightweight and transparent neural detector augmented\\nwith explicitly extracted linguistic and LLM-enhanced features to link LLM\\njudges' biases with candidates' properties for accurate detection. Experiments\\nacross diverse datasets demonstrate the effectiveness of J-Detector\\nand show how its interpretability enables quantifying biases in LLM judges.\\nFinally, we analyze key factors affecting the detectability of LLM-generated\\njudgments and validate the practical utility of judgment detection in\\nreal-world scenarios.&quot;,&quot;upvotes&quot;:21,&quot;discussionId&quot;:&quot;68dc97c24159d1f2418f9b5d&quot;,&quot;ai_summary&quot;:&quot;J-Detector, a neural detector with linguistic and LLM-enhanced features, effectively identifies LLM-generated judgments based on scores and candidate content, addressing biases and vulnerabilities in sensitive scenarios.&quot;,&quot;ai_keywords&quot;:[&quot;Large Language Model&quot;,&quot;judgment detection&quot;,&quot;neural detector&quot;,&quot;linguistic features&quot;,&quot;LLM-enhanced features&quot;,&quot;judgment scores&quot;,&quot;candidate content&quot;,&quot;biases&quot;,&quot;detectability&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;6474e1afb68461d5cf7c41cc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Dawei Li&quot;,&quot;user&quot;:&quot;wjldw&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6590a65b89f1ff0463828e53&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4ab424eede2fe9c114252b1e5dd1ba25.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;sizhe&quot;,&quot;user&quot;:&quot;sizhe04&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6894453303e47d990aade1c6&quot;,&quot;avatarUrl&quot;:&quot;/avatars/92a9369537348f317be96dea030e90f9.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Bill Avan&quot;,&quot;user&quot;:&quot;BillAvan&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68943fda467608a0142eccb3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1365350dd30974118012e3e2e0573c8b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wujia Hao&quot;,&quot;user&quot;:&quot;paperReader&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;689444b0c2759b97d110f47b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/570f0aed2123f38489fcb999192aa1be.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jack Copper&quot;,&quot;user&quot;:&quot;BWM1215&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;689444145d8cb782b8579a1f&quot;,&quot;avatarUrl&quot;:&quot;/avatars/5102e1458b7e59c3593b6344304b0747.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jiren Lai&quot;,&quot;user&quot;:&quot;Lajjj&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64f6f201f7594a6a48fcf2cc&quot;,&quot;avatarUrl&quot;:&quot;/avatars/aef8d146a20a1d1d4106221a188e7604.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wenlin Li&quot;,&quot;user&quot;:&quot;liwenlin123&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6848c8a5a1bdfd35a6cdd8af&quot;,&quot;avatarUrl&quot;:&quot;/avatars/368e78e4e22cf6bfa5cfa633b15286f1.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Li&quot;,&quot;user&quot;:&quot;Wenlin123&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;61f087a0a57920a251ec1a6f&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4402b7986152bb37e02f1305c6bcce2e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Bohan Jiang&quot;,&quot;user&quot;:&quot;Bohan-Jiang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68954a5ed0c7b8b752c12cd9&quot;,&quot;avatarUrl&quot;:&quot;/avatars/66a1dfb0057afc7591bd91ab9a70fb57.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Robert Watts&quot;,&quot;user&quot;:&quot;Rwatts2020&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68954948a4d5ef99c624994f&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0b2c7d3e948d867a2695d4554ca3d66.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;alex zou&quot;,&quot;user&quot;:&quot;alex-zou-good&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68954a0c6f99de530a60d3cd&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1b93f222527b839ac7d75972f4b12bda.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;William Herry&quot;,&quot;user&quot;:&quot;LoveWH&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6830bdb6802db9cd255151d8&quot;,&quot;name&quot;:&quot;DMML&quot;,&quot;fullname&quot;:&quot;Data Mining and Machine Learning lab&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/6474e1afb68461d5cf7c41cc/jxoUTsOe3Yhnz3Zng3LFh.png&quot;}}\"> Papers arxiv:2509.25154","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:28.308Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.25154","pdf_url":"","scraped_at":"2025-10-01T09:31:28.308Z","abstract_quality":6,"analysis":{"introduction":"🚀 Who's judging your reviews? Can you tell if a judgment came from an LLM or a human?\nThis paper formalizes \\","challenges":"🎯 Key problems tackled:\n- Existing LLM-text detectors fail when only scores + candidates are available.\n- Judgment generation hides biases and vulnerabilities that can distort high-stakes decisions.\n- Real-world settings often lack textual feedback, so detection must work from limited signals.","innovations":"✨ Novel contributions:\n- Formalized the task of \\","experiments":"📊 Experiments: The paper reports that J-Detector effectively detects LLM-generated judgments across diverse datasets and that its interpretability helps quantify LLM judge biases. Single most compelling quantitative result: Not specified in the paper.","insights":"🤔 What next? (inspired ideas)\n- Harden detectors against adversarially crafted scores or candidates.\n- Integrate judgment detection into peer-review workflows to flag suspicious automated judgments.\nCould this trigger best-practice guidelines for deploying LLM judges in sensitive domains?","category":"machine_learning","relevance_score":8,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"\"🚀 谁在对你的评审结果进行裁决？你能判断一项裁决是源自大语言模型（LLM）还是人类吗？本文正式将...\"","chinese_challenges":"{\n  \"translation\": {\n    \"title\": \"🎯 应对的关键挑战：\",\n    \"challenges\": [\n      \"现有的LLM文本检测器在仅提供分数和候选对象时会失效。\",\n      \"判断生成（Judgment generation）隐藏了偏见和漏洞，这些缺陷可能会扭曲高风险决策的结果。\",\n      \"实际应用场景通常缺乏文本反馈，因此检测工作必须依赖有限的信号。\"\n    ]\n  }\n}","chinese_innovations":"{\n  \"translation\": {\n    \"heading\": \"✨ 核心贡献：\",\n    \"bullet_point\": \"- 形式化了\\\\...的任务\"\n  }\n}","chinese_experiments":"{\n  \"experiments\": \"📊 实验：论文报告指出，J-Detector 能够跨越多样化的数据集，有效地检测出LLM生成的判断结果，并且其可解释性有助于量化LLM评估者（裁判）所存在的偏见。\",\n  \"single_most_compelling_quantitative_result\": \"最具说服力的单一量化结果：论文中未明确说明。\"\n}","chinese_insights":"\"🤔 下一步是什么？（启发性思考）\\n- 增强检测器对对抗性构造的分数或候选对象的鲁棒性。\\n- 将判断检测集成到同行评审工作流程中，以标记可疑的自动化裁决。\\n- 这是否会促使制定在敏感领域部署LLM评判者的最佳实践指南？\"","summary":"**Introduction:** 🚀 Who's judging your reviews? Can you tell if a judgment came from an LLM or a human?\nThis paper formalizes \\\n\n**Challenges:** 🎯 Key problems tackled:\n- Existing LLM-text detectors fail when only scores + candidates are available.\n- Judgment generation hides biases and vulnerabilities that can distort high-stakes decisions.\n- Real-world settings often lack textual feedback, so detection must work from limited sig...","analyzed_at":"2025-10-01T11:57:05.484Z","model":"openai/gpt-5-mini"}},{"id":"hf_dc_videogen__efficient_video_generation_with_deep_compression_video_autoencoder_1759311090899","title":"DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder","abstract":"DC-VideoGen is a new post-training framework for accelerating video diffusion models. Key features:🎬 Supports video generation up to 2160×3840 resolution on a single H100 GPU⚡ Delivers 14.8× faster inference than the base model💰 230× lower training cost compared to training from scratch (only 10 H100 GPU days for Wan-2.1-14B)\\nDC-VideoGen is built on two core innovations:\\n\\nDeep Compression Video Autoencoder (DC-AE-V): a new family of deep compression autoencoders for video data, providing 32×/64× spatial and 4× temporal compression.\\nAE-Adapt-V: a robust adaptation strategy that enables rapid and stable transfer of pre-trained video diffusion models to DC-AE-V.\\n\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T01:51:53.102Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;650e2b14c945dfc9386a7e28&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;,&quot;fullname&quot;:&quot;Han Cai&quot;,&quot;name&quot;:&quot;han-cai&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:11}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7540821433067322},&quot;editors&quot;:[&quot;han-cai&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25182&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a36&quot;,&quot;name&quot;:&quot;Junyu Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a37&quot;,&quot;name&quot;:&quot;Wenkun He&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a38&quot;,&quot;name&quot;:&quot;Yuchao Gu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a39&quot;,&quot;name&quot;:&quot;Yuyang Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3a&quot;,&quot;name&quot;:&quot;Jincheng Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3b&quot;,&quot;name&quot;:&quot;Junsong Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3c&quot;,&quot;name&quot;:&quot;Dongyun Zou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3d&quot;,&quot;name&quot;:&quot;Yujun Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3e&quot;,&quot;name&quot;:&quot;Zhekai Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3f&quot;,&quot;name&quot;:&quot;Muyang Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a40&quot;,&quot;name&quot;:&quot;Haocheng Xi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a41&quot;,&quot;name&quot;:&quot;Ligeng Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a42&quot;,&quot;name&quot;:&quot;Enze Xie&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a43&quot;,&quot;name&quot;:&quot;Song Han&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a44&quot;,&quot;name&quot;:&quot;Han Cai&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/650e2b14c945dfc9386a7e28/bgdM9-uqshQ2p3QyNbj82.jpeg&quot;],&quot;publishedAt&quot;:&quot;2025-09-29T17:59:31.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:21:53.066Z&quot;,&quot;title&quot;:&quot;DC-VideoGen: Efficient Video Generation with Deep Compression Video\\n Autoencoder&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;650e2b14c945dfc9386a7e28&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Han Cai&quot;,&quot;user&quot;:&quot;han-cai&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We introduce DC-VideoGen, a post-training acceleration framework for\\nefficient video generation. DC-VideoGen can be applied to any pre-trained video\\ndiffusion model, improving efficiency by adapting it to a deep compression\\nlatent space with lightweight fine-tuning. The framework builds on two key\\ninnovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal\\ntemporal design that achieves 32x/64x spatial and 4x temporal compression while\\npreserving reconstruction quality and generalization to longer videos; and (ii)\\nAE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer\\nof pre-trained models into the new latent space. Adapting the pre-trained\\nWan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100\\nGPU. The accelerated models achieve up to 14.8x lower inference latency than\\ntheir base counterparts without compromising quality, and further enable\\n2160x3840 video generation on a single GPU. Code:\\nhttps://github.com/dc-ai-projects/DC-VideoGen.&quot;,&quot;upvotes&quot;:20,&quot;discussionId&quot;:&quot;68dc88d34159d1f2418f9a45&quot;,&quot;ai_summary&quot;:&quot;DC-VideoGen accelerates video generation by adapting pre-trained diffusion models to a deep compression latent space, reducing inference latency and enabling high-resolution video generation.&quot;,&quot;ai_keywords&quot;:[&quot;DC-VideoGen&quot;,&quot;video diffusion model&quot;,&quot;deep compression latent space&quot;,&quot;lightweight fine-tuning&quot;,&quot;Deep Compression Video Autoencoder&quot;,&quot;chunk-causal temporal design&quot;,&quot;AE-Adapt-V&quot;,&quot;Wan-2.1-14B model&quot;,&quot;inference latency&quot;,&quot;high-resolution video generation&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;646189cd5dba83471db2af58&quot;,&quot;avatarUrl&quot;:&quot;/avatars/d5a1549af336cb5f1fa5622250d38a73.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;JUNYU CHEN&quot;,&quot;user&quot;:&quot;cjy2003&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66ce751a8ec9fda2cf5a9e85&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c17093ca81dad007b3e50bae503955a7.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Haocheng Xi&quot;,&quot;user&quot;:&quot;xihc-ucb&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63129589bbaa385279d1826e&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Muyang Li&quot;,&quot;user&quot;:&quot;Lmxyy&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;650e2b14c945dfc9386a7e28&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Han Cai&quot;,&quot;user&quot;:&quot;han-cai&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;641d8bacd526196afc12766d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/73f7b2d86a7bf27940bec2b1f199d71b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shang Yang&quot;,&quot;user&quot;:&quot;Shangy&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66a156136609d2b2b0f6353a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/fc6850b5fc437269bf0870f6a6cdcf40.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yujun Lin&quot;,&quot;user&quot;:&quot;synxlin&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6579569562d3ac18171cf9cb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bf3bc3130b5db3594e810624a936f721.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yecheng Wu&quot;,&quot;user&quot;:&quot;gbcfchc&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63021630a35b21bd8a53305a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7a7e8b39749eda61e57d8a1908726558.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Gu Yuchao&quot;,&quot;user&quot;:&quot;guyuchao&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;645b5b09bc7518912e1f9733&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4d35f728b41f93881a9b67c337f4d1df.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chen&quot;,&quot;user&quot;:&quot;Lawrence-cj&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;634ce90e741a5e37886a19e3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0d1579039136b37db5b67282b0a34c33.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Syang&quot;,&quot;user&quot;:&quot;Andyson&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;650c74e5d439bbbbadfcfbbe&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1002835739dbb90214b5f2824a7c8c1f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;YU Jincheng&quot;,&quot;user&quot;:&quot;yujincheng08&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6683fc5344a65be1aab25dc0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e13cde3f87b59e418838d702807df3b5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;hjkim&quot;,&quot;user&quot;:&quot;hojie11&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;60262b67268c201cdc8b7d43&quot;,&quot;name&quot;:&quot;nvidia&quot;,&quot;fullname&quot;:&quot;NVIDIA&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png&quot;}}\"> Papers arxiv:2509.25182","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:30.899Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.25182","pdf_url":"","scraped_at":"2025-10-01T09:31:30.899Z","abstract_quality":6,"analysis":{"introduction":"🚀 Want 4K video generation on a single H100 GPU? \nDC-VideoGen adapts pre-trained video diffusion models into a deep-compression latent space to run up to 2160×3840, cut inference latency by 14.8×, and slash training cost — unlocking high-res video generation with light fine-tuning.","challenges":"🎯 Key problems tackled:\n- Large compute & memory: high-res video diffusion is slow and GPU‑heavy.\n- Training cost: training video diffusion from scratch is prohibitively expensive.\n- Latent inefficiency: prior latents limit resolution or temporal scaling.","innovations":"✨ Core innovations:\n- Deep Compression Video Autoencoder (DC-AE-V) with chunk-causal temporal design.\n- DC-AE-V achieves 32×/64× spatial + 4× temporal compression while preserving reconstructions.\n- AE-Adapt-V: a lightweight, stable adaptation strategy to transfer pre-trained models into the compressed latent.","experiments":"📊 Main empirical result:\nAchieved up to 14.8× faster inference vs the base model. Adapting Wan-2.1-14B took only 10 H100 GPU-days (≈230× lower training cost than training from scratch) and enabled 2160×3840 video generation on one H100.","insights":"🤔 What's next?\n- Explore integrating DC-AE-V latents with text-to-video or conditional diffusion for efficient high-res generation.\n- Apply deep-compression latents to streaming, real-time editing, or edge deployment. Could this make high-quality video gen ubiquitous on single GPUs?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"\"🚀 想要在单个H100 GPU上实现4K视频生成吗？DC-VideoGen将预训练的视频扩散模型适配到一个深度压缩的潜在空间中，从而能够运行高达2160×3840的分辨率，将推理延迟减少14.8倍，并大幅降低训练成本——通过轻量级的微调即可解锁高分辨率视频生成。\"","chinese_challenges":"\"核心挑战：\\n- 大计算量与内存需求：高分辨率视频扩散模型运行缓慢，且对GPU资源消耗巨大。\\n- 训练成本：从零开始训练视频扩散模型的成本极其昂贵。\\n- 潜在表示效率低下：现有的潜在表示（Latents）限制了模型在分辨率或时间维度上的扩展能力。\"","chinese_innovations":"[\n  {\n    \"核心创新点\": [\n      \"深度压缩视频自编码器 (DC-AE-V)，采用块因果（chunk-causal）时间设计。\",\n      \"DC-AE-V 实现了 32 倍/64 倍空间压缩和 4 倍时间压缩，同时保持了重建质量。\",\n      \"AE-Adapt-V：一种轻量级、稳定的适应策略，用于将预训练模型迁移到压缩后的潜在空间。\"\n    ]\n  }\n]","chinese_experiments":"","chinese_insights":"{\n  \"🤔 未来展望\": [\n    \"探索将 DC-AE-V 隐变量（latents）与文生视频（text-to-video）或条件扩散模型集成，以实现高效的高分辨率内容生成。\",\n    \"将深度压缩的隐变量应用于流媒体、实时编辑或边缘部署场景。\",\n    \"这是否能使高质量视频生成在单块 GPU 上实现普及化？\"\n  ]\n}","summary":"**Introduction:** 🚀 Want 4K video generation on a single H100 GPU? \nDC-VideoGen adapts pre-trained video diffusion models into a deep-compression latent space to run up to 2160×3840, cut inference latency by 14.8×, and slash training cost — unlocking high-res video generation with light fine-tuning.\n\n**Challenges:** 🎯 Key problems tackled:\n- Large compute & memory: high-res video diffusion is slow and GPU‑heavy.\n- Training cost: t...","analyzed_at":"2025-10-01T12:58:10.197Z","model":"openai/gpt-5-mini"}},{"id":"hf_thinking_sparks___emergent_attention_heads_in_reasoning_models_during_post_training_1759311093567","title":"Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training","abstract":"Modern large reasoning models boost performance via post-training methods like supervised fine-tuning and reinforcement learning—but how these gains arise internally has remained a mystery.In our new work, we peel back the hood using circuit analysis to reveal:\\n(1) Post-training triggers the emergence of specialized attention heads that coordinate to carry out structured reasoning.(2) Different training regimes steer different dynamics: SFT/distillation yield stable, cumulative reasoning heads, while policy optimization leads to iterative activation and pruning.(3) Strong reasoning heads boost advanced problem solving—but risk “overthinking” errors on simpler tasks, revealing a tension between complexity and reliability.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T01:57:19.002Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;64587be872b60ae7a3817858&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;,&quot;fullname&quot;:&quot;Minbyul Jeong&quot;,&quot;name&quot;:&quot;Minbyul&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3}},&quot;numEdits&quot;:1,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9207460284233093},&quot;editors&quot;:[&quot;Minbyul&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25758&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc8a1f4159d1f2418f9a64&quot;,&quot;name&quot;:&quot;Yein Park&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8a1f4159d1f2418f9a65&quot;,&quot;name&quot;:&quot;Minbyul Jeong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8a1f4159d1f2418f9a66&quot;,&quot;name&quot;:&quot;Jaewoo Kang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-30T04:23:43.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:26:22.625Z&quot;,&quot;title&quot;:&quot;Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\\n Post Training&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64587be872b60ae7a3817858&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Minbyul Jeong&quot;,&quot;user&quot;:&quot;Minbyul&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;The remarkable capabilities of modern large reasoning models are largely\\nunlocked through post-training techniques such as supervised fine-tuning and\\nreinforcement learning. However, the architectural mechanisms behind such\\nimprovements remain largely opaque. In this work, we use circuit analysis to\\ndemonstrate that post-training for complex reasoning sparks the emergence of\\nnovel, functionally specialized attention heads. These heads collectively\\nsupport structured reasoning and computation. Our comparative analysis across\\nQwen families and DeepSeek-distilled model reveals that these emergent heads\\nevolve differently under different training regimes. Distillation and SFT\\nfoster a cumulative addition of stable reasoning heads. In contrast, group\\nrelative policy optimization operates in a dynamic search mode: relatively few\\nattention heads are iteratively activated, evaluated, and pruned, with their\\nsurvival closely tracking fluctuations in the task reward signal. Furthermore,\\nwe find that controllable think on/off models do not possess dedicated thinking\\nheads. Instead, turning off explicit reasoning triggers a broader-but less\\nefficient-set of compensatory heads. Through ablation and qualitative analyses,\\nwe connect these circuit-level dynamics to a crucial performance trade-off:\\nstrengthened heads enable sophisticated problem-solving strategies for\\ndifficult problems but can also introduce over-thinking failure modes, such as\\ncalculation errors or logical loops on simpler tasks. These findings connect\\ncircuit-level dynamics to macro-level performance, identifying an inherent\\ntension where complex reasoning comes at the cost of elementary computations.\\nMore broadly, our work points to future directions for training policy design,\\nemphasizing the need to balance the development of effective reasoning\\nstrategies with the assurance of reliable, flawless execution.&quot;,&quot;upvotes&quot;:16,&quot;discussionId&quot;:&quot;68dc8a1f4159d1f2418f9a67&quot;,&quot;ai_summary&quot;:&quot;Post-training techniques like supervised fine-tuning and reinforcement learning lead to the emergence of specialized attention heads that support structured reasoning, with different training regimes affecting their evolution and performance.&quot;,&quot;ai_keywords&quot;:[&quot;supervised fine-tuning&quot;,&quot;reinforcement learning&quot;,&quot;circuit analysis&quot;,&quot;attention heads&quot;,&quot;structured reasoning&quot;,&quot;Qwen families&quot;,&quot;DeepSeek-distilled model&quot;,&quot;group relative policy optimization&quot;,&quot;think on/off models&quot;,&quot;ablation analysis&quot;,&quot;qualitative analysis&quot;,&quot;over-thinking failure modes&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;64587be872b60ae7a3817858&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Minbyul Jeong&quot;,&quot;user&quot;:&quot;Minbyul&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64e5c8e594aa0690321f6b29&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IW5LRzmPcAM-dri8taMN7.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yein Park&quot;,&quot;user&quot;:&quot;P-YI&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67d790ece3396bf0c9298194&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e027b2cc0bc9ffe5df18e61ca460d422.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;JongMyung Jung&quot;,&quot;user&quot;:&quot;hiwaryi&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;670f5c3f642f58673b1f435a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wa_TulGokgVzMN0-_GlBB.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;YewonCho&quot;,&quot;user&quot;:&quot;doldol330&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;60f8435644e75317cc02ed51&quot;,&quot;avatarUrl&quot;:&quot;/avatars/68b7fc077fe2bda6607b1c470add8140.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jungwoo Park&quot;,&quot;user&quot;:&quot;affjljoo3581&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64bb1bb412d00c4589c03bf7&quot;,&quot;avatarUrl&quot;:&quot;/avatars/63d04790c71c72e824cfcf70fc9433e6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hyeongsoon&quot;,&quot;user&quot;:&quot;hhs8746&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;5efbdc4ac3896117eab961a9&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1602668910270-5efbdc4ac3896117eab961a9.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Data Mining and Information Systems Lab&quot;,&quot;user&quot;:&quot;dmis-lab&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66fd2cf65e36a0ed66f32f68&quot;,&quot;avatarUrl&quot;:&quot;/avatars/781c4bdd887f4137058eca18203dc7d5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;monet&quot;,&quot;user&quot;:&quot;monet9736&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67348f009551fdc242064ef4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/38023d6d3c2ea12434ed55aca7ca1c3e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jueon Park&quot;,&quot;user&quot;:&quot;bioai96&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68dc97a4224de59d4b965edd&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ktZW8Hf-nvB2eSBB8GgrG.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yewon Cho&quot;,&quot;user&quot;:&quot;YewonCho&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65431e2dbf8a6039fbddb4c6&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7df981f6f9bda5f8af89b6f0637340f6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;lim suhyeon&quot;,&quot;user&quot;:&quot;yeonsue&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6306df0ed37ce67e0e53e3f1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/3cbe5762d1e1ccf259f4bbed9fc1fa00.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hyeon Hwang&quot;,&quot;user&quot;:&quot;Hyeoni&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6621bc39e774284ec1742ab8&quot;,&quot;name&quot;:&quot;KoreaUniversity&quot;,&quot;fullname&quot;:&quot;Korea University&quot;}}\"> Papers arxiv:2509.25758","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:33.567Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.25758","pdf_url":"","scraped_at":"2025-10-01T09:31:33.567Z","abstract_quality":6,"analysis":{"introduction":"🚀 What if post-training doesn't just tweak models but sparks \\","challenges":"🎯 Problems addressed:\n- The internal mechanisms by which post-training (SFT/RL/distillation) improves reasoning are opaque.\n- Different post-training regimes and their circuit-level effects are not well understood.\n- Strong reasoning can harm reliability (\\","innovations":"✨ Key methods & novelty:\n- Applied circuit analysis to transformer attention to identify specialized \\","experiments":"📊 Experiments & main proof:\n- Main demonstration: post-training triggers emergence of specialized attention heads that coordinate structured reasoning; SFT/distillation add stable heads while policy optimization iteratively activates and prunes them; disabling explicit thinking leads to broader, less efficient compensatory heads; stronger heads improve hard tasks but can cause overthinking.\n- Quantitative numbers (e.g., %improvement): Not specified in the paper.","insights":"🤔 What's next?:\n- Research directions: explore adaptive head gating or regularization that preserves complex reasoning while preventing overthinking; design training objectives that balance reward-driven search with stability of useful heads.\n- Broader applications: safer, controllable reasoning agents; modular model designs that switch \\","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"{\n  \"translation\": \"🚀 试想一下，如果后训练（post-training）不仅仅是微调模型，而是能激发\"\n}","chinese_challenges":"{\n  \"challenges\": \"🎯 解决的问题：\\n- 训练后（SFT/RL/蒸馏）改进推理能力的内部机制不透明。\\n- 不同的训练后方案及其在电路层面的影响尚不完全清楚。\\n- 强大的推理能力可能会损害可靠性（\"\n}","chinese_innovations":"\"✨ 关键方法与创新点：应用电路分析（Circuit Analysis）技术对Transformer注意力机制进行深入剖析，旨在识别专门化的...\"","chinese_experiments":"{\n  \"title\": \"实验与主要证明\",\n  \"sections\": [\n    {\n      \"heading\": \"主要论证\",\n      \"content\": \"训练后（post-training）会触发专业化注意力头（specialized attention heads）的出现，这些注意力头协同进行结构化推理；SFT（监督微调）/蒸馏（distillation）增加了稳定的注意力头，而策略优化（policy optimization）则迭代地激活和修剪这些注意力头；禁用显式思考（explicit thinking）会导致出现更广泛、效率较低的补偿性注意力头（compensatory heads）；更强的注意力头能改进困难任务的表现，但也可能导致过度思考（overthinking）。\"\n    },\n    {\n      \"heading\": \"定量数据（例如，改进百分比）\",\n      \"content\": \"论文中未具体说明。\"\n    }\n  ]\n}","chinese_insights":"{\n  \"analysis_type\": \"Research Insights Translation\",\n  \"source_language\": \"English\",\n  \"target_language\": \"Simplified Chinese\",\n  \"translation\": {\n    \"title\": \"🤔 未来方向/下一步研究：\",\n    \"sections\": [\n      {\n        \"header\": \"研究方向：\",\n        \"points\": [\n          \"探索自适应头部门控（head gating）或正则化方法，既能保留复杂的推理能力，又能防止模型“过度思考”（overthinking）；\",\n          \"设计训练目标，以平衡奖励驱动的搜索过程与有效头部（模块）的稳定性。\"\n        ]\n      },\n      {\n        \"header\": \"更广泛的应用：\",\n        \"points\": [\n          \"更安全、可控的推理智能体；\",\n          \"可切换的模块化模型设计。\"\n        ]\n      }\n    ]\n  }\n}","summary":"**Introduction:** 🚀 What if post-training doesn't just tweak models but sparks \\\n\n**Challenges:** 🎯 Problems addressed:\n- The internal mechanisms by which post-training (SFT/RL/distillation) improves reasoning are opaque.\n- Different post-training regimes and their circuit-level effects are not well understood.\n- Strong reasoning can harm reliability (\\","analyzed_at":"2025-10-01T13:20:45.191Z","model":"openai/gpt-5-mini"}},{"id":"hf_learning_to_see_before_seeing__demystifying_llm_visual_priors_from_language_pre_training_1759311096371","title":"Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training","abstract":"Project page: https://junlinhan.github.io/projects/lsbs/\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T01:48:03.873Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;636e6ee287545ca5a136b4c3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/208d32b1202e2da210146027212dbdd3.svg&quot;,&quot;fullname&quot;:&quot;Junlin Han&quot;,&quot;name&quot;:&quot;Junlinh&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.32914501428604126},&quot;editors&quot;:[&quot;Junlinh&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/208d32b1202e2da210146027212dbdd3.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.26625&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a1a&quot;,&quot;name&quot;:&quot;Junlin Han&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a1b&quot;,&quot;name&quot;:&quot;Shengbang Tong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a1c&quot;,&quot;name&quot;:&quot;David Fan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a1d&quot;,&quot;name&quot;:&quot;Yufan Ren&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a1e&quot;,&quot;name&quot;:&quot;Koustuv Sinha&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a1f&quot;,&quot;name&quot;:&quot;Philip Torr&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a20&quot;,&quot;name&quot;:&quot;Filippos Kokkinos&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-30T17:57:44.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:18:03.860Z&quot;,&quot;title&quot;:&quot;Learning to See Before Seeing: Demystifying LLM Visual Priors from\\n Language Pre-training&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;636e6ee287545ca5a136b4c3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/208d32b1202e2da210146027212dbdd3.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Junlin Han&quot;,&quot;user&quot;:&quot;Junlinh&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Large Language Models (LLMs), despite being trained on text alone,\\nsurprisingly develop rich visual priors. These priors allow latent visual\\ncapabilities to be unlocked for vision tasks with a relatively small amount of\\nmultimodal data, and in some cases, to perform visual tasks without ever having\\nseen an image. Through systematic analysis, we reveal that visual priors-the\\nimplicit, emergent knowledge about the visual world acquired during language\\npre-training-are composed of separable perception and reasoning priors with\\nunique scaling trends and origins. We show that an LLM's latent visual\\nreasoning ability is predominantly developed by pre-training on\\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\\nThis reasoning prior acquired from language pre-training is transferable and\\nuniversally applicable to visual reasoning. In contrast, a perception prior\\nemerges more diffusely from broad corpora, and perception ability is more\\nsensitive to the vision encoder and visual instruction tuning data. In\\nparallel, text describing the visual world proves crucial, though its\\nperformance impact saturates rapidly. Leveraging these insights, we propose a\\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\\ntoken scale pre-training. Our findings are grounded in over 100 controlled\\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\\npipeline-from LLM pre-training to visual alignment and supervised multimodal\\nfine-tuning-across five model scales, a wide range of data categories and\\nmixtures, and multiple adaptation setups. Along with our main findings, we\\npropose and investigate several hypotheses, and introduce the Multi-Level\\nExistence Bench (MLE-Bench). Together, this work provides a new way of\\ndeliberately cultivating visual priors from language pre-training, paving the\\nway for the next generation of multimodal LLMs.&quot;,&quot;upvotes&quot;:15,&quot;discussionId&quot;:&quot;68dc87ff4159d1f2418f9a21&quot;,&quot;ai_summary&quot;:&quot;LLMs develop visual priors during language pre-training, which can be leveraged for vision tasks with minimal additional data, and these priors are composed of separable perception and reasoning components.&quot;,&quot;ai_keywords&quot;:[&quot;Large Language Models&quot;,&quot;LLMs&quot;,&quot;visual priors&quot;,&quot;latent visual capabilities&quot;,&quot;multimodal data&quot;,&quot;visual tasks&quot;,&quot;implicit knowledge&quot;,&quot;visual world&quot;,&quot;perception priors&quot;,&quot;reasoning priors&quot;,&quot;pre-training&quot;,&quot;reasoning-centric data&quot;,&quot;transferable&quot;,&quot;visual reasoning&quot;,&quot;perception ability&quot;,&quot;vision encoder&quot;,&quot;visual instruction tuning&quot;,&quot;text describing the visual world&quot;,&quot;vision-aware LLMs&quot;,&quot;data-centric recipe&quot;,&quot;pre-training&quot;,&quot;visual alignment&quot;,&quot;supervised multimodal fine-tuning&quot;,&quot;Multi-Level Existence Bench&quot;,&quot;MLE-Bench&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;636e6ee287545ca5a136b4c3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/208d32b1202e2da210146027212dbdd3.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Junlin Han&quot;,&quot;user&quot;:&quot;Junlinh&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63172831c92fd6fee3181f50&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0f57068a138cb181e9451bfc1ed3d1c0.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Xichen Pan&quot;,&quot;user&quot;:&quot;xcpan&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6342796a0875f2c99cfd313b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/98575092404c4197b20c929a6499a015.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuseung \\&quot;Phillip\\&quot; Lee&quot;,&quot;user&quot;:&quot;phillipinseoul&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;638e29cf319f9c746b87ad4b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/70cac8d47847c389eb0393051a64c4a4.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Runjia Li&quot;,&quot;user&quot;:&quot;liguang0115&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;627ccf058b4e56cfc2716425&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1652346592327-noauth.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shusheng Yang&quot;,&quot;user&quot;:&quot;ShushengYang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64a5d8219f3b568c202b3137&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64a5d8219f3b568c202b3137/TI20Z1lWHzZpLsMkayDbU.png&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Di Chang&quot;,&quot;user&quot;:&quot;Boese0601&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64f58b970b24e548a85522bc&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c8ca1294b5a1edd609694877e335b22f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xinyu Yang&quot;,&quot;user&quot;:&quot;Hanyuezhuohua&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6374cbb7255276f3a22b4b35&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7cf1bbb83447441e5fa2e1e4fcf7617b.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Peter Tong&quot;,&quot;user&quot;:&quot;tsbpp&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6190bb39d221f4281b3833b9&quot;,&quot;avatarUrl&quot;:&quot;/avatars/45900996e281967275d1bc2cee5f88b0.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;lingfeng shen&quot;,&quot;user&quot;:&quot;xiaoyaoyou&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;620783f24e28382272337ba4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/620783f24e28382272337ba4/zkUveQPNiDfYjgGhuFErj.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;GuoLiangTang&quot;,&quot;user&quot;:&quot;Tommy930&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6270324ebecab9e2dcf245de&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6270324ebecab9e2dcf245de/cMbtWSasyNlYc9hvsEEzt.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kye Gomez&quot;,&quot;user&quot;:&quot;kye&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0}\"> Papers arxiv:2509.26625","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:36.371Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.26625","pdf_url":"","scraped_at":"2025-10-01T09:31:36.371Z","abstract_quality":6,"analysis":{"introduction":"🚀 Ever wondered if language-only LLMs can 'learn to see'?\nThis paper shows LLMs acquire rich visual priors during language pre-training — separable perception vs reasoning priors — letting vision skills be unlocked with little multimodal data. Crucial for data-efficient multimodal AI.","challenges":"🎯 Key problems tackled:\n- Lack of understanding how text-only pretraining yields visual abilities.\n- Data-inefficient vision alignment: large multimodal sets normally required.\n- No clear recipe for cultivating transferable visual reasoning from LLMs.","innovations":"✨ Core contributions:\n- Systematic decomposition of visual priors into perception vs reasoning.\n- Evidence that reasoning priors arise from reasoning-centric corpora (code/math/academia).\n- Data-centric recipe + 1T-token pretraining verification and the MLE-Bench for analysis.","experiments":"📊 Most compelling quantitative result: Not specified in the paper.\nExperimental breakthrough: Demonstrated separable perception & reasoning priors; reasoning prior scales with reasoning-centric text and transfers to visual reasoning. Verified via 100+ controlled experiments (≈500k GPU‑hours) and 1T-token pretraining.","insights":"🤔 What's next?\n- Investigate targeted pretraining to induce specific visual priors (e.g., spatial vs semantic).\n- Explore encoder-design & visual tuning to better surface perception priors.\nPotential apps: low-data multimodal assistants, robotics perception bootstrapping. Could deliberate pretraining reshape multimodal AI?","keywords":["LLMs","visual priors","multimodal","language pre-training","perception","reasoning","data-centric","MLE-Bench"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"\"你是否曾好奇，纯语言大模型（LLMs）能否“学会看”？本文表明，LLMs 在语言预训练过程中获得了丰富的视觉先验知识——这些先验知识可分离为感知先验和推理先验——从而使得仅需少量多模态数据就能激活其视觉技能。这对于构建数据高效的多模态AI至关重要。\"","chinese_challenges":"{\n  \"challenges\": [\n    \"缺乏对纯文本预训练如何产生视觉能力的理解。\",\n    \"数据效率低下的视觉对齐：通常需要大规模多模态数据集。\",\n    \"缺乏明确的方法来培养大语言模型（LLMs）的可迁移视觉推理能力。\"\n  ]\n}","chinese_innovations":"\"核心贡献：\\n- 系统性地将视觉先验分解为感知先验与推理先验。\\n- 证明推理先验来源于以推理为中心的语料库（如代码、数学、学术资料）。\\n- 提出了以数据为中心的方案，并通过万亿（1T）词元预训练进行验证，同时构建了用于分析的MLE-Bench基准。\"","chinese_experiments":"{\n  \"name\": \"The Lord of the Rings: The Fellowship of the Ring\",\n  \"director\": \"Peter Jackson\",\n  \"year\": 2001,\n  \"runtime_minutes\": 178,\n  \"genre\": [\n    \"Adventure\",\n    \"Fantasy\",\n    \"Drama\"\n  ],\n  \"cast\": [\n    {\n      \"actor\": \"Elijah Wood\",\n      \"character\": \"Frodo Baggins\"\n    },\n    {\n      \"actor\": \"Ian McKellen\",\n      \"character\": \"Gandalf\"\n    },\n    {\n      \"actor\": \"Viggo Mortensen\",\n      \"character\": \"Aragorn\"\n    }\n  ],\n  \"box_office_usd\": 898204459.0,\n  \"is_part_of_trilogy\": true,\n  \"rating_imdb\": 8.8,\n  \"awards_won\": [\n    \"Academy Award for Best Cinematography\",\n    \"Academy Award for Best Makeup\",\n    \"Academy Award for Best Original Score\",\n    \"Academy Award for Best Visual Effects\"\n  ]\n}","chinese_insights":"{\n  \"request\": \"Translate the following English content into Simplified Chinese. The translation must be accurate, natural, and suitable for AI researchers and enthusiasts. Maintain the professionalism of technical terms, but use accessible language when explaining complex concepts. Only return the translated Chinese text, without any extra explanations or formatting.\",\n  \"english_content\": {\n    \"title\": \"🤔 What's next?\",\n    \"points\": [\n      \"Investigate targeted pretraining to induce specific visual priors (e.g., spatial vs semantic).\",\n      \"Explore encoder-design & visual tuning to better surface perception priors.\"\n    ],\n    \"applications\": \"Potential apps: low-data multimodal assistants, robotics perception bootstrapping.\",\n    \"question\": \"Could deliberate pretraining reshape multimodal AI?\"\n  },\n  \"translation_analysis\": {\n    \"targeted_pretraining\": \"目标导向的预训练 / 针对性预训练\",\n    \"visual_priors\": \"视觉先验（知识）\",\n    \"spatial_vs_semantic\": \"空间性 vs 语义性\",\n    \"encoder_design\": \"编码器设计\",\n    \"visual_tuning\": \"视觉调优 / 视觉微调\",\n    \"surface_perception_priors\": \"更好地显现/挖掘感知先验\",\n    \"low_data_multimodal_assistants\": \"低数据量多模态助手\",\n    \"robotics_perception_bootstrapping\": \"机器人感知自举 / 机器人感知引导\",\n    \"deliberate_pretraining\": \"审慎的预训练 / 精心设计的预训练\"\n  },\n  \"simplified_chinese_translation\": \"🤔 下一步是什么？\\n- 研究目标导向的预训练，以诱导特定的视觉先验（例如，空间性 vs 语义性）。\\n- 探索编码器设计和视觉调优，以更好地显现感知先验。\\n潜在应用：低数据量多模态助手、机器人感知自举。审慎的预训练能否重塑多模态AI？\"\n}","summary":"**Introduction:** 🚀 Ever wondered if language-only LLMs can 'learn to see'?\nThis paper shows LLMs acquire rich visual priors during language pre-training — separable perception vs reasoning priors — letting vision skills be unlocked with little multimodal data. Crucial for data-efficient multimodal AI.\n\n**Challenges:** 🎯 Key problems tackled:\n- Lack of understanding how text-only pretraining yields visual abilities.\n- Data-ineffic...","analyzed_at":"2025-10-01T13:32:59.973Z","model":"openai/gpt-5-mini"}},{"id":"hf_vitabench__benchmarking_llm_agents_with_versatile_interactive_tasks_in_real_world_applications_1759311099288","title":"VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications","abstract":"The code, dataset, and leaderboard are available at https://vitabench.github.io/\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:08:43.287Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66ecee857264238429a1211f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg&quot;,&quot;fullname&quot;:&quot;Wei He&quot;,&quot;name&quot;:&quot;hewei2001&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.8008732795715332},&quot;editors&quot;:[&quot;hewei2001&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.26490&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a7e&quot;,&quot;name&quot;:&quot;Wei He&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a7f&quot;,&quot;name&quot;:&quot;Yueqing Sun&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a80&quot;,&quot;name&quot;:&quot;Hongyan Hao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a81&quot;,&quot;name&quot;:&quot;Xueyuan Hao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a82&quot;,&quot;name&quot;:&quot;Zhikang Xia&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a83&quot;,&quot;name&quot;:&quot;Qi Gu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a84&quot;,&quot;name&quot;:&quot;Chengcheng Han&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a85&quot;,&quot;name&quot;:&quot;Dengchang Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a86&quot;,&quot;name&quot;:&quot;Hui Su&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a87&quot;,&quot;name&quot;:&quot;Kefeng Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a88&quot;,&quot;name&quot;:&quot;Man Gao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a89&quot;,&quot;name&quot;:&quot;Xi Su&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a8a&quot;,&quot;name&quot;:&quot;Xiaodong Cai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a8b&quot;,&quot;name&quot;:&quot;Xunliang Cai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a8c&quot;,&quot;name&quot;:&quot;Yu Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a8d&quot;,&quot;name&quot;:&quot;Yunke Zhao&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-30T16:33:49.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:38:43.265Z&quot;,&quot;title&quot;:&quot;VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\\n Real-world Applications&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;66ecee857264238429a1211f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wei He&quot;,&quot;user&quot;:&quot;hewei2001&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;As LLM-based agents are increasingly deployed in real-life scenarios,\\nexisting benchmarks fail to capture their inherent complexity of handling\\nextensive information, leveraging diverse resources, and managing dynamic user\\ninteractions. To address this gap, we introduce VitaBench, a challenging\\nbenchmark that evaluates agents on versatile interactive tasks grounded in\\nreal-world settings. Drawing from daily applications in food delivery, in-store\\nconsumption, and online travel services, VitaBench presents agents with the\\nmost complex life-serving simulation environment to date, comprising 66 tools.\\nThrough a framework that eliminates domain-specific policies, we enable\\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\\nmultiple real user requests and requires agents to reason across temporal and\\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\\ninstructions, and track shifting user intent throughout multi-turn\\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\\nenabling robust assessment of diverse solution pathways in complex environments\\nand stochastic interactions. Our comprehensive evaluation reveals that even the\\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\\nless than 50% success rate on others. Overall, we believe VitaBench will serve\\nas a valuable resource for advancing the development of AI agents in practical\\nreal-world applications. The code, dataset, and leaderboard are available at\\nhttps://vitabench.github.io/&quot;,&quot;upvotes&quot;:13,&quot;discussionId&quot;:&quot;68dc8ca44159d1f2418f9a8e&quot;,&quot;projectPage&quot;:&quot;https://vitabench.github.io/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/meituan/vitabench&quot;,&quot;ai_summary&quot;:&quot;VitaBench is a benchmark for evaluating LLM-based agents in complex, real-world interactive tasks using a diverse set of tools and scenarios.&quot;,&quot;ai_keywords&quot;:[&quot;LLM-based agents&quot;,&quot;VitaBench&quot;,&quot;interactive tasks&quot;,&quot;real-world settings&quot;,&quot;food delivery&quot;,&quot;in-store consumption&quot;,&quot;online travel services&quot;,&quot;life-serving simulation environment&quot;,&quot;domain-specific policies&quot;,&quot;flexible composition&quot;,&quot;cross-scenario tasks&quot;,&quot;single-scenario tasks&quot;,&quot;real user requests&quot;,&quot;temporal dimensions&quot;,&quot;spatial dimensions&quot;,&quot;complex tool sets&quot;,&quot;ambiguous instructions&quot;,&quot;shifting user intent&quot;,&quot;multi-turn conversations&quot;,&quot;rubric-based sliding window evaluator&quot;,&quot;stochastic interactions&quot;],&quot;githubStars&quot;:2},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;66ecee857264238429a1211f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wei He&quot;,&quot;user&quot;:&quot;hewei2001&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;643910dbabdc6ce5351e4eb5&quot;,&quot;avatarUrl&quot;:&quot;/avatars/92ec189cd4325b4d85fdfcd59f1ff1e3.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yueqing Sun&quot;,&quot;user&quot;:&quot;leqing&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;619ddd708ae9cafd72ab20d5&quot;,&quot;avatarUrl&quot;:&quot;/avatars/6b44e4928de0fc27287bf922c3f1802d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chengcheng Han&quot;,&quot;user&quot;:&quot;hccngu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66f547c5405af2cd3256ba27&quot;,&quot;avatarUrl&quot;:&quot;/avatars/fc52bac1785191af0eb9a1d108c4397e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;JP Zhu&quot;,&quot;user&quot;:&quot;JPZhu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64eb4f5507987950ae5e2b0f&quot;,&quot;avatarUrl&quot;:&quot;/avatars/77dc21b195bc94490e45bfe208abfbc4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jiapeng Zhu&quot;,&quot;user&quot;:&quot;JasonZhujp&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;650819ad95c45596854271a3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/9517fd62eef104c33884dfb3f64249bf.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;wen&quot;,&quot;user&quot;:&quot;cindywen&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63340dbbd92c5842ae71d1e9&quot;,&quot;avatarUrl&quot;:&quot;/avatars/3a3182996bd41b526dcbfa8687d91963.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kanzhi Cheng&quot;,&quot;user&quot;:&quot;cckevinn&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;636f526a6cd69d9a36ff2b53&quot;,&quot;avatarUrl&quot;:&quot;/avatars/8f2271a193fcac609d9be270552b5afa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Qiguang Chen&quot;,&quot;user&quot;:&quot;LightChen2333&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66ba08d4bafd993a5428e051&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2bd7779582ee3ae2ff9688e1627d745e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kangyang Luo&quot;,&quot;user&quot;:&quot;lKangyang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64beb69801f1983a86a05de2&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64beb69801f1983a86a05de2/tFyCoqZ6gT8NWkZfuncID.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chuanyang Jin&quot;,&quot;user&quot;:&quot;Chuanyang-Jin&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63a3eb8af460e4379b5991e7&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7564a048d8496cac38d689178d90a8f9.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xiaohan Xu&quot;,&quot;user&quot;:&quot;Tebmer&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;68b28d79a176a9beb30d2049&quot;,&quot;name&quot;:&quot;meituan-longcat&quot;,&quot;fullname&quot;:&quot;LongCat&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png&quot;}}\"> Papers arxiv:2509.26490","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:39.288Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.26490","pdf_url":"","scraped_at":"2025-10-01T09:31:39.288Z","abstract_quality":6,"analysis":{"introduction":"🚀 How well can LLM agents handle messy, multi-tool real-world tasks? \nVitaBench introduces a life-serving benchmark that stresses agents with versatile interactive tasks across food delivery, in-store consumption, and travel services. \nBuilt to push real-world agent progress.","challenges":"🎯 Key problems addressed:\n- Existing benchmarks fail to capture agents' complexity in handling extensive info and diverse resources.\n- Difficulty modeling dynamic, multi-turn user interactions with shifting intent.\n- Lack of flexible cross-domain tool composition for realistic evaluation.","innovations":"✨ Core innovations:\n- VitaBench: a life-serving simulation benchmark grounded in real apps (food delivery, in-store, travel).\n- Large tool ecosystem: 66 tools and a framework that removes domain-specific policies for flexible composition.\n- Task suite: 100 cross-scenario tasks + 300 single-scenario tasks from real user requests.\n- Rubric-based sliding-window evaluator to score diverse solution paths and stochastic interactions.","experiments":"📊 Main result: Even the most advanced models reach only ~30% success on cross-scenario tasks and <50% on single-scenario tasks. \nThis demonstrates substantial gaps remain for agents in realistic, tool-rich, multi-turn settings.","insights":"🤔 Next steps & implications:\n- Research: train agents for robust multi-tool coordination and temporal/spatial reasoning; improve evaluation for stochastic interactions.\n- Applications: stronger multi-domain personal assistants, service automation in delivery and travel.\nCould better tool-aware learning close the large performance gap?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"\"🚀 LLM智能体处理复杂、多工具的真实世界任务的能力如何？\\nVitaBench引入了一个生活服务基准，通过涵盖外卖配送、店内消费和旅行服务等领域的多样化交互任务，对智能体进行严格考验。\\n旨在推动真实世界智能体的发展和进步。\"","chinese_challenges":"[\n  \"🎯 核心解决的问题：\",\n  \"- 现有基准未能捕捉智能体在处理海量信息和多样化资源时的复杂性。\",\n  \"- 难以对意图不断变化的动态多轮用户交互进行建模。\",\n  \"- 缺乏灵活的跨领域工具组合机制，难以进行现实场景的评估。\"\n]","chinese_innovations":"{\n  \"核心创新\": [\n    \"VitaBench：一个服务于生活的仿真基准测试，植根于真实应用（餐饮外卖、店内购物、旅行）。\",\n    \"大型工具生态系统：包含66种工具和一个框架，该框架移除了特定领域策略，实现了灵活的组合。\",\n    \"任务套件：包含100个跨场景任务和300个源自真实用户请求的单场景任务。\",\n    \"基于评分标准的滑动窗口评估器，用于对多样化的解决方案路径和随机交互进行评分。\"\n  ]\n}","chinese_experiments":"\"主要结果：即使是最先进的模型，在跨场景任务上的成功率也仅达到约30%，而在单场景任务上的成功率则低于50%。这表明在真实的、工具丰富的、多轮次的设置中，智能体仍存在巨大的差距。\"","chinese_insights":"\"🤔 后续步骤与潜在影响：\\n- 研究方向：训练智能体以实现鲁棒的多工具协调和时空推理；改进对随机交互的评估。\\n- 应用方向：更强大的多领域个人助理；实现配送和旅行中的服务自动化。\\n更好的工具感知学习能否缩小巨大的性能差距？\"","summary":"**Introduction:** 🚀 How well can LLM agents handle messy, multi-tool real-world tasks? \nVitaBench introduces a life-serving benchmark that stresses agents with versatile interactive tasks across food delivery, in-store consumption, and travel services. \nBuilt to push real-world agent progress.\n\n**Challenges:** 🎯 Key problems addressed:\n- Existing benchmarks fail to capture agents' complexity in handling extensive info and diverse ...","analyzed_at":"2025-10-01T13:34:09.230Z","model":"openai/gpt-5-mini"}},{"id":"hf_dparallel__learnable_parallel_decoding_for_dllms_1759311102154","title":"dParallel: Learnable Parallel Decoding for dLLMs","abstract":"We present dParallel, a novel method that unlocks the inherent parallelism of dLLMs for fast sampling. Our paper, code, models, and dataset are all available now!\\nCode: https://github.com/czg1225/dParallelPaper: https://arxiv.org/pdf/2509.26488Model: https://huggingface.co/Zigeng/dParallel-LLaDA-8B-instructData: https://huggingface.co/datasets/Zigeng/dParallel_LLaDA_Distill_Data\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:04:29.960Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;65811eeaa2284a018e51f1ba&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;,&quot;fullname&quot;:&quot;Zigeng Chen&quot;,&quot;name&quot;:&quot;Zigeng&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:7}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7858399152755737},&quot;editors&quot;:[&quot;Zigeng&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.26488&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a47&quot;,&quot;name&quot;:&quot;Zigeng Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a48&quot;,&quot;name&quot;:&quot;Gongfan Fang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a49&quot;,&quot;name&quot;:&quot;Xinyin Ma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a4a&quot;,&quot;name&quot;:&quot;Ruonan Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a4b&quot;,&quot;name&quot;:&quot;Xinchao Wang&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/65811eeaa2284a018e51f1ba/0NK-gFnfpLThLyS0atsLH.mp4&quot;],&quot;publishedAt&quot;:&quot;2025-09-30T16:32:52.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:34:29.943Z&quot;,&quot;title&quot;:&quot;dParallel: Learnable Parallel Decoding for dLLMs&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;65811eeaa2284a018e51f1ba&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zigeng Chen&quot;,&quot;user&quot;:&quot;Zigeng&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Diffusion large language models (dLLMs) have recently drawn considerable\\nattention within the research community as a promising alternative to\\nautoregressive generation, offering parallel token prediction and lower\\ninference latency. Yet, their parallel decoding potential remains largely\\nunderexplored, as existing open-source models still require nearly token-length\\ndecoding steps to ensure performance. To address this, we introduce dParallel,\\na simple and effective method that unlocks the inherent parallelism of dLLMs\\nfor fast sampling. We identify that the key bottleneck to parallel decoding\\narises from the sequential certainty convergence for masked tokens. Building on\\nthis insight, we introduce the core of our approach: certainty-forcing\\ndistillation, a novel training strategy that distills the model to follow its\\noriginal sampling trajectories while enforcing it to achieve high certainty on\\nmasked tokens more rapidly and in parallel. Extensive experiments across\\nvarious benchmarks demonstrate that our method can dramatically reduce the\\nnumber of decoding steps while maintaining performance. When applied to the\\nLLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on\\nGSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP\\nbenchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup\\nwhile maintaining accuracy. Our code is available at\\nhttps://github.com/czg1225/dParallel&quot;,&quot;upvotes&quot;:13,&quot;discussionId&quot;:&quot;68dc88d74159d1f2418f9a4c&quot;,&quot;githubRepo&quot;:&quot;https://github.com/czg1225/dParallel&quot;,&quot;ai_summary&quot;:&quot;dParallel is a method that enhances the parallel decoding of diffusion large language models, significantly reducing decoding steps without compromising performance.&quot;,&quot;ai_keywords&quot;:[&quot;diffusion large language models&quot;,&quot;dLLMs&quot;,&quot;autoregressive generation&quot;,&quot;parallel token prediction&quot;,&quot;parallel decoding&quot;,&quot;masked tokens&quot;,&quot;certainty-forcing distillation&quot;,&quot;LLaDA-8B-Instruct&quot;,&quot;GSM8K&quot;,&quot;MBPP benchmark&quot;],&quot;githubStars&quot;:7},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;65811eeaa2284a018e51f1ba&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zigeng Chen&quot;,&quot;user&quot;:&quot;Zigeng&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;640ebdfefdeaae139086f4d8&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/640ebdfefdeaae139086f4d8/2N94gbHubplYD8njmUTPf.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Yuanshi&quot;,&quot;user&quot;:&quot;Yuanshi&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6683fc5344a65be1aab25dc0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e13cde3f87b59e418838d702807df3b5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;hjkim&quot;,&quot;user&quot;:&quot;hojie11&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6684b1a9986286e214df1e03&quot;,&quot;avatarUrl&quot;:&quot;/avatars/515efb62b0ec923ea525a90ea7aa9221.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;XinyinMa&quot;,&quot;user&quot;:&quot;XinyinHorseee&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6486fb33570a419f41a882e4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/860a42074439a23c629cd23851ae4da6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ruonan Yu&quot;,&quot;user&quot;:&quot;roseannelexie&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6342796a0875f2c99cfd313b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/98575092404c4197b20c929a6499a015.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuseung \\&quot;Phillip\\&quot; Lee&quot;,&quot;user&quot;:&quot;phillipinseoul&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;668e740f1173ab43d9d9ed5e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/caa9b47c2a5f6d6d679759b8b234a0ab.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zeqing Wang&quot;,&quot;user&quot;:&quot;INV-WZQ&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;620783f24e28382272337ba4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/620783f24e28382272337ba4/zkUveQPNiDfYjgGhuFErj.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;GuoLiangTang&quot;,&quot;user&quot;:&quot;Tommy930&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;634cfebc350bcee9bed20a4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xingyi Yang&quot;,&quot;user&quot;:&quot;adamdad&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6270324ebecab9e2dcf245de&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6270324ebecab9e2dcf245de/cMbtWSasyNlYc9hvsEEzt.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kye Gomez&quot;,&quot;user&quot;:&quot;kye&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;646a1939c37ca1e12308fe81&quot;,&quot;avatarUrl&quot;:&quot;/avatars/752e9d86018e7d33ad8bcd741203fd86.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Gongfan Fang&quot;,&quot;user&quot;:&quot;Vinnnf&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6508ab2b349930913196378b&quot;,&quot;name&quot;:&quot;NationalUniversityofSingapore&quot;,&quot;fullname&quot;:&quot;National University of Singapore&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png&quot;}}\"> Papers arxiv:2509.26488","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:42.154Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.26488","pdf_url":"","scraped_at":"2025-10-01T09:31:42.154Z","abstract_quality":5,"analysis":{"introduction":"🚀 What if diffusion LLMs could sample 8–10x faster? \ndParallel unlocks the parallel decoding potential of dLLMs by using a learnable distillation that forces rapid certainty on masked tokens — slashing decoding steps and cutting latency for real-time and research use. 🔥","challenges":"🎯 Key problems tackled:\n- dLLMs still need nearly token-length decoding steps, limiting speed.\n- Sequential certainty convergence of masked tokens blocks true parallel decoding.\n- Hard to reduce steps without hurting accuracy or changing sampling trajectories.","innovations":"✨ Core ideas:\n- dParallel: a method to enable fast parallel sampling in dLLMs.\n- Certainty-forcing distillation: distill models to follow original sampling trajectories while forcing high certainty on masked tokens faster and in parallel.\n- Simple, training-based fix that unlocks inherent dLLM parallelism.","experiments":"📊 Standout result: Applied to LLaDA-8B-Instruct, dParallel reduces decoding steps on GSM8K from 256 → 30 (≈8.5× speedup) while maintaining performance. \n(Also: MBPP 256 → 24 ≈10.5× speedup with accuracy preserved.)","insights":"🤔 Where this could go next:\n- Explore scaling to larger dLLMs and multimodal diffusion LMs to bring low-latency sampling to more tasks.\n- Combine with hardware-aware scheduling / quantization for on-device, interactive LMs.\nCould this make diffusion-based assistants practical in real time? 🚀","category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"英文内容不可用 / English content not available","chinese_introduction":"翻译失败，请查看英文原文 / Translation failed, please see English original","chinese_challenges":"{\n  \"challenges\": \"🎯 解决的关键问题：\\n- dLLMs（解码器大语言模型）仍然需要接近于 token 长度的解码步骤，限制了速度。\\n- 掩码 token 的顺序确定性收敛阻碍了真正的并行解码。\\n- 很难在不损害准确性或改变采样轨迹的情况下减少解码步骤。\"\n}","chinese_innovations":"{\n  \"核心思想\": [\n    \"dParallel：一种在 dLLM 中实现快速并行采样的方法。\",\n    \"强制确定性蒸馏：蒸馏模型以遵循原始采样轨迹，同时更快、更并行地对被掩码的 token 强制施加高确定性。\",\n    \"一种简单的、基于训练的修正方法，能够释放 dLLM 固有的并行性。\"\n  ]\n}","chinese_experiments":"[\n  {\n    \"title\": \"实验结果分析\",\n    \"content\": \"📊 突出成果：将 dParallel 应用于 LLaDA-8B-Instruct 模型，在 GSM8K 数据集上，解码步骤从 256 减少到 30（约 8.5 倍加速），同时保持了性能。 \\n（此外：在 MBPP 数据集上，解码步骤从 256 减少到 24，实现了约 10.5 倍加速，且准确率得以保持。）\"\n  }\n]","chinese_insights":"{\n  \"translation\": \"🤔 接下来的发展方向：\\n- 探索扩展到更大的dLLMs（扩散式大型语言模型）和多模态扩散式语言模型，将低延迟采样带到更多任务中。\\n- 与硬件感知调度/量化相结合，实现设备上的交互式语言模型。\\n这能否使基于扩散的助手在实时应用中变得实用？🚀\"\n}","summary":"**Introduction:** 🚀 What if diffusion LLMs could sample 8–10x faster? \ndParallel unlocks the parallel decoding potential of dLLMs by using a learnable distillation that forces rapid certainty on masked tokens — slashing decoding steps and cutting latency for real-time and research use. 🔥\n\n**Challenges:** 🎯 Key problems tackled:\n- dLLMs still need nearly token-length decoding steps, limiting speed.\n- Sequential certainty convergen...","analyzed_at":"2025-10-01T14:12:20.197Z","model":"openai/gpt-5-mini"}}]