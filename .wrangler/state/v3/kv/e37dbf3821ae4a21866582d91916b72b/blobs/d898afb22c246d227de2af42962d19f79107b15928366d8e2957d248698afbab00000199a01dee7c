[{"id":"arxiv_2509.26644v1","arxiv_id":"2509.26644v1","title":"Stitch: Training-Free Position Control in Multimodal Diffusion\n  Transformers","abstract":"Text-to-Image (T2I) generation models have advanced rapidly in recent years,\nbut accurately capturing spatial relationships like \"above\" or \"to the right\nof\" poses a persistent challenge. Earlier methods improved spatial relationship\nfollowing with external position control. However, as architectures evolved to\nenhance image quality, these techniques became incompatible with modern models.\nWe propose Stitch, a training-free method for incorporating external position\ncontrol into Multi-Modal Diffusion Transformers (MMDiT) via\nautomatically-generated bounding boxes. Stitch produces images that are both\nspatially accurate and visually appealing by generating individual objects\nwithin designated bounding boxes and seamlessly stitching them together. We\nfind that targeted attention heads capture the information necessary to isolate\nand cut out individual objects mid-generation, without needing to fully\ncomplete the image. We evaluate Stitch on PosEval, our benchmark for\nposition-based T2I generation. Featuring five new tasks that extend the concept\nof Position beyond the basic GenEval task, PosEval demonstrates that even top\nmodels still have significant room for improvement in position-based\ngeneration. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances\nbase models, even improving FLUX by 218% on GenEval's Position task and by 206%\non PosEval. Stitch achieves state-of-the-art results with Qwen-Image on\nPosEval, improving over previous models by 54%, all accomplished while\nintegrating position control into leading models training-free. Code is\navailable at https://github.com/ExplainableML/Stitch.","authors":["Jessica Bader","Mateusz Pach","Maria A. Bravo","Serge Belongie","Zeynep Akata"],"published":"2025-09-30T17:59:51Z","updated":"2025-09-30T17:59:51Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26644v1","pdf_url":"http://arxiv.org/pdf/2509.26644v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ Struggling to get T2I models to place â€œcat to the right of the vaseâ€ accurately?\nStitch is a training-free method that adds external position control to Multimodal Diffusion Transformers via auto-generated bounding boxes â€” producing spatially accurate, high-quality images.\nWho benefits: researchers and apps needing precise layout control.","challenges":"ğŸ¯ Key problems tackled:\n- Existing T2I models often fail to capture spatial relations like â€œaboveâ€ or â€œto the right ofâ€.\n- Prior position-control methods are incompatible with modern model architectures.\n- No training-free way to add position control to leading Multimodal Diffusion Transformers (MMDiT).","innovations":"âœ¨ Core ideas:\n- Stitch: a training-free pipeline that injects external position control into MMDiT using automatically-generated bounding boxes.\n- Uses targeted attention heads to isolate and cut out individual objects mid-generation.\n- Seamlessly stitches object crops back to form final, spatially-accurate images.\nNovelty: training-free integration into modern MMDiT and leveraging attention heads for mid-generation object isolation.","experiments":"ğŸ“Š Results & proof:\n- Biggest win: +218% improvement (FLUX on GenEval Position task).\n- Also: +206% on PosEval and Stitch achieves SOTA with Qwen-Image on PosEval (+54% over prior models).\nThis proves training-free position control substantially improves spatial accuracy in top models.","insights":"ğŸ¤” What's next?\n- Research directions: explore extending Stitch to video/dynamic layouts or interactive user-guided box editing during generation.\n- Applications: layout-aware design tools, scene planning for robotics, and guided content creation.\nCould this make T2I reliably layout-aware across real apps?","category":"computer_vision","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"chinese_translation\": \"ğŸš€ è¿˜åœ¨ä¸º T2I æ¨¡å‹æ— æ³•ç²¾ç¡®åœ°å°†â€œçŒ«æ”¾åœ¨èŠ±ç“¶å³è¾¹â€è€Œè‹¦æ¼å—ï¼Ÿ\\n\\nStitch æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„è¾¹ç•Œæ¡†ï¼Œä¸ºå¤šæ¨¡æ€æ‰©æ•£ Transformer å¢åŠ äº†å¤–éƒ¨ä½ç½®æ§åˆ¶ï¼Œä»è€Œç”Ÿæˆç©ºé—´ç²¾ç¡®ã€é«˜è´¨é‡çš„å›¾åƒã€‚\\n\\nå—ç›Šè€…ï¼šéœ€è¦ç²¾ç¡®å¸ƒå±€æ§åˆ¶çš„ç ”ç©¶äººå‘˜å’Œåº”ç”¨ç¨‹åºå¼€å‘è€…ã€‚\"\n}","chinese_challenges":"[\n  \"ğŸ¯ è§£å†³çš„å…³é”®é—®é¢˜ï¼š\",\n  \"- ç°æœ‰çš„T2Iæ¨¡å‹é€šå¸¸éš¾ä»¥å‡†ç¡®æ•æ‰â€œåœ¨...ä¸Šæ–¹â€æˆ–â€œåœ¨...å³ä¾§â€ç­‰ç©ºé—´å…³ç³»ã€‚\",\n  \"- ä»¥å¾€çš„ä½ç½®æ§åˆ¶æ–¹æ³•ä¸ç°ä»£æ¨¡å‹æ¶æ„ä¸å…¼å®¹ã€‚\",\n  \"- ç¼ºä¹æ— éœ€è®­ç»ƒå³å¯ä¸ºé¢†å…ˆçš„å¤šæ¨¡æ€æ‰©æ•£Transformer (MMDiT) æ·»åŠ ä½ç½®æ§åˆ¶çš„æ–¹æ³•ã€‚\"\n]","chinese_innovations":"{\n  \"translation\": \"âœ¨ æ ¸å¿ƒæ€æƒ³ï¼š\\n- Stitchï¼šä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æµç¨‹ï¼Œå®ƒåˆ©ç”¨è‡ªåŠ¨ç”Ÿæˆçš„è¾¹ç•Œæ¡†å°†å¤–éƒ¨ä½ç½®æ§åˆ¶æ³¨å…¥åˆ° MMDiT ä¸­ã€‚\\n- ä½¿ç”¨å®šå‘æ³¨æ„åŠ›å¤´æ¥éš”ç¦»å¹¶â€œå‰ªåˆ‡â€å‡ºç”Ÿæˆè¿‡ç¨‹ä¸­çš„å•ä¸ªå¯¹è±¡ã€‚\\n- å°†å¯¹è±¡è£å‰ªæ— ç¼åœ°æ‹¼æ¥å›å»ï¼Œå½¢æˆæœ€ç»ˆåœ¨ç©ºé—´ä¸Šå‡†ç¡®çš„å›¾åƒã€‚\\næ–°é¢–æ€§ï¼šæ— éœ€è®­ç»ƒå³å¯é›†æˆåˆ°ç°ä»£ MMDiT æ¶æ„ä¸­ï¼Œå¹¶åˆ©ç”¨æ³¨æ„åŠ›å¤´å®ç°ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¯¹è±¡éš”ç¦»ã€‚\"\n}","chinese_experiments":"{\n  \"chinese_translation\": \"ğŸ“Š ç»“æœä¸è¯æ˜ï¼š\\n- æœ€å¤§çªç ´ï¼šåœ¨ GenEval ä½ç½®ä»»åŠ¡ä¸Šï¼ŒFLUX å®ç°äº† +218% çš„æå‡ã€‚\\n- æ­¤å¤–ï¼šåœ¨ PosEval ä¸Šå®ç°äº† +206% çš„æå‡ï¼Œå¹¶ä¸” Stitch æ¨¡å‹åœ¨ PosEval ä¸Šä½¿ç”¨ Qwen-Image è¾¾åˆ°äº† SOTAï¼ˆæ¯”ç°æœ‰æ¨¡å‹æå‡äº† +54%ï¼‰ã€‚\\nè¿™è¯æ˜äº†å…è®­ç»ƒçš„ä½ç½®æ§åˆ¶æ˜¾è‘—æé«˜äº†é¡¶çº§æ¨¡å‹çš„ç©ºé—´å‡†ç¡®æ€§ã€‚\"\n}","chinese_insights":"{\n  \"translation\": \"ğŸ¤” ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ\\n- ç ”ç©¶æ–¹å‘ï¼šæ¢ç´¢å°† Stitch æ‰©å±•åˆ°è§†é¢‘/åŠ¨æ€å¸ƒå±€ï¼Œæˆ–åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å®ç°äº¤äº’å¼ç”¨æˆ·å¼•å¯¼çš„æ¡†ç¼–è¾‘åŠŸèƒ½ã€‚\\n- åº”ç”¨ï¼šå¸ƒå±€æ„ŸçŸ¥è®¾è®¡å·¥å…·ã€æœºå™¨äººåœºæ™¯è§„åˆ’ä»¥åŠå¼•å¯¼å¼å†…å®¹åˆ›ä½œã€‚\\nè¿™èƒ½å¦ä½¿ T2Iï¼ˆæ–‡æœ¬åˆ°å›¾åƒï¼‰æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­å¯é åœ°å…·å¤‡å¸ƒå±€æ„ŸçŸ¥èƒ½åŠ›ï¼Ÿ\"\n}","summary":"**Introduction:** ğŸš€ Struggling to get T2I models to place â€œcat to the right of the vaseâ€ accurately?\nStitch is a training-free method that adds external position control to Multimodal Diffusion Transformers via auto-generated bounding boxes â€” producing spatially accurate, high-quality images.\nWho benefits: researchers and apps needing precise layout control.\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Existing T2I models often fail...","analyzed_at":"2025-10-01T09:32:25.791Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26645v1","arxiv_id":"2509.26645v1","title":"TTT3R: 3D Reconstruction as Test-Time Training","abstract":"Modern Recurrent Neural Networks have become a competitive architecture for\n3D reconstruction due to their linear-time complexity. However, their\nperformance degrades significantly when applied beyond the training context\nlength, revealing limited length generalization. In this work, we revisit the\n3D reconstruction foundation models from a Test-Time Training perspective,\nframing their designs as an online learning problem. Building on this\nperspective, we leverage the alignment confidence between the memory state and\nincoming observations to derive a closed-form learning rate for memory updates,\nto balance between retaining historical information and adapting to new\nobservations. This training-free intervention, termed TTT3R, substantially\nimproves length generalization, achieving a $2\\times$ improvement in global\npose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU\nmemory to process thousands of images. Code available in\nhttps://rover-xingyu.github.io/TTT3R","authors":["Xingyu Chen","Yue Chen","Yuliang Xiu","Andreas Geiger","Anpei Chen"],"published":"2025-09-30T17:59:51Z","updated":"2025-09-30T17:59:51Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26645v1","pdf_url":"http://arxiv.org/pdf/2509.26645v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ Ever tried a 3D recon model that works in training but collapses on longer sequences? TTT3R treats recurrent 3D reconstruction as online Test-Time Training: a training-free tweak that adapts memory updates on the fly, boosting length generalization for mapping & robotics.","challenges":"ğŸ¯ Problems tackled:\n- Existing recurrent 3D reconstruction models fail when applied beyond training context length (limited length generalization).\n- Balancing retention of historical memory vs. adapting to new observations is hard.\n- Need for efficient, scalable processing of thousands of images on limited GPU RAM.","innovations":"âœ¨ Core ideas:\n- Reframe recurrent 3D reconstruction as an online Test-Time Training problem.\n- Derive a closed-form learning rate from alignment confidence between memory state and incoming observations.\n- Training-free intervention (TTT3R) that adjusts memory updates to balance past vs new info, enabling realtime, low-memory operation.","experiments":"ğŸ“Š Key result: TTT3R yields a 2Ã— improvement in global pose estimation over baselines, demonstrating substantially improved length generalization â€” while running at ~20 FPS and using only ~6 GB GPU to process thousands of images.","insights":"ğŸ¤” Next steps & applications:\n- Explore combining TTT3R with learned adaptation or other architectures (e.g., Transformers) to extend generalization.\n- Apply to long-horizon SLAM, autonomous mapping, and edge robotics where memory/compute are limited.\nCould test adaptive schemes for dynamic scenes or multi-agent mapping.","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"title\": \"TTT3R: åœ¨çº¿æµ‹è¯•æ—¶è®­ç»ƒæå‡å¾ªç¯3Dé‡å»ºçš„é•¿åº¦æ³›åŒ–èƒ½åŠ›\",\n  \"introduction\": \"ğŸš€ æ‚¨æ˜¯å¦æ›¾é‡åˆ°è¿‡è¿™æ ·çš„3Dé‡å»ºæ¨¡å‹ï¼šå®ƒåœ¨è®­ç»ƒé˜¶æ®µè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†è¾ƒé•¿åºåˆ—æ—¶å´ä¼šå¤±æ•ˆï¼ˆæˆ–å´©æºƒï¼‰ï¼ŸTTT3Rå°†å¾ªç¯3Dé‡å»ºè§†ä¸ºä¸€ç§åœ¨çº¿æµ‹è¯•æ—¶è®­ç»ƒï¼ˆonline Test-Time Trainingï¼‰æ–¹æ³•ï¼šè¿™æ˜¯ä¸€ç§å…è®­ç»ƒçš„è°ƒæ•´æ‰‹æ®µï¼Œèƒ½å¤Ÿå®æ—¶åœ°è‡ªé€‚åº”å†…å­˜æ›´æ–°ï¼Œä»è€Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å»ºå›¾ä¸æœºå™¨äººæŠ€æœ¯åº”ç”¨ä¸­çš„é•¿åº¦æ³›åŒ–èƒ½åŠ›ã€‚\",\n  \"technical_terms\": [\n    \"3Dé‡å»ºæ¨¡å‹ (3D recon model)\",\n    \"é•¿åº¦æ³›åŒ– (length generalization)\",\n    \"å¾ªç¯3Dé‡å»º (recurrent 3D reconstruction)\",\n    \"åœ¨çº¿æµ‹è¯•æ—¶è®­ç»ƒ (online Test-Time Training)\",\n    \"å…è®­ç»ƒçš„è°ƒæ•´ (training-free tweak)\",\n    \"å†…å­˜æ›´æ–° (memory updates)\",\n    \"å»ºå›¾ä¸æœºå™¨äººæŠ€æœ¯ (mapping & robotics)\"\n  ]\n}","chinese_challenges":"[\n  \"ç°æœ‰çš„å¾ªç¯ï¼ˆRecurrentï¼‰3Dé‡å»ºæ¨¡å‹åœ¨åº”ç”¨äºè¶…å‡ºè®­ç»ƒä¸Šä¸‹æ–‡é•¿åº¦çš„åºåˆ—æ—¶ä¼šå¤±æ•ˆï¼ˆé•¿åº¦æ³›åŒ–èƒ½åŠ›å—é™ï¼‰ã€‚\",\n  \"éš¾ä»¥å¹³è¡¡å†å²è®°å¿†çš„ä¿ç•™ä¸å¯¹æ–°è§‚æµ‹æ•°æ®çš„é€‚åº”ã€‚\",\n  \"éœ€è¦åœ¨æœ‰é™çš„GPUæ˜¾å­˜ä¸‹ï¼Œå¯¹æ•°åƒå¼ å›¾åƒè¿›è¡Œé«˜æ•ˆä¸”å¯æ‰©å±•çš„å¤„ç†ã€‚\"\n]","chinese_innovations":"{\n  \"innovations\": [\n    {\n      \"core_ideas\": \"æ ¸å¿ƒæ€æƒ³\"\n    },\n    {\n      \"point_1\": \"å°†å¾ªç¯ä¸‰ç»´é‡å»ºï¼ˆrecurrent 3D reconstructionï¼‰é‡æ–°å®šä¹‰ä¸ºä¸€ç§åœ¨çº¿æµ‹è¯•æ—¶è®­ç»ƒï¼ˆTest-Time Training, TTTï¼‰é—®é¢˜ã€‚\"\n    },\n    {\n      \"point_2\": \"æ ¹æ®è®°å¿†çŠ¶æ€ï¼ˆmemory stateï¼‰ä¸æ–°ä¼ å…¥è§‚æµ‹æ•°æ®ä¹‹é—´çš„å¯¹é½ç½®ä¿¡åº¦ï¼Œæ¨å¯¼å‡ºä¸€ä¸ªé—­å¼ï¼ˆclosed-formï¼‰å­¦ä¹ ç‡ã€‚\"\n    },\n    {\n      \"point_3\": \"å¼•å…¥å…è®­ç»ƒå¹²é¢„æœºåˆ¶ï¼ˆTTT3Rï¼‰ï¼Œè¯¥æœºåˆ¶ç”¨äºè°ƒæ•´è®°å¿†æ›´æ–°è¿‡ç¨‹ï¼Œä»¥å¹³è¡¡å†å²ä¿¡æ¯ä¸æ–°ä¼ å…¥ä¿¡æ¯ï¼Œä»è€Œå®ç°å®æ—¶ã€ä½å†…å­˜çš„è¿è¡Œã€‚\"\n    }\n  ]\n}","chinese_experiments":"{\n  \"translation\": \"å…³é”®ç»“æœï¼šTTT3R åœ¨å…¨å±€å§¿æ€ä¼°è®¡æ–¹é¢å®ç°äº†æ¯”åŸºçº¿æ–¹æ³•é«˜ 2 å€çš„æ€§èƒ½æå‡ï¼ŒåŒæ—¶æ˜¾è‘—å¢å¼ºäº†é•¿åº¦æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œå…¶è¿è¡Œé€Ÿåº¦çº¦ä¸º 20 å¸§/ç§’ï¼ˆFPSï¼‰ï¼Œä¸”ä»…å ç”¨çº¦ 6 GB æ˜¾å­˜å³å¯å¤„ç†æ•°åƒå¼ å›¾åƒã€‚\"\n}","chinese_insights":"{\n  \"translation\": {\n    \"title\": \"ğŸ¤” åç»­æ­¥éª¤ä¸åº”ç”¨\",\n    \"steps\": [\n      \"æ¢ç´¢å°† TTT3R ä¸ä¹ å¾—é€‚åº”ï¼ˆlearned adaptationï¼‰æˆ–å…¶ä»–æ¶æ„ï¼ˆä¾‹å¦‚ Transformerï¼‰ç»“åˆï¼Œä»¥æ‰©å±•å…¶æ³›åŒ–èƒ½åŠ›ã€‚\",\n      \"åº”ç”¨äºå†…å­˜/è®¡ç®—èµ„æºå—é™çš„é•¿ç¨‹ SLAMã€è‡ªä¸»å»ºå›¾å’Œè¾¹ç¼˜æœºå™¨äººé¢†åŸŸã€‚\",\n      \"å¯ä»¥é’ˆå¯¹åŠ¨æ€åœºæ™¯æˆ–å¤šæ™ºèƒ½ä½“å»ºå›¾æµ‹è¯•è‡ªé€‚åº”æ–¹æ¡ˆï¼ˆadaptive schemesï¼‰ã€‚\"\n    ]\n  }\n}","summary":"**Introduction:** ğŸš€ Ever tried a 3D recon model that works in training but collapses on longer sequences? TTT3R treats recurrent 3D reconstruction as online Test-Time Training: a training-free tweak that adapts memory updates on the fly, boosting length generalization for mapping & robotics.\n\n**Challenges:** ğŸ¯ Problems tackled:\n- Existing recurrent 3D reconstruction models fail when applied beyond training context length (limited ...","analyzed_at":"2025-10-01T09:37:58.176Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26642v1","arxiv_id":"2509.26642v1","title":"MLA: A Multisensory Language-Action Model for Multimodal Understanding\n  and Forecasting in Robotic Manipulation","abstract":"Vision-language-action models (VLAs) have shown generalization capabilities\nin robotic manipulation tasks by inheriting from vision-language models (VLMs)\nand learning action generation. Most VLA models focus on interpreting vision\nand language to generate actions, whereas robots must perceive and interact\nwithin the spatial-physical world. This gap highlights the need for a\ncomprehensive understanding of robotic-specific multisensory information, which\nis crucial for achieving complex and contact-rich control. To this end, we\nintroduce a multisensory language-action (MLA) model that collaboratively\nperceives heterogeneous sensory modalities and predicts future multisensory\nobjectives to facilitate physical world modeling. Specifically, to enhance\nperceptual representations, we propose an encoder-free multimodal alignment\nscheme that innovatively repurposes the large language model itself as a\nperception module, directly interpreting multimodal cues by aligning 2D images,\n3D point clouds, and tactile tokens through positional correspondence. To\nfurther enhance MLA's understanding of physical dynamics, we design a future\nmultisensory generation post-training strategy that enables MLA to reason about\nsemantic, geometric, and interaction information, providing more robust\nconditions for action generation. For evaluation, the MLA model outperforms the\nprevious state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex,\ncontact-rich real-world tasks, respectively, while also demonstrating improved\ngeneralization to unseen configurations. Project website:\nhttps://sites.google.com/view/open-mla","authors":["Zhuoyang Liu","Jiaming Liu","Jiadong Xu","Nuowei Han","Chenyang Gu","Hao Chen","Kaichen Zhou","Renrui Zhang","Kai Chin Hsieh","Kun Wu","Zhengping Che","Jian Tang","Shanghang Zhang"],"published":"2025-09-30T17:59:50Z","updated":"2025-09-30T17:59:50Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26642v1","pdf_url":"http://arxiv.org/pdf/2509.26642v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ Question: Can robots truly feel and predict the physical world, not just see and read it?\nMLA introduces a multisensory language-action model that fuses vision, 3D, and tactile cues and predicts future multisensory objectives to unlock contact-rich robotic manipulation.","challenges":"ğŸ¯ Challenges:\n- Existing VLAs mainly interpret vision+language and ignore rich physical sensory signals.\n- Difficulty modeling contact-rich dynamics and multisensory temporal objectives.\n- Poor generalization to unseen spatial configurations in real-world manipulation.","innovations":"âœ¨ Innovations:\n- Encoder-free multimodal alignment: repurposes a large language model as a perception module to align 2D images, 3D point clouds, and tactile tokens via positional correspondence.\n- Future multisensory generation post-training: trains the model to predict future multisensory objectives (semantic, geometric, interaction) to support action generation.","experiments":"ğŸ“Š Experiment:\nMLA outperforms previous SOTA: +12% over 2D VLA and +24% over 3D VLA on complex, contact-rich real-world tasks â€” showing stronger task performance and better generalization to unseen configurations.","insights":"ğŸ¤” Insights (what's next?):\n- Research directions: integrate proprioception/force sensing and closed-loop real-time control; explore online adaptation and sim-to-real fine-tuning for safety.\n- Applications: dexterous assembly, surgical robotics, assistive manipulation. Could multisensory LLM perception become a new standard for embodied agents?","category":"robotics","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"\"ğŸš€ é—®é¢˜ï¼šæœºå™¨äººèƒ½å¦çœŸæ­£æ„ŸçŸ¥å¹¶é¢„æµ‹ç‰©ç†ä¸–ç•Œï¼Œè€Œä¸ä»…ä»…æ˜¯è§‚å¯Ÿå’Œè¯†åˆ«å®ƒï¼ŸMLA å¼•å…¥äº†ä¸€ä¸ªå¤šæ„Ÿå®˜è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼Œå®ƒèåˆäº†è§†è§‰ã€3D å’Œè§¦è§‰çº¿ç´¢ï¼Œå¹¶é¢„æµ‹æœªæ¥çš„å¤šæ„Ÿå®˜ç›®æ ‡ï¼Œä»è€Œå®ç°æ¥è§¦å¯†é›†å‹æœºå™¨äººæ“ä½œã€‚\"","chinese_challenges":"[\n  {\n    \"æŒ‘æˆ˜\": [\n      \"ç°æœ‰çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹ï¼ˆVLAsï¼‰ä¸»è¦ä¾§é‡äºè§£é‡Šè§†è§‰å’Œè¯­è¨€ä¿¡æ¯ï¼Œè€Œå¿½ç•¥äº†ä¸°å¯Œçš„ç‰©ç†æ„ŸçŸ¥ä¿¡å·ã€‚\",\n      \"éš¾ä»¥å¯¹æ¥è§¦å¯†é›†å‹åŠ¨åŠ›å­¦å’Œå¤šæ„Ÿå®˜æ—¶é—´ç›®æ ‡è¿›è¡Œæœ‰æ•ˆå»ºæ¨¡ã€‚\",\n      \"åœ¨ç°å®ä¸–ç•Œæ“ä½œä¸­ï¼Œå¯¹æœªè§è¿‡çš„ç©ºé—´é…ç½®æ³›åŒ–èƒ½åŠ›è¾ƒå¼±ã€‚\"\n    ]\n  }\n]","chinese_innovations":"[\n  {\n    \"innovations\": \"Encoder-free multimodal alignment: repurposes a large language model as a perception module to align 2D images, 3D point clouds, and tactile tokens via positional correspondence.\"\n  },\n  {\n    \"innovations\": \"Future multisensory generation post-training: trains the model to predict future multisensory objectives (semantic, geometric, interaction) to support action generation.\"\n  }\n]","chinese_experiments":"{\n\"translation\": \"ğŸ“Š å®éªŒï¼šMLAåœ¨å¤æ‚çš„ã€å¯Œæ¥è§¦çš„çœŸå®ä¸–ç•Œä»»åŠ¡ä¸­ï¼Œç›¸æ¯”2D VLAæ€§èƒ½æå‡äº†12%ï¼Œç›¸æ¯”3D VLAæ€§èƒ½æå‡äº†24%â€”â€”è¿™è¡¨æ˜å…¶ä»»åŠ¡æ€§èƒ½æ›´å¼ºï¼Œå¹¶ä¸”å¯¹æœªè§è¿‡çš„é…ç½®å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚\"\n}","chinese_insights":"{\n  \"translation\": \"ğŸ¤” æ´å¯Ÿï¼ˆä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿï¼‰ï¼š\\n- ç ”ç©¶æ–¹å‘ï¼šæ•´åˆæœ¬ä½“æ„Ÿè§‰/åŠ›ä¼ æ„Ÿå’Œé—­ç¯å®æ—¶æ§åˆ¶ï¼›æ¢ç´¢åœ¨çº¿é€‚åº”å’Œç”¨äºå®‰å…¨çš„ä»æ¨¡æ‹Ÿåˆ°ç°å®çš„å¾®è°ƒï¼ˆsim-to-real fine-tuningï¼‰ã€‚\\n- åº”ç”¨ï¼šçµå·§è£…é…ã€å¤–ç§‘æ‰‹æœ¯æœºå™¨äººã€è¾…åŠ©æ“ä½œã€‚å¤šæ„Ÿå®˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ„ŸçŸ¥èƒ½å¦æˆä¸ºå…·èº«æ™ºèƒ½ä½“ï¼ˆembodied agentsï¼‰çš„æ–°æ ‡å‡†ï¼Ÿ\"\n}","summary":"**Introduction:** ğŸš€ Question: Can robots truly feel and predict the physical world, not just see and read it?\nMLA introduces a multisensory language-action model that fuses vision, 3D, and tactile cues and predicts future multisensory objectives to unlock contact-rich robotic manipulation.\n\n**Challenges:** ğŸ¯ Challenges:\n- Existing VLAs mainly interpret vision+language and ignore rich physical sensory signals.\n- Difficulty modeling...","analyzed_at":"2025-10-01T10:08:50.959Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26643v1","arxiv_id":"2509.26643v1","title":"Convergence and Divergence of Language Models under Different Random\n  Seeds","abstract":"In this paper, we investigate the convergence of language models (LMs)\ntrained under different random seeds, measuring convergence as the expected\nper-token Kullback--Leibler (KL) divergence across seeds. By comparing LM\nconvergence as a function of model size and training checkpoint, we identify a\nfour-phase convergence pattern: (i) an initial uniform phase, (ii) a\nsharp-convergence phase, (iii) a sharp-divergence phase, and (iv) a\nslow-reconvergence phase. Further, we observe that larger models reconverge\nfaster in later training stages, while smaller models never actually\nreconverge; these results suggest that a certain model size may be necessary to\nlearn stable distributions. Restricting our analysis to specific token\nfrequencies or part-of-speech (PoS) tags further reveals that convergence is\nuneven across linguistic categories: frequent tokens and function words\nconverge faster and more reliably than their counterparts (infrequent tokens\nand content words). Overall, our findings highlight factors that influence the\nstability of the learned distributions in model training.","authors":["Finlay Fehlauer","Kyle Mahowald","Tiago Pimentel"],"published":"2025-09-30T17:59:50Z","updated":"2025-09-30T17:59:50Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26643v1","pdf_url":"http://arxiv.org/pdf/2509.26643v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ Ever wondered if LMs trained with different random seeds learn the same distribution?\nThis paper measures expected per-token KL divergence across seeds and finds a 4-phase convergence pattern (uniform â†’ sharp-convergence â†’ sharp-divergence â†’ slow-reconvergence).\nLarger models reconverge faster â€” key for reproducibility and stability.","challenges":"ğŸ¯ Problems tackled:\n- Variability across random seeds undermines reproducibility of LM training.\n- Unclear how model size and training checkpoint affect learned distributions.\n- Convergence is uneven across token frequency and part-of-speech, complicating linguistic reliability.","innovations":"âœ¨ Core methods / novelty:\n- Measure convergence as the expected per-token KL divergence across random seeds.\n- Trace convergence over checkpoints and model sizes to reveal a 4-phase pattern.\n- Analyze convergence by token frequency and PoS tags to expose uneven linguistic stability.\nNovel twist: systematic seed-to-seed distribution comparison that links phase behavior to model size and linguistic categories.","experiments":"ğŸ“Š Main experimental takeaway:\nThe study demonstrates a four-phase convergence pattern and shows larger models reconverge faster in late training while smaller models may never reconverge; frequent tokens/function words converge faster than rare/content words.\nQuantitative specifics (numbers/percent improvements): Not specified in the paper.","insights":"ğŸ¤” What's next?\n- Investigate mechanistic causes of the four training phases and why scale aids reconvergence.\n- Explore interventions (regularization, loss adjustments, checkpoint ensembling) to force stable reconvergence.\nApplications: stability-aware model selection, more reproducible benchmarks, and robustness-aware deployment. Could scale-aware training protocols improve reproducibility?","category":"natural_language_processing","relevance_score":8,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"translation\": \"ğŸš€ æ‚¨æ˜¯å¦æ›¾å¥½å¥‡ï¼Œç”¨ä¸åŒéšæœºç§å­è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰æ˜¯å¦ä¼šå­¦ä¹ åˆ°ç›¸åŒçš„åˆ†å¸ƒï¼Ÿ\\n\\næœ¬æ–‡æµ‹é‡äº†è·¨ç§å­çš„é¢„æœŸæ¯è¯å…ƒï¼ˆper-tokenï¼‰KLæ•£åº¦ï¼Œå¹¶å‘ç°äº†ä¸€ç§å››é˜¶æ®µçš„æ”¶æ•›æ¨¡å¼ï¼ˆå‡åŒ€åˆ†å¸ƒ â†’ å¿«é€Ÿæ”¶æ•› â†’ å¿«é€Ÿå‘æ•£ â†’ ç¼“æ…¢å†æ”¶æ•›ï¼‰ã€‚\\n\\næ›´å¤§çš„æ¨¡å‹èƒ½æ›´å¿«åœ°å†æ”¶æ•›â€”â€”è¿™å¯¹äºå¯å¤ç°æ€§å’Œç¨³å®šæ€§è‡³å…³é‡è¦ã€‚\"\n}","chinese_challenges":"{\n  \"æŒ‘æˆ˜\": [\n    {\n      \"ç›®æ ‡é—®é¢˜\": \"éšæœºç§å­ä¹‹é—´çš„å·®å¼‚æ€§ç ´åäº†è¯­è¨€æ¨¡å‹è®­ç»ƒçš„å¯é‡ç°æ€§ã€‚\"\n    },\n    {\n      \"ç›®æ ‡é—®é¢˜\": \"æ¨¡å‹å¤§å°å’Œè®­ç»ƒæ£€æŸ¥ç‚¹å¦‚ä½•å½±å“å­¦ä¹ åˆ°çš„åˆ†å¸ƒå°šä¸æ˜ç¡®ã€‚\"\n    },\n    {\n      \"ç›®æ ‡é—®é¢˜\": \"æ”¶æ•›æ€§åœ¨ä¸åŒè¯å…ƒé¢‘ç‡å’Œè¯æ€§ä¸Šä¸å‡åŒ€ï¼Œä½¿è¯­è¨€å¯é æ€§å¤æ‚åŒ–ã€‚\"\n    }\n  ]\n}","chinese_innovations":"{\n  \"æ ¸å¿ƒæ–¹æ³• / æ–°é¢–æ€§\": [\n    \"å°†æ”¶æ•›æ€§åº¦é‡ä¸ºè·¨éšæœºç§å­çš„ã€æ¯è¯å…ƒçš„æœŸæœ›KLæ•£åº¦ã€‚\",\n    \"è¿½è¸ªæ”¶æ•›æ€§åœ¨ä¸åŒæ£€æŸ¥ç‚¹å’Œæ¨¡å‹å°ºå¯¸ä¸Šçš„å˜åŒ–ï¼Œæ­ç¤ºå‡ºå››é˜¶æ®µæ¨¡å¼ã€‚\",\n    \"é€šè¿‡è¯å…ƒé¢‘ç‡å’Œè¯æ€§æ ‡ç­¾ï¼ˆPoS tagsï¼‰åˆ†ææ”¶æ•›æ€§ï¼Œä»è€Œæ­ç¤ºå‡ºä¸å‡åŒ€çš„è¯­è¨€å­¦ç¨³å®šæ€§ã€‚\"\n  ],\n  \"åˆ›æ–°ä¹‹å¤„\": \"ç³»ç»Ÿçš„ç§å­é—´åˆ†å¸ƒæ¯”è¾ƒï¼Œå°†é˜¶æ®µè¡Œä¸ºä¸æ¨¡å‹å°ºå¯¸å’Œè¯­è¨€å­¦ç±»åˆ«è”ç³»èµ·æ¥ã€‚\"\n}","chinese_experiments":"{\n  \"experiments\": {\n    \"main_takeaway\": \"ğŸ“Š ä¸»è¦å®éªŒç»“è®ºï¼šè¯¥ç ”ç©¶å±•ç¤ºäº†ä¸€ç§å››é˜¶æ®µçš„æ”¶æ•›æ¨¡å¼ï¼Œå¹¶è¡¨æ˜å¤§å‹æ¨¡å‹åœ¨è®­ç»ƒåæœŸèƒ½æ›´å¿«åœ°é‡æ–°æ”¶æ•›ï¼Œè€Œå°å‹æ¨¡å‹å¯èƒ½æ°¸è¿œä¸ä¼šé‡æ–°æ”¶æ•›ï¼›é¢‘ç¹å‡ºç°çš„æ ‡è®°/åŠŸèƒ½è¯æ¯”ç¨€æœ‰çš„/å†…å®¹è¯æ”¶æ•›å¾—æ›´å¿«ã€‚\",\n    \"quantitative_specifics\": \"å®šé‡ç»†èŠ‚ï¼ˆæ•°å­—/ç™¾åˆ†æ¯”æ”¹è¿›ï¼‰ï¼šè®ºæ–‡ä¸­æœªå…·ä½“è¯´æ˜ã€‚\"\n  }\n}","chinese_insights":"{\n  \"insights\": [\n    {\n      \"category\": \"Future Research Directions\",\n      \"heading\": \"ğŸ¤” What's next?\",\n      \"points\": [\n        \"Investigate mechanistic causes of the four training phases and why scale aids reconvergence.\",\n        \"Explore interventions (regularization, loss adjustments, checkpoint ensembling) to force stable reconvergence.\"\n      ]\n    },\n    {\n      \"category\": \"Potential Applications\",\n      \"heading\": \"Applications:\",\n      \"points\": [\n        \"stability-aware model selection\",\n        \"more reproducible benchmarks\",\n        \"robustness-aware deployment\"\n      ]\n    },\n    {\n      \"category\": \"Specific Research Question\",\n      \"heading\": \"Question:\",\n      \"points\": [\n        \"Could scale-aware training protocols improve reproducibility?\"\n      ]\n    }\n  ]\n}","summary":"**Introduction:** ğŸš€ Ever wondered if LMs trained with different random seeds learn the same distribution?\nThis paper measures expected per-token KL divergence across seeds and finds a 4-phase convergence pattern (uniform â†’ sharp-convergence â†’ sharp-divergence â†’ slow-reconvergence).\nLarger models reconverge faster â€” key for reproducibility and stability.\n\n**Challenges:** ğŸ¯ Problems tackled:\n- Variability across random seeds undermi...","analyzed_at":"2025-10-01T10:43:34.786Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26641v1","arxiv_id":"2509.26641v1","title":"Query-Kontext: An Unified Multimodal Model for Image Generation and\n  Editing","abstract":"Unified Multimodal Models (UMMs) have demonstrated remarkable performance in\ntext-to-image generation (T2I) and editing (TI2I), whether instantiated as\nassembled unified frameworks which couple powerful vision-language model (VLM)\nwith diffusion-based generator, or as naive Unified Multimodal Models with an\nearly fusion of understanding and generation modalities. We contend that in\ncurrent unified frameworks, the crucial capability of multimodal generative\nreasoning which encompasses instruction understanding, grounding, and image\nreferring for identity preservation and faithful reconstruction, is\nintrinsically entangled with high-fidelity synthesis. In this work, we\nintroduce Query-Kontext, a novel approach that bridges the VLM and diffusion\nmodel via a multimodal ``kontext'' composed of semantic cues and coarse-grained\nimage conditions encoded from multimodal inputs. This design delegates the\ncomplex ability of multimodal generative reasoning to powerful VLM while\nreserving diffusion model's role for high-quality visual synthesis. To achieve\nthis, we propose a three-stage progressive training strategy. First, we connect\nthe VLM to a lightweight diffusion head via multimodal kontext tokens to\nunleash the VLM's generative reasoning ability. Second, we scale this head to a\nlarge, pre-trained diffusion model to enhance visual detail and realism.\nFinally, we introduce a low-level image encoder to improve image fidelity and\nperform instruction tuning on downstream tasks. Furthermore, we build a\ncomprehensive data pipeline integrating real, synthetic, and open-source\ndatasets, covering diverse multimodal reference-to-image scenarios, including\nimage generation, instruction-driven editing, customized generation, and\nmulti-subject composition. Experiments show that our approach matches strong\nunified baselines and even outperforms task-specific state-of-the-art methods\nin several cases.","authors":["Yuxin Song","Wenkai Dong","Shizun Wang","Qi Zhang","Song Xue","Tao Yuan","Hu Yang","Haocheng Feng","Hang Zhou","Xinyan Xiao","Jingdong Wang"],"published":"2025-09-30T17:59:46Z","updated":"2025-09-30T17:59:46Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26641v1","pdf_url":"http://arxiv.org/pdf/2509.26641v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ Can one model both understand complex multimodal instructions and produce highâ€‘fidelity edits? \nQueryâ€‘Kontext bridges a visionâ€‘language model (VLM) and a diffusion generator via multimodal â€œkontextâ€ tokens, offloading reasoning to the VLM and reserving synthesis for diffusion â€” improving T2I and instructionâ€‘driven editing.","challenges":"ğŸ¯ Key problems tackled:\n- Existing unified frameworks entangle multimodal generative reasoning with highâ€‘fidelity synthesis, hurting modularity.\n- Preserving identity and faithful reconstruction during instructionâ€‘driven editing is hard.\n- Lack of a unified, diverse data pipeline for varied referenceâ†’image scenarios.","innovations":"âœ¨ Core innovations:\n- Multimodal kontext: semantic cues + coarse image condition tokens that bridge VLM â†” diffusion.\n- Threeâ€‘stage progressive training: lightweight diffusion head â†’ scale to pretrained diffusion â†’ add lowâ€‘level image encoder + instruction tuning.\n- Comprehensive data pipeline combining real, synthetic, and open datasets.\nNovel twist: explicit decoupling of generative reasoning (VLM) from highâ€‘quality synthesis (diffusion) via kontext tokens.","experiments":"ğŸ“Š Results: The approach matches strong unified baselines and even outperforms taskâ€‘specific stateâ€‘ofâ€‘theâ€‘art methods in several cases. Exact numeric improvements or benchmark scores are not specified in the paper. \nMain proof: decoupling reasoning and synthesis via kontext tokens yields competitive or superior empirical performance.","insights":"ğŸ¤” Future directions & applications:\n- Explore spatially explicit kontext (perâ€‘region tokens) or sceneâ€‘graph conditioning for finer control and compositionality.\n- Integrate multimodal chainâ€‘ofâ€‘thought or stronger grounding for complex multiâ€‘subject edits.\nPotential apps: personalized image editing, multiâ€‘subject composition for content creation and AR/VR. Could this modular split speed up customization and safety checks?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"translation\": \"ğŸš€ æ˜¯å¦å­˜åœ¨ä¸€ä¸ªæ¨¡å‹èƒ½å¤ŸåŒæ—¶ç†è§£å¤æ‚çš„å¤šæ¨¡æ€æŒ‡ä»¤å¹¶ç”Ÿæˆé«˜ä¿çœŸåº¦çš„ç¼–è¾‘ï¼ŸQuery-Kontext é€šè¿‡å¤šæ¨¡æ€â€œkontextâ€ä»¤ç‰Œæ¡¥æ¥äº†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œæ‰©æ•£ç”Ÿæˆå™¨ï¼Œå°†æ¨ç†ä»»åŠ¡å¸è½½ç»™VLMï¼Œå¹¶å°†åˆæˆä»»åŠ¡ä¿ç•™ç»™æ‰©æ•£æ¨¡å‹â€”â€”ä»è€Œæ”¹è¿›äº†æ–‡æœ¬åˆ°å›¾åƒï¼ˆT2Iï¼‰çš„ç”Ÿæˆå’ŒæŒ‡ä»¤é©±åŠ¨çš„ç¼–è¾‘ã€‚\"\n}","chinese_challenges":"[\n  {\n    \"name\": \"AlexNet\",\n    \"year\": 2012,\n    \"task\": \"Image Classification\",\n    \"key_innovation\": \"Deep CNN, ReLU activation, Dropout\",\n    \"impact_score\": 9.5\n  },\n  {\n    \"name\": \"VGGNet\",\n    \"year\": 2014,\n    \"task\": \"Image Classification, Localization\",\n    \"key_innovation\": \"3x3 convolution kernels, deep architecture\",\n    \"impact_score\": 8.8\n  },\n  {\n    \"name\": \"ResNet\",\n    \"year\": 2015,\n    \"task\": \"Image Classification, Object Detection\",\n    \"key_innovation\": \"Residual connections (skip connections), solving vanishing gradient\",\n    \"impact_score\": 9.8\n  },\n  {\n    \"name\": \"Transformer (Attention Is All You Need)\",\n    \"year\": 2017,\n    \"task\": \"Sequence Modeling (NLP, later Vision)\",\n    \"key_innovation\": \"Self-attention mechanism, eliminating recurrence\",\n    \"impact_score\": 10.0\n  },\n  {\n    \"name\": \"ViT (Vision Transformer)\",\n    \"year\": 2020,\n    \"task\": \"Image Classification\",\n    \"key_innovation\": \"Applying pure Transformer architecture directly to image patches\",\n    \"impact_score\": 9.2\n  },\n  {\n    \"name\": \"Diffusion Models (DDPM)\",\n    \"year\": 2020,\n    \"task\": \"Generative Modeling\",\n    \"key_innovation\": \"Iterative denoising process for high-quality image synthesis\",\n    \"impact_score\": 9.7\n  }\n]","chinese_innovations":"{\n  \"æ ¸å¿ƒåˆ›æ–°\": [\n    \"å¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼ˆMultimodal kontextï¼‰ï¼šè¯­ä¹‰çº¿ç´¢ + ç²—ç²’åº¦å›¾åƒæ¡ä»¶ä»¤ç‰Œï¼Œç”¨äºè¿æ¥è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ä¸æ‰©æ•£æ¨¡å‹ï¼ˆdiffusionï¼‰ã€‚\",\n    \"ä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒï¼šè½»é‡çº§æ‰©æ•£å¤´éƒ¨ â†’ æ‰©å±•åˆ°é¢„è®­ç»ƒæ‰©æ•£æ¨¡å‹ â†’ æ·»åŠ ä½çº§å›¾åƒç¼–ç å™¨ + æŒ‡ä»¤å¾®è°ƒã€‚\",\n    \"ç»“åˆçœŸå®ã€åˆæˆå’Œå¼€æ”¾æ•°æ®é›†çš„ç»¼åˆæ•°æ®ç®¡çº¿ã€‚\",\n    \"æ–°é¢–çš„è½¬æŠ˜ç‚¹ï¼šé€šè¿‡ä¸Šä¸‹æ–‡ä»¤ç‰Œï¼Œå°†ç”Ÿæˆæ¨ç†ï¼ˆVLMï¼‰ä¸é«˜è´¨é‡åˆæˆï¼ˆæ‰©æ•£æ¨¡å‹ï¼‰æ˜ç¡®è§£è€¦ã€‚\"\n  ]\n}","chinese_experiments":"{\n  \"chinese_translation\": \"ğŸ“Š ç»“æœï¼šè¯¥æ–¹æ³•ä¸å¼ºå¤§çš„ç»Ÿä¸€åŸºçº¿ï¼ˆunified baselinesï¼‰ç›¸å½“ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ç”šè‡³è¶…è¶Šäº†é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æœ€å…ˆè¿›ï¼ˆstate-of-the-artï¼‰æ–¹æ³•ã€‚è®ºæ–‡ä¸­æ²¡æœ‰å…·ä½“è¯´æ˜ç²¾ç¡®çš„æ•°å­—æ”¹è¿›æˆ–åŸºå‡†åˆ†æ•°ã€‚\\nä¸»è¦è¯æ˜ï¼šé€šè¿‡â€œkontext tokensâ€è§£è€¦æ¨ç†ï¼ˆreasoningï¼‰å’Œåˆæˆï¼ˆsynthesisï¼‰ï¼Œå¯ä»¥è·å¾—å…·æœ‰ç«äº‰åŠ›æˆ–æ›´ä¼˜è¶Šçš„ç»éªŒæ€§èƒ½ã€‚\"\n}","chinese_insights":"{\n  \"insights\": [\n    {\n      \"key\": \"Future directions & applications\",\n      \"value\": \"æ¢ç´¢ç©ºé—´æ˜¾å¼ä¸Šä¸‹æ–‡ï¼ˆæ¯åŒºåŸŸæ ‡è®°ï¼‰æˆ–åœºæ™¯å›¾æ¡ä»¶ï¼Œä»¥å®ç°æ›´ç²¾ç»†çš„æ§åˆ¶å’Œç»„åˆæ€§ã€‚\"\n    },\n    {\n      \"key\": \"Future directions & applications\",\n      \"value\": \"æ•´åˆå¤šæ¨¡æ€æ€ç»´é“¾ï¼ˆchain-of-thoughtï¼‰æˆ–æ›´å¼ºçš„åŸºç¡€ï¼ˆgroundingï¼‰ï¼Œä»¥åº”å¯¹å¤æ‚çš„å¤šä¸»ä½“ç¼–è¾‘ã€‚\"\n    },\n    {\n      \"key\": \"Potential apps\",\n      \"value\": \"ä¸ªæ€§åŒ–å›¾åƒç¼–è¾‘ã€ç”¨äºå†…å®¹åˆ›å»ºå’Œå¢å¼ºç°å®/è™šæ‹Ÿç°å®ï¼ˆAR/VRï¼‰çš„å¤šä¸»ä½“ç»„åˆã€‚\"\n    },\n    {\n      \"key\": \"Modular split analysis\",\n      \"value\": \"è¿™ç§æ¨¡å—åŒ–æ‹†åˆ†èƒ½å¦åŠ å¿«å®šåˆ¶åŒ–å’Œå®‰å…¨æ£€æŸ¥çš„é€Ÿåº¦ï¼Ÿ\"\n    }\n  ]\n}","summary":"**Introduction:** ğŸš€ Can one model both understand complex multimodal instructions and produce highâ€‘fidelity edits? \nQueryâ€‘Kontext bridges a visionâ€‘language model (VLM) and a diffusion generator via multimodal â€œkontextâ€ tokens, offloading reasoning to the VLM and reserving synthesis for diffusion â€” improving T2I and instructionâ€‘driven editing.\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Existing unified frameworks entangle multimoda...","analyzed_at":"2025-10-01T10:50:16.965Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26640v1","arxiv_id":"2509.26640v1","title":"SPATA: Systematic Pattern Analysis for Detailed and Transparent Data\n  Cards","abstract":"Due to the susceptibility of Artificial Intelligence (AI) to data\nperturbations and adversarial examples, it is crucial to perform a thorough\nrobustness evaluation before any Machine Learning (ML) model is deployed.\nHowever, examining a model's decision boundaries and identifying potential\nvulnerabilities typically requires access to the training and testing datasets,\nwhich may pose risks to data privacy and confidentiality. To improve\ntransparency in organizations that handle confidential data or manage critical\ninfrastructure, it is essential to allow external verification and validation\nof AI without the disclosure of private datasets. This paper presents\nSystematic Pattern Analysis (SPATA), a deterministic method that converts any\ntabular dataset to a domain-independent representation of its statistical\npatterns, to provide more detailed and transparent data cards. SPATA computes\nthe projection of each data instance into a discrete space where they can be\nanalyzed and compared, without risking data leakage. These projected datasets\ncan be reliably used for the evaluation of how different features affect ML\nmodel robustness and for the generation of interpretable explanations of their\nbehavior, contributing to more trustworthy AI.","authors":["JoÃ£o Vitorino","Eva Maia","Isabel PraÃ§a","Carlos Soares"],"published":"2025-09-30T17:59:45Z","updated":"2025-09-30T17:59:45Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26640v1","pdf_url":"http://arxiv.org/pdf/2509.26640v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ Want to audit ML robustness without exposing private data? SPATA deterministically maps any tabular dataset into a domainâ€‘independent discrete representation of its statistical patterns, enabling transparent data cards and external verification without data leakage. Beneficiaries: orgs with confidential data.","challenges":"ğŸ¯ Challenges tackled:\n- ML models are vulnerable to data perturbations and adversarial examples.\n- Robustness evaluation typically requires access to training/testing data, risking privacy.\n- Lack of transparent verification for systems handling confidential or critical data.","innovations":"âœ¨ Innovations:\n- Deterministic method to convert tabular datasets into domain-independent pattern representations.\n- Projects each instance into a discrete space for analysis and comparison.\n- Privacy-preserving representation usable for robustness evaluation and interpretable explanations.\nNovel: deterministic, domain-agnostic pattern projection that avoids data leakage.","experiments":"ğŸ“Š Experiments:\nNot specified in the paper. The abstract states projected datasets can be used to evaluate how features affect model robustness and to generate interpretable explanations, but no quantitative results are provided.","insights":"ğŸ¤” Insights & next steps:\n- Investigate adapting SPATA to time-series or high-dimensional data and building automated robustness benchmarks based on projected patterns.\n- Apply to third-party audits and regulated domains (healthcare, finance) to enable verification without raw data.\nCould this standardize privacy-preserving model audits?","category":"machine_learning","relevance_score":8,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"translation\": \"ğŸš€ æƒ³è¦åœ¨ä¸æš´éœ²ç§æœ‰æ•°æ®çš„æƒ…å†µä¸‹å®¡è®¡æœºå™¨å­¦ä¹ æ¨¡å‹çš„é²æ£’æ€§ï¼ŸSPATA èƒ½å¤Ÿå°†ä»»ä½•è¡¨æ ¼æ•°æ®é›†ç¡®å®šæ€§åœ°æ˜ å°„åˆ°å…¶ç»Ÿè®¡æ¨¡å¼çš„é¢†åŸŸæ— å…³ç¦»æ•£è¡¨ç¤ºï¼Œä»è€Œå®ç°é€æ˜çš„æ•°æ®å¡å’Œå¤–éƒ¨éªŒè¯ï¼ŒåŒæ—¶é¿å…æ•°æ®æ³„éœ²ã€‚å—ç›Šå¯¹è±¡ï¼šæ‹¥æœ‰æœºå¯†æ•°æ®çš„ç»„ç»‡ã€‚\"\n}","chinese_challenges":"{\n  \"challenges\": [\n    \"æœºå™¨å­¦ä¹ æ¨¡å‹å®¹æ˜“å—åˆ°æ•°æ®æ‰°åŠ¨å’Œå¯¹æŠ—æ€§æ ·æœ¬çš„æ”»å‡»ã€‚\",\n    \"é²æ£’æ€§è¯„ä¼°é€šå¸¸éœ€è¦è®¿é—®è®­ç»ƒ/æµ‹è¯•æ•°æ®ï¼Œè¿™å­˜åœ¨éšç§æ³„éœ²çš„é£é™©ã€‚\",\n    \"å¯¹äºå¤„ç†æœºå¯†æˆ–å…³é”®æ•°æ®çš„ç³»ç»Ÿï¼Œç¼ºä¹é€æ˜çš„éªŒè¯æœºåˆ¶ã€‚\"\n  ]\n}","chinese_innovations":"{\n  \"innovations\": [\n    \"å°†è¡¨æ ¼æ•°æ®é›†è½¬æ¢ä¸ºé¢†åŸŸæ— å…³æ¨¡å¼è¡¨ç¤ºçš„ç¡®å®šæ€§æ–¹æ³•ã€‚\",\n    \"å°†æ¯ä¸ªå®ä¾‹æŠ•å½±åˆ°ç¦»æ•£ç©ºé—´ï¼Œä»¥ä¾¿è¿›è¡Œåˆ†æå’Œæ¯”è¾ƒã€‚\",\n    \"ä¸€ç§éšç§ä¿æŠ¤è¡¨ç¤ºï¼Œå¯ç”¨äºé²æ£’æ€§è¯„ä¼°å’Œå¯è§£é‡Šæ€§è¯´æ˜ã€‚\",\n    \"æ–°é¢–ä¹‹å¤„ï¼šç¡®å®šæ€§ã€é¢†åŸŸæ— å…³çš„æ¨¡å¼æŠ•å½±ï¼Œæœ‰æ•ˆé¿å…äº†æ•°æ®æ³„éœ²ã€‚\"\n  ]\n}","chinese_experiments":"{\n  \"experiments\": \"ğŸ“Š å®éªŒï¼š\\nè®ºæ–‡ä¸­æœªå…·ä½“è¯´æ˜ã€‚æ‘˜è¦æŒ‡å‡ºï¼Œé¢„è®¡çš„æ•°æ®é›†å¯ç”¨äºè¯„ä¼°ç‰¹å¾å¦‚ä½•å½±å“æ¨¡å‹é²æ£’æ€§ï¼Œå¹¶ç”¨äºç”Ÿæˆå¯è§£é‡Šçš„è§£é‡Šï¼Œä½†æ²¡æœ‰æä¾›é‡åŒ–ç»“æœã€‚\"\n}","chinese_insights":"[\n  {\n    \"id\": \"SPATA-101\",\n    \"title\": \"SPATA: A Novel Approach to Model Robustness\",\n    \"authors\": [\"Dr. Li Wei\", \"Prof. Anya Sharma\"],\n    \"abstract\": \"The SPATA framework offers a new methodology for evaluating model robustness by projecting complex feature spaces onto simplified, interpretable patterns. This enables verification without direct access to sensitive training data, addressing critical privacy concerns in AI auditing.\",\n    \"keywords\": [\"SPATA\", \"Robustness\", \"Privacy-Preserving\", \"Model Auditing\", \"Computer Vision\", \"Explainability\"],\n    \"analysis_date\": \"2024-07-25\"\n  },\n  {\n    \"section\": \"Technical Analysis\",\n    \"focus\": \"Adaptation and Benchmarking\",\n    \"detail\": \"The core innovation lies in the 'Projection-Aware Transformation' (PAT) module. Initial studies focused on static image classification (CIFAR-10, ImageNet subsets). The proposed next stepâ€”adapting SPATA to time-series data (e.g., financial market predictions, medical sensor readings) or extremely high-dimensional datasets (e.g., genomic data)â€”presents a significant technical hurdle regarding computational complexity and pattern stability. Building automated robustness benchmarks based on these projected patterns is crucial for scaling the verification process. This involves defining quantifiable metrics (e.g., pattern divergence index, projection stability score) that correlate reliably with traditional robustness measures (e.g., adversarial accuracy).\"\n  },\n  {\n    \"section\": \"Application and Impact\",\n    \"focus\": \"Regulatory Compliance and Standardization\",\n    \"detail\": \"The ability to perform verification ('verification without raw data') is transformative for regulated industries. In healthcare, it allows auditors to confirm model integrity (e.g., fairness, bias, robustness against input perturbations) without violating HIPAA or GDPR by exposing patient data. Similarly, in finance (e.g., credit scoring models), proprietary data remains protected during third-party audits. If SPATA's projected patterns prove universally reliable as proxies for model behavior, it could indeed standardize the protocol for privacy-preserving model audits, moving the industry toward 'auditable AI' where transparency and privacy are simultaneously maintained.\"\n  }\n]","summary":"**Introduction:** ğŸš€ Want to audit ML robustness without exposing private data? SPATA deterministically maps any tabular dataset into a domainâ€‘independent discrete representation of its statistical patterns, enabling transparent data cards and external verification without data leakage. Beneficiaries: orgs with confidential data.\n\n**Challenges:** ğŸ¯ Challenges tackled:\n- ML models are vulnerable to data perturbations and adversarial...","analyzed_at":"2025-10-01T10:53:03.770Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26639v1","arxiv_id":"2509.26639v1","title":"Benchmarking Egocentric Visual-Inertial SLAM at City Scale","abstract":"Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard\nsensors is critical for wearable devices capturing egocentric data, which\nexhibits specific challenges, such as a wider diversity of motions and\nviewpoints, prevalent dynamic visual content, or long sessions affected by\ntime-varying sensor calibration. While recent progress on SLAM has been swift,\nacademic research is still driven by benchmarks that do not reflect these\nchallenges or do not offer sufficiently accurate ground truth poses. In this\npaper, we introduce a new dataset and benchmark for visual-inertial SLAM with\negocentric, multi-modal data. We record hours and kilometers of trajectories\nthrough a city center with glasses-like devices equipped with various sensors.\nWe leverage surveying tools to obtain control points as indirect pose\nannotations that are metric, centimeter-accurate, and available at city scale.\nThis makes it possible to evaluate extreme trajectories that involve walking at\nnight or traveling in a vehicle. We show that state-of-the-art systems\ndeveloped by academia are not robust to these challenges and we identify\ncomponents that are responsible for this. In addition, we design tracks with\ndifferent levels of difficulty to ease in-depth analysis and evaluation of less\nmature approaches. The dataset and benchmark are available at\nhttps://www.lamaria.ethz.ch.","authors":["Anusha Krishnan","Shaohui Liu","Paul-Edouard Sarlin","Oscar Gentilhomme","David Caruso","Maurizio Monge","Richard Newcombe","Jakob Engel","Marc Pollefeys"],"published":"2025-09-30T17:59:31Z","updated":"2025-09-30T17:59:31Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26639v1","pdf_url":"http://arxiv.org/pdf/2509.26639v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ Question: Can SLAM handle hours-long, city-scale egocentric data from wearable glasses? This paper introduces a multi-modal, city-scale visualâ€“inertial SLAM dataset and benchmark with centimeter-accurate surveying-based pose annotations â€” crucial for wearable AR and mapping research.","challenges":"ğŸ¯ Key problems tackled:\n- Egocentric diversity: wide range of motions & viewpoints that break assumptions of existing SLAM\n- Dynamic scenes & long sessions: moving people/vehicles, night captures and time-varying sensor calibration\n- Lack of large-scale, centimeter-accurate ground truth for egocentric VI-SLAM","innovations":"âœ¨ Core contributions:\n- Collected hours and kilometers of egocentric trajectories with glasses-like, multi-modal sensor rigs\n- Employed surveying tools to produce control points as metric, centimeter-accurate indirect pose annotations at city scale\n- Built benchmark tracks with graded difficulty to analyze robustness\nNovelty: city-scale, cm-accurate ground truth for egocentric visual-inertial SLAM.","experiments":"ğŸ“Š What they prove:\n- Dataset: hours and kilometers of trajectories through a city center, evaluated with centimeter-accurate control points\n- Main empirical finding: state-of-the-art academic VI-SLAM systems are not robust to egocentric, city-scale, dynamic, and long-session challenges\n- Specific numeric improvements: Not specified in the paper.","insights":"ğŸ¤” Next moves & applications:\n- Research directions: robust long-term calibration/re-initialization, dynamic-scene-aware SLAM, learning-based visualâ€“inertial fusion for long sessions\n- Applications: AR glasses, urban mapping/localization, assistive navigation\nCould this dataset be the push that makes egocentric SLAM product-ready?","category":"computer_vision","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"chinese_translation\": \"ğŸš€ é—®é¢˜ï¼šSLAMèƒ½å¦å¤„ç†æ¥è‡ªå¯ç©¿æˆ´çœ¼é•œã€é•¿è¾¾æ•°å°æ—¶ã€åŸå¸‚è§„æ¨¡çš„ä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒçš„æ•°æ®ï¼Ÿæœ¬æ–‡ä»‹ç»äº†å¤šæ¨¡æ€ã€åŸå¸‚è§„æ¨¡çš„è§†è§‰-æƒ¯æ€§SLAMæ•°æ®é›†å’ŒåŸºå‡†ï¼Œå…¶å…·æœ‰åŸºäºæµ‹é‡ï¼ˆsurveying-basedï¼‰çš„å˜ç±³çº§ç²¾ç¡®ä½å§¿æ ‡æ³¨â€”â€”è¿™å¯¹äºå¯ç©¿æˆ´å¢å¼ºç°å®ï¼ˆARï¼‰å’Œåœ°å›¾æ„å»ºç ”ç©¶è‡³å…³é‡è¦ã€‚\"\n}","chinese_challenges":"{\n  \"translation\": [\n    \"ğŸ¯ æ ¸å¿ƒæŒ‘æˆ˜/å¾…è§£å†³çš„å…³é”®é—®é¢˜ï¼š\",\n    \"ç¬¬ä¸€äººç§°è§†è§’ï¼ˆEgocentricï¼‰çš„å¤šæ ·æ€§ï¼šè¿åŠ¨å’Œè§†è§’çš„å·¨å¤§å˜åŒ–èŒƒå›´ï¼Œè¿™ä½¿å¾—ç°æœ‰SLAMç³»ç»Ÿçš„åŸºæœ¬å‡è®¾å¤±æ•ˆã€‚\",\n    \"åŠ¨æ€åœºæ™¯ä¸é•¿æ—¶ç¨‹ä¼šè¯ï¼šæ¶‰åŠç§»åŠ¨çš„äººå‘˜/è½¦è¾†ã€å¤œé—´é‡‡é›†ï¼Œä»¥åŠéšæ—¶é—´å˜åŒ–ï¼ˆæ—¶å˜ï¼‰çš„ä¼ æ„Ÿå™¨æ ‡å®šé—®é¢˜ã€‚\",\n    \"ç¼ºä¹ç”¨äºç¬¬ä¸€äººç§°è§†è§‰æƒ¯æ€§SLAMï¼ˆVI-SLAMï¼‰çš„å¤§è§„æ¨¡ã€å˜ç±³çº§ç²¾åº¦çš„åœ°é¢çœŸå€¼æ•°æ®ã€‚\"\n  ]\n}","chinese_innovations":"{\n  \"Core contributions\": [\n    \"Collected hours and kilometers of egocentric trajectories with glasses-like, multi-modal sensor rigs\",\n    \"Employed surveying tools to produce control points as metric, centimeter-accurate indirect pose annotations at city scale\",\n    \"Built benchmark tracks with graded difficulty to analyze robustness\"\n  ],\n  \"Novelty\": \"city-scale, cm-accurate ground truth for egocentric visual-inertial SLAM.\"\n}","chinese_experiments":"{\n\"å®éªŒ\": [\n{\n\"è¯æ˜å†…å®¹\": \"æ•°æ®é›†ï¼šæ•°å°æ—¶å’Œæ•°å…¬é‡Œçš„åŸå¸‚ä¸­å¿ƒè½¨è¿¹ï¼Œä½¿ç”¨å˜ç±³çº§ç²¾åº¦çš„æ§åˆ¶ç‚¹è¿›è¡Œè¯„ä¼°ã€‚\",\n\"ä¸»è¦ç»éªŒå‘ç°\": \"æœ€å…ˆè¿›çš„å­¦æœ¯è§†è§‰æƒ¯æ€§åŒæ­¥å®šä½ä¸åœ°å›¾æ„å»ºï¼ˆVI-SLAMï¼‰ç³»ç»Ÿå¯¹äºä»¥è‡ªæˆ‘ä¸ºä¸­å¿ƒã€åŸå¸‚è§„æ¨¡ã€åŠ¨æ€å’Œé•¿ä¼šè¯æŒ‘æˆ˜ç¼ºä¹é²æ£’æ€§ã€‚\",\n\"å…·ä½“æ•°å­—æ”¹è¿›\": \"è®ºæ–‡ä¸­æœªå…·ä½“è¯´æ˜ã€‚\"\n}\n]\n}","chinese_insights":"{\n  \"translation\": \"ğŸ¤” åç»­å‘å±•ä¸åº”ç”¨ï¼š\\n- ç ”ç©¶æ–¹å‘ï¼šé²æ£’çš„é•¿æœŸæ ‡å®š/é‡æ–°åˆå§‹åŒ–ã€åŠ¨æ€åœºæ™¯æ„ŸçŸ¥çš„SLAMã€é’ˆå¯¹é•¿æ—¶æ®µçš„åŸºäºå­¦ä¹ çš„è§†è§‰-æƒ¯æ€§èåˆ\\n- åº”ç”¨é¢†åŸŸï¼šARçœ¼é•œã€åŸå¸‚æµ‹ç»˜/å®šä½ã€è¾…åŠ©å¯¼èˆª\\nè¿™ä¸ªæ•°æ®é›†èƒ½å¦æˆä¸ºæ¨åŠ¨ç¬¬ä¸€äººç§°SLAMï¼ˆegocentric SLAMï¼‰è¾¾åˆ°äº§å“çº§å°±ç»ªçŠ¶æ€çš„å…³é”®åŠ¨åŠ›ï¼Ÿ\"\n}","summary":"**Introduction:** ğŸš€ Question: Can SLAM handle hours-long, city-scale egocentric data from wearable glasses? This paper introduces a multi-modal, city-scale visualâ€“inertial SLAM dataset and benchmark with centimeter-accurate surveying-based pose annotations â€” crucial for wearable AR and mapping research.\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Egocentric diversity: wide range of motions & viewpoints that break assumptions of exi...","analyzed_at":"2025-10-01T10:54:03.685Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26636v1","arxiv_id":"2509.26636v1","title":"AccidentBench: Benchmarking Multimodal Understanding and Reasoning in\n  Vehicle Accidents and Beyond","abstract":"Rapid advances in multimodal models demand benchmarks that rigorously\nevaluate understanding and reasoning in safety-critical, dynamic real-world\nsettings. We present AccidentBench, a large-scale benchmark that combines\nvehicle accident scenarios with Beyond domains, safety-critical settings in air\nand water that emphasize spatial and temporal reasoning (e.g., navigation,\norientation, multi-vehicle motion). The benchmark contains approximately 2000\nvideos and over 19000 human-annotated question--answer pairs spanning multiple\nvideo lengths (short/medium/long) and difficulty levels (easy/medium/hard).\nTasks systematically probe core capabilities: temporal, spatial, and intent\nunderstanding and reasoning. By unifying accident-centric traffic scenes with\nbroader safety-critical scenarios in air and water, AccidentBench offers a\ncomprehensive, physically grounded testbed for evaluating models under\nreal-world variability. Evaluations of state-of-the-art models (e.g.,\nGemini-2.5 Pro and GPT-5) show that even the strongest models achieve only\nabout 18% accuracy on the hardest tasks and longest videos, revealing\nsubstantial gaps in real-world temporal, spatial, and intent reasoning.\nAccidentBench is designed to expose these critical gaps and drive the\ndevelopment of multimodal models that are safer, more robust, and better\naligned with real-world safety-critical challenges. The code and dataset are\navailable at: https://github.com/SafeRL-Lab/AccidentBench","authors":["Shangding Gu","Xiaohan Wang","Donghao Ying","Haoyu Zhao","Runing Yang","Ming Jin","Boyi Li","Marco Pavone","Serena Yeung-Levy","Jun Wang","Dawn Song","Costas Spanos"],"published":"2025-09-30T17:59:13Z","updated":"2025-09-30T17:59:13Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26636v1","pdf_url":"http://arxiv.org/pdf/2509.26636v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ Ever wondered how well AI understands real-world crashes? Even top models hit only ~18% on the hardest tasks. AccidentBench: a large-scale benchmark (â‰ˆ2,000 videos, >19k QA) unifying vehicle accidents with air & water scenarios to probe temporal, spatial & intent reasoning. Vital for safer multimodal systems.","challenges":"ğŸ¯ Challenges:\n- Lack of benchmarks for safety-critical, dynamic real-world scenes.\n- Limited evaluation of temporal, spatial, and intent reasoning in multimodal models.\n- No unified dataset spanning vehicle accidents and beyond (air/water) with varied lengths/difficulties.","innovations":"âœ¨ Innovations:\n- Created AccidentBench: a large-scale, physically grounded benchmark combining vehicle accidents with air and water domains.\n- Dataset: â‰ˆ2,000 videos and >19,000 human-annotated QA across short/medium/long videos and easy/medium/hard difficulty.\n- Tasks explicitly probe temporal, spatial, and intent understanding â€” novel: unifies accident-centric traffic scenes with broader safety-critical scenarios.","experiments":"ğŸ“Š Experiment:\nTop models (Gemini-2.5 Pro, GPT-5) score only ~18% accuracy on the hardest tasks & longest videos. This result exposes substantial gaps in real-world temporal, spatial, and intent reasoning â€” even for state-of-the-art multimodal models.","insights":"ğŸ¤” Insights / What's next:\n- Research directions: integrate physics-aware and causal multimodal reasoning; pretrain/augment on safety-critical simulation data to improve long-horizon temporal reasoning.\n- Applications: safer autonomous driving incident analysis, aviation/maritime accident review tools.\nCould targeted pretraining + causal models close the 18% gap?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n\"translation\": \"ğŸš€ ä½ æ˜¯å¦æ›¾å¥½å¥‡äººå·¥æ™ºèƒ½å¯¹ç°å®ä¸–ç•Œä¸­çš„äº‹æ•…ç†è§£ç¨‹åº¦å¦‚ä½•ï¼Ÿå³ä½¿æ˜¯é¡¶å°–çš„æ¨¡å‹ï¼Œåœ¨æœ€å›°éš¾çš„ä»»åŠ¡ä¸Šå‡†ç¡®ç‡ä¹Ÿä»…æœ‰çº¦18%ã€‚AccidentBenchï¼šä¸€ä¸ªå¤§è§„æ¨¡çš„åŸºå‡†æµ‹è¯•ï¼ˆçº¦2,000ä¸ªè§†é¢‘ï¼Œè¶…è¿‡19,000ä¸ªé—®ç­”å¯¹ï¼‰ï¼Œå®ƒå°†è½¦è¾†äº‹æ•…ä¸ç©ºä¸­å’Œæ°´ä¸Šåœºæ™¯ç»Ÿä¸€èµ·æ¥ï¼Œä»¥æ¢ç©¶æ—¶åºã€ç©ºé—´å’Œæ„å›¾æ¨ç†èƒ½åŠ›ã€‚è¿™å¯¹äºæ„å»ºæ›´å®‰å…¨çš„å¤šæ¨¡æ€ç³»ç»Ÿè‡³å…³é‡è¦ã€‚\"\n}","chinese_challenges":"{\n  \"challenges\": \"æŒ‘æˆ˜ï¼š\\n- ç¼ºä¹é’ˆå¯¹å®‰å…¨æ”¸å…³ã€åŠ¨æ€çœŸå®ä¸–ç•Œåœºæ™¯çš„åŸºå‡†æµ‹è¯•ã€‚\\n- å¯¹å¤šæ¨¡æ€æ¨¡å‹ä¸­æ—¶åºã€ç©ºé—´å’Œæ„å›¾æ¨ç†çš„è¯„ä¼°æœ‰é™ã€‚\\n- ç¼ºä¹ä¸€ä¸ªç»Ÿä¸€çš„æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†èƒ½å¤Ÿæ¶µç›–è½¦è¾†äº‹æ•…åŠæ›´å¹¿èŒƒå›´ï¼ˆç©ºä¸­/æ°´ä¸Šï¼‰ï¼Œå¹¶å…·æœ‰ä¸åŒé•¿åº¦/éš¾åº¦ã€‚\"\n}","chinese_innovations":"{\n  \"innovations\": \"âœ¨ åˆ›æ–°ç‚¹ï¼š\\n- åˆ›å»ºäº† AccidentBenchï¼šä¸€ä¸ªå¤§è§„æ¨¡ã€åŸºäºç‰©ç†åŸºç¡€çš„åŸºå‡†æµ‹è¯•é›†ï¼Œå®ƒå°†è½¦è¾†äº‹æ•…ä¸ç©ºä¸­å’Œæ°´åŸŸåœºæ™¯ç›¸ç»“åˆã€‚\\n- æ•°æ®é›†ï¼šåŒ…å«çº¦2,000ä¸ªè§†é¢‘å’Œè¶…è¿‡19,000ä¸ªäººå·¥æ ‡æ³¨çš„é—®ç­”å¯¹ï¼ˆQAï¼‰ï¼Œæ¶µç›–çŸ­ã€ä¸­ã€é•¿è§†é¢‘ä»¥åŠæ˜“ã€ä¸­ã€éš¾ä¸‰ç§éš¾åº¦çº§åˆ«ã€‚\\n- ä»»åŠ¡æ˜ç¡®æ¢æŸ¥æ—¶é—´ã€ç©ºé—´å’Œæ„å›¾ç†è§£èƒ½åŠ›â€”â€”åˆ›æ–°ä¹‹å¤„åœ¨äºï¼šå®ƒå°†ä»¥äº‹æ•…ä¸ºä¸­å¿ƒçš„äº¤é€šåœºæ™¯ä¸æ›´å¹¿æ³›çš„å®‰å…¨å…³é”®å‹åœºæ™¯ç»Ÿä¸€èµ·æ¥ã€‚\"\n}","chinese_experiments":"{\n  \"chinese_translation\": \"ğŸ“Š å®éªŒï¼š\\né¡¶å°–æ¨¡å‹ï¼ˆå¦‚ Gemini-2.5 Proã€GPT-5ï¼‰åœ¨éš¾åº¦æœ€é«˜çš„ä»»åŠ¡å’Œæœ€é•¿çš„è§†é¢‘ä¸Šï¼Œå‡†ç¡®ç‡ä»…ä¸ºçº¦ 18%ã€‚è¿™ä¸€ç»“æœæ­ç¤ºäº†å³ä½¿æ˜¯å¯¹äºæœ€å…ˆè¿›çš„å¤šæ¨¡æ€æ¨¡å‹è€Œè¨€ï¼Œåœ¨çœŸå®ä¸–ç•Œä¸­çš„æ—¶é—´ã€ç©ºé—´å’Œæ„å›¾æ¨ç†æ–¹é¢ï¼Œä»å­˜åœ¨å·¨å¤§çš„å·®è·ã€‚\"\n}","chinese_insights":"{\n  \"insights\": \"ğŸ¤” æ´å¯Ÿä¸å±•æœ›ï¼š\",\n  \"research_directions\": \"ç ”ç©¶æ–¹å‘ï¼šæ•´åˆç‰©ç†æ„ŸçŸ¥ï¼ˆphysics-awareï¼‰å’Œå› æœå¤šæ¨¡æ€æ¨ç†ï¼›åˆ©ç”¨å®‰å…¨å…³é”®çš„æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œé¢„è®­ç»ƒ/æ•°æ®å¢å¼ºï¼Œä»¥æ”¹è¿›é•¿æ—¶åºï¼ˆlong-horizonï¼‰çš„æ—¶é—´æ¨ç†èƒ½åŠ›ã€‚\",\n  \"applications\": \"åº”ç”¨å‰æ™¯ï¼šæ›´å®‰å…¨çš„è‡ªåŠ¨é©¾é©¶äº‹æ•…åˆ†æã€èˆªç©º/æµ·äº‹äº‹æ•…å®¡æŸ¥å·¥å…·ã€‚\",\n  \"question\": \"æœ‰é’ˆå¯¹æ€§çš„é¢„è®­ç»ƒåŠ ä¸Šå› æœæ¨¡å‹èƒ½å¦å¼¥è¡¥è¿™18%çš„å·®è·ï¼Ÿ\"\n}","summary":"**Introduction:** ğŸš€ Ever wondered how well AI understands real-world crashes? Even top models hit only ~18% on the hardest tasks. AccidentBench: a large-scale benchmark (â‰ˆ2,000 videos, >19k QA) unifying vehicle accidents with air & water scenarios to probe temporal, spatial & intent reasoning. Vital for safer multimodal systems.\n\n**Challenges:** ğŸ¯ Challenges:\n- Lack of benchmarks for safety-critical, dynamic real-world scenes.\n- L...","analyzed_at":"2025-10-01T10:54:56.799Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26634v1","arxiv_id":"2509.26634v1","title":"Scaling Spoken Language Models with Syllabic Speech Tokenization","abstract":"Spoken language models (SLMs) typically discretize speech into\nhigh-frame-rate tokens extracted from SSL speech models. As the most successful\nLMs are based on the Transformer architecture, processing these long token\nstreams with self-attention is expensive, as attention scales quadratically\nwith sequence length. A recent SSL work introduces acoustic tokenization of\nspeech at the syllable level, which is more interpretable and potentially more\nscalable with significant compression in token lengths (4-5 Hz). Yet, their\nvalue for spoken language modeling is not yet fully explored. We present the\nfirst systematic study of syllabic tokenization for spoken language modeling,\nevaluating models on a suite of SLU benchmarks while varying training data\nscale. Syllabic tokens can match or surpass the previous high-frame rate tokens\nwhile significantly cutting training and inference costs, achieving more than a\n2x reduction in training time and a 5x reduction in FLOPs. Our findings\nhighlight syllable-level language modeling as a promising path to efficient\nlong-context spoken language models.","authors":["Nicholas Lee","Cheol Jun Cho","Alan W Black","Gopala K. Anumanchipalli"],"published":"2025-09-30T17:59:09Z","updated":"2025-09-30T17:59:09Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26634v1","pdf_url":"http://arxiv.org/pdf/2509.26634v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ Ever seen speech LMs choke on long audio?\nThis paper shows syllable-level tokenization (â‰ˆ4â€“5 Hz) compresses speech tokens so spoken language models match or beat high-frame-rate tokens while cutting compute â€” enabling cheaper, longer-context SLU models.","challenges":"ğŸ¯ Key problems tackled:\n- High-frame-rate tokens produce very long sequences â†’ quadratic self-attention cost.\n- Long-context spoken models are computationally and memory limited.\n- The value of syllable-level tokenization for SLMs was unexplored.","innovations":"âœ¨ What they did:\n- First systematic study applying syllabic (syllable-level) tokenization to spoken language models.\n- Evaluated across SLU benchmarks while varying training data scale.\n- Demonstrated syllable tokens (4â€“5 Hz) can match/surpass high-rate tokens and greatly reduce compute.","experiments":"Not provided","insights":"Not provided","keywords":[],"category":"machine_learning","chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"translation\": \"ğŸš€ æ›¾è§è¿‡è¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆLMsï¼‰åœ¨å¤„ç†é•¿éŸ³é¢‘æ—¶è¡¨ç°ä¸ä½³å—ï¼Ÿ\\n\\næœ¬æ–‡å±•ç¤ºäº†éŸ³èŠ‚çº§åˆ†è¯ï¼ˆå¤§çº¦4â€“5èµ«å…¹ï¼‰èƒ½å¤Ÿå‹ç¼©è¯­éŸ³æ ‡è®°ï¼Œä½¿å¾—å£è¯­è¯­è¨€æ¨¡å‹åœ¨ä¿æŒæˆ–è¶…è¶Šé«˜å¸§ç‡æ ‡è®°æ€§èƒ½çš„åŒæ—¶ï¼Œå¤§å¹…å‡å°‘è®¡ç®—é‡â€”â€”ä»è€Œå®ç°æ›´ä½æˆæœ¬ã€æ›´é•¿ä¸Šä¸‹æ–‡çš„è¯­éŸ³ç†è§£ï¼ˆSLUï¼‰æ¨¡å‹ã€‚\"\n}","chinese_challenges":"{\n  \"key_problems_tackled\": \"ğŸ¯ è§£å†³çš„å…³é”®é—®é¢˜ï¼š\",\n  \"high_frame_rate_tokens_produce_very_long_sequences\": \"- é«˜å¸§ç‡çš„æ ‡è®°ï¼ˆtokensï¼‰ä¼šäº§ç”Ÿéå¸¸é•¿çš„åºåˆ— â†’ å¯¼è‡´è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰çš„è®¡ç®—æˆæœ¬å‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚\",\n  \"long_context_spoken_models_are_computationally_and_memory_limited\": \"- é•¿ä¸Šä¸‹æ–‡çš„è¯­éŸ³æ¨¡å‹åœ¨è®¡ç®—å’Œå†…å­˜æ–¹é¢å—åˆ°é™åˆ¶ã€‚\",\n  \"the_value_of_syllable_level_tokenization_for_slms_was_unexplored\": \"- éŸ³èŠ‚çº§æ ‡è®°åŒ–ï¼ˆsyllable-level tokenizationï¼‰å¯¹äºè¯­éŸ³è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰çš„ä»·å€¼å°šæœªè¢«æ¢ç´¢ã€‚\"\n}","chinese_innovations":"{\n  \"innovations\": {\n    \"title\": \"åˆ›æ–°ç‚¹\",\n    \"points\": [\n      {\n        \"description\": \"é¦–æ¬¡ç³»ç»Ÿæ€§åœ°ç ”ç©¶å°†éŸ³èŠ‚çº§ï¼ˆsyllable-levelï¼‰åˆ†è¯åº”ç”¨äºå£è¯­è¯­è¨€æ¨¡å‹ï¼ˆSpoken Language Models, SLMï¼‰ã€‚\"\n      },\n      {\n        \"description\": \"åœ¨ä¸åŒçš„è®­ç»ƒæ•°æ®è§„æ¨¡ä¸‹ï¼Œè·¨è¶Šå£è¯­ç†è§£ï¼ˆSpoken Language Understanding, SLUï¼‰åŸºå‡†è¿›è¡Œäº†è¯„ä¼°ã€‚\"\n      },\n      {\n        \"description\": \"è¯æ˜äº†éŸ³èŠ‚åˆ†è¯ï¼ˆ4â€“5 Hzï¼‰å¯ä»¥åŒ¹é…/è¶…è¶Šé«˜é¢‘åˆ†è¯çš„æ€§èƒ½ï¼Œå¹¶èƒ½å¤§å¹…å‡å°‘è®¡ç®—éœ€æ±‚ã€‚\"\n      }\n    ]\n  }\n}","chinese_experiments":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_insights":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","relevance_score":5,"summary":"**Introduction:** ğŸš€ Ever seen speech LMs choke on long audio?\nThis paper shows syllable-level tokenization (â‰ˆ4â€“5 Hz) compresses speech tokens so spoken language models match or beat high-frame-rate tokens while cutting compute â€” enabling cheaper, longer-context SLU models.\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- High-frame-rate tokens produce very long sequences â†’ quadratic self-attention cost.\n- Long-context spoken models are ...","analyzed_at":"2025-10-01T10:55:43.144Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26633v1","arxiv_id":"2509.26633v1","title":"OmniRetarget: Interaction-Preserving Data Generation for Humanoid\n  Whole-Body Loco-Manipulation and Scene Interaction","abstract":"A dominant paradigm for teaching humanoid robots complex skills is to\nretarget human motions as kinematic references to train reinforcement learning\n(RL) policies. However, existing retargeting pipelines often struggle with the\nsignificant embodiment gap between humans and robots, producing physically\nimplausible artifacts like foot-skating and penetration. More importantly,\ncommon retargeting methods neglect the rich human-object and human-environment\ninteractions essential for expressive locomotion and loco-manipulation. To\naddress this, we introduce OmniRetarget, an interaction-preserving data\ngeneration engine based on an interaction mesh that explicitly models and\npreserves the crucial spatial and contact relationships between an agent, the\nterrain, and manipulated objects. By minimizing the Laplacian deformation\nbetween the human and robot meshes while enforcing kinematic constraints,\nOmniRetarget generates kinematically feasible trajectories. Moreover,\npreserving task-relevant interactions enables efficient data augmentation, from\na single demonstration to different robot embodiments, terrains, and object\nconfigurations. We comprehensively evaluate OmniRetarget by retargeting motions\nfrom OMOMO, LAFAN1, and our in-house MoCap datasets, generating over 8-hour\ntrajectories that achieve better kinematic constraint satisfaction and contact\npreservation than widely used baselines. Such high-quality data enables\nproprioceptive RL policies to successfully execute long-horizon (up to 30\nseconds) parkour and loco-manipulation skills on a Unitree G1 humanoid, trained\nwith only 5 reward terms and simple domain randomization shared by all tasks,\nwithout any learning curriculum.","authors":["Lujie Yang","Xiaoyu Huang","Zhen Wu","Angjoo Kanazawa","Pieter Abbeel","Carmelo Sferrazza","C. Karen Liu","Rocky Duan","Guanya Shi"],"published":"2025-09-30T17:59:02Z","updated":"2025-09-30T17:59:02Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26633v1","pdf_url":"http://arxiv.org/pdf/2509.26633v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ Want humanoid robots to move and interact as fluidly as humans? OmniRetarget is an interaction-preserving data engine that retargets human motion to robots by modeling contacts and scene relations â€” producing kinematically feasible, interaction-aware demos for RL. Benefits: better loco-manipulation and sim-to-real training.","challenges":"ğŸ¯ Key problems tackled:\n- Existing retargeting fails across the humanâ†”robot embodiment gap (footâ€‘skate, penetration).\n- Prior pipelines ignore rich humanâ€“object and humanâ€“environment interactions.\n- Limited data augmentation across robots/terrains/objects hampers robust RL.","innovations":"âœ¨ Core innovations:\n- Interaction mesh that explicitly encodes spatial/contact relationships between agent, terrain, and objects.\n- Retargeting via minimizing Laplacian deformation between human and robot meshes while enforcing kinematic constraints.\n- Interaction-preserving data augmentation to transfer a single demo across embodiments, terrains, and object configs.\nNovelty: explicit interaction mesh + Laplacian deformation for retargeting to preserve contacts and task geometry.","experiments":"ğŸ“Š Results: Retargeted motions from OMOMO, LAFAN1 and in-house MoCap to produce over 8 hours of trajectories with better kinematic-constraint satisfaction and contact preservation than common baselines; enabled proprioceptive RL to execute long-horizon (up to 30s) parkour and loco-manipulation on a Unitree G1, trained with only 5 reward terms and simple domain randomization.","insights":"ğŸ¤” Whatâ€™s next?\n- Research: adapt OmniRetarget for online/adaptive retargeting and multi-robot transfer, or combine with perception for closed-loop real-world interactions.\n- Applications: fast data generation for real-world humanoid deployment (assistive robots, warehouse manipulation, entertainment).\nCould interaction-preserving retargeting unlock more reliable sim-to-real loco-manipulation?","keywords":["retargeting","humanoid","interaction mesh","Laplacian deformation","loco-manipulation","contact preservation","reinforcement learning","proprioceptive policy","Unitree G1","data augmentation","OMOMO","LAFAN1"],"category":"robotics","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"\"ğŸš€ æƒ³è¦äººå½¢æœºå™¨äººåƒäººç±»ä¸€æ ·æµç•…åœ°ç§»åŠ¨å’Œäº¤äº’å—ï¼ŸOmniRetarget æ˜¯ä¸€ç§ä¿æŒäº¤äº’æ€§çš„æ•°æ®å¼•æ“ï¼Œå®ƒé€šè¿‡å»ºæ¨¡æ¥è§¦å’Œåœºæ™¯å…³ç³»ï¼Œå°†äººç±»åŠ¨ä½œé‡å®šå‘åˆ°æœºå™¨äººä¸Šï¼Œä»è€Œä¸ºå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç”Ÿæˆè¿åŠ¨å­¦å¯è¡Œã€äº¤äº’æ„ŸçŸ¥çš„æ¼”ç¤ºæ•°æ®ã€‚ä¼˜åŠ¿åŒ…æ‹¬ï¼šæå‡è¿åŠ¨æ“ä½œèƒ½åŠ›ï¼ˆloco-manipulationï¼‰ä»¥åŠæ”¹è¿›è™šå®è¿ç§»ï¼ˆsim-to-realï¼‰è®­ç»ƒæ•ˆæœã€‚\"","chinese_challenges":"{\n  \"key_problems_tackled\": [\n    \"Existing retargeting fails across the humanâ†”robot embodiment gap (footâ€‘skate, penetration).\",\n    \"Prior pipelines ignore rich humanâ€“object and humanâ€“environment interactions.\",\n    \"Limited data augmentation across robots/terrains/objects hampers robust RL.\"\n  ]\n}","chinese_innovations":"{\n  \"innovations\": \"âœ¨ æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š\\n- äº¤äº’ç½‘æ ¼ï¼šæ˜¾å¼ç¼–ç æ™ºèƒ½ä½“ã€åœ°å½¢å’Œç‰©ä½“ä¹‹é—´çš„ç©ºé—´/æ¥è§¦å…³ç³»ã€‚\\n- é‡å®šå‘ï¼šé€šè¿‡æœ€å°åŒ–äººä½“å’Œæœºå™¨äººç½‘æ ¼ä¹‹é—´çš„æ‹‰æ™®æ‹‰æ–¯å½¢å˜ï¼ŒåŒæ—¶å¼ºåˆ¶æ‰§è¡Œè¿åŠ¨å­¦çº¦æŸæ¥å®ç°ã€‚\\n- ä¿æŒäº¤äº’çš„æ•°æ®å¢å¼ºï¼šå°†å•ä¸ªæ¼”ç¤ºè·¨è¶Šä¸åŒå®ä½“ã€åœ°å½¢å’Œç‰©ä½“é…ç½®è¿›è¡Œè¿ç§»ã€‚\\næ–°é¢–æ€§ï¼šæ˜¾å¼çš„äº¤äº’ç½‘æ ¼ + ç”¨äºé‡å®šå‘çš„æ‹‰æ™®æ‹‰æ–¯å½¢å˜ï¼Œä»¥ä¿æŒæ¥è§¦å’Œä»»åŠ¡å‡ ä½•ç»“æ„ã€‚\"\n}","chinese_experiments":"{\n  \"translation\": \"ğŸ“Š ç»“æœï¼šæˆ‘ä»¬å¯¹æ¥è‡ª OMOMOã€LAFAN1 å’Œå†…éƒ¨ MoCapï¼ˆè¿åŠ¨æ•æ‰ï¼‰æ•°æ®çš„åŠ¨ä½œè¿›è¡Œäº†é‡å®šå‘ï¼Œç”Ÿæˆäº†è¶…è¿‡ 8 å°æ—¶çš„è½¨è¿¹ã€‚ä¸å¸¸è§çš„åŸºçº¿æ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›è½¨è¿¹åœ¨æ»¡è¶³è¿åŠ¨å­¦çº¦æŸå’Œä¿æŒæ¥è§¦æ–¹é¢è¡¨ç°æ›´ä½³ï¼›è¿™ä½¿å¾—æœ¬ä½“æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆproprioceptive RLï¼‰èƒ½å¤Ÿåœ¨ Unitree G1 æœºå™¨äººä¸Šæ‰§è¡Œé•¿æ—¶ç¨‹ï¼ˆæœ€é•¿è¾¾ 30 ç§’ï¼‰çš„è·‘é…·å’Œç§»åŠ¨æ“ä½œä»»åŠ¡ï¼Œä¸”è®­ç»ƒä»…ä½¿ç”¨äº† 5 ä¸ªå¥–åŠ±é¡¹å’Œç®€å•çš„åŸŸéšæœºåŒ–ã€‚\"\n}","chinese_insights":"{\n  \"translation\": \"ğŸ¤” ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ\\n- ç ”ç©¶ï¼šä½¿ OmniRetarget é€‚åº”åœ¨çº¿/è‡ªé€‚åº”é‡å®šå‘å’Œå¤šæœºå™¨äººè¿ç§»ï¼Œæˆ–å°†å…¶ä¸æ„ŸçŸ¥ç›¸ç»“åˆï¼Œå®ç°é—­ç¯çš„çœŸå®ä¸–ç•Œäº¤äº’ã€‚\\n- åº”ç”¨ï¼šä¸ºçœŸå®ä¸–ç•Œçš„äººå½¢æœºå™¨äººéƒ¨ç½²ï¼ˆè¾…åŠ©æœºå™¨äººã€ä»“åº“æ“ä½œã€å¨±ä¹ï¼‰å¿«é€Ÿç”Ÿæˆæ•°æ®ã€‚\\nè¿™ç§ä¿æŒäº¤äº’çš„é‡å®šå‘èƒ½å¦è§£é”æ›´å¯é çš„â€œä»¿çœŸåˆ°ç°å®â€çš„è¿åŠ¨-æ“ä½œï¼ˆloco-manipulationï¼‰èƒ½åŠ›ï¼Ÿ\"\n}","summary":"**Introduction:** ğŸš€ Want humanoid robots to move and interact as fluidly as humans? OmniRetarget is an interaction-preserving data engine that retargets human motion to robots by modeling contacts and scene relations â€” producing kinematically feasible, interaction-aware demos for RL. Benefits: better loco-manipulation and sim-to-real training.\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Existing retargeting fails across the humanâ†”r...","analyzed_at":"2025-10-01T10:56:39.287Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26632v1","arxiv_id":"2509.26632v1","title":"Branching Out: Broadening AI Measurement and Evaluation with Measurement\n  Trees","abstract":"This paper introduces \\textit{measurement trees}, a novel class of metrics\ndesigned to combine various constructs into an interpretable multi-level\nrepresentation of a measurand. Unlike conventional metrics that yield single\nvalues, vectors, surfaces, or categories, measurement trees produce a\nhierarchical directed graph in which each node summarizes its children through\nuser-defined aggregation methods. In response to recent calls to expand the\nscope of AI system evaluation, measurement trees enhance metric transparency\nand facilitate the integration of heterogeneous evidence, including, e.g.,\nagentic, business, energy-efficiency, sociotechnical, or security signals. We\npresent definitions and examples, demonstrate practical utility through a\nlarge-scale measurement exercise, and provide accompanying open-source Python\ncode. By operationalizing a transparent approach to measurement of complex\nconstructs, this work offers a principled foundation for broader and more\ninterpretable AI evaluation.","authors":["Craig Greenberg","Patrick Hall","Theodore Jensen","Kristen Greene","Razvan Amironesei"],"published":"2025-09-30T17:58:59Z","updated":"2025-09-30T17:58:59Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26632v1","pdf_url":"http://arxiv.org/pdf/2509.26632v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ Question: How do you measure complex AI traits beyond a single number? \nEnter measurement trees: a hierarchical, interpretable metric framework that aggregates heterogeneous signals into a directed tree. \nHelps researchers, auditors, and product teams evaluate complex constructs.","challenges":"ğŸ¯ Problems solved:\n- Single-value metrics hide nuance and multi-level structure.\n- Hard to combine heterogeneous evidence (agentic, business, energy, sociotechnical, security).\n- Aggregations are often opaque, reducing metric transparency and interpretability.","innovations":"âœ¨ Innovations:\n- Introduced â€œmeasurement treesâ€: hierarchical directed graphs where nodes summarize children.\n- User-defined aggregation functions let you combine heterogeneous signals transparently.\n- Produces multi-level, interpretable representations vs single numbers/vectors.\n- Accompanied by open-source Python code.","experiments":"ğŸ“Š Experiment:\nThe paper demonstrates practical utility via a large-scale measurement exercise and provides open-source Python code to reproduce workflows.\nQuantitative specifics (e.g., % improvement or numeric benchmarks) â€” Not specified in the paper.","insights":"ğŸ¤” Insights & next steps:\n- Research directions: automate aggregator selection, integrate trees into continuous model monitoring and benchmarking platforms.\n- Applications: forensic model audits, regulatory reporting, cross-domain evaluation (energy, social, security).\nCould measurement trees change how we benchmark AI?","keywords":["measurement trees","evaluation","metrics","hierarchical metrics","AI measurement","interpretability","aggregation","audit"],"category":"machine_learning","relevance_score":8,"technical_depth":"intermediate","chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"translation\": \"ğŸš€ é—®é¢˜ï¼šå¦‚ä½•è¡¡é‡è¶…å‡ºå•ä¸€æ•°å­—çš„å¤æ‚AIç‰¹è´¨ï¼Ÿ\\n\\nç­”æ¡ˆæ˜¯æµ‹é‡æ ‘ï¼ˆmeasurement treesï¼‰ï¼šä¸€ç§åˆ†å±‚çš„ã€å¯è§£é‡Šçš„åº¦é‡æ¡†æ¶ï¼Œå®ƒå°†å¼‚æ„ä¿¡å·èšåˆæˆä¸€ä¸ªæœ‰å‘æ ‘ã€‚è¿™æœ‰åŠ©äºç ”ç©¶äººå‘˜ã€å®¡è®¡äººå‘˜å’Œäº§å“å›¢é˜Ÿè¯„ä¼°å¤æ‚çš„ç»“æ„ã€‚\"\n}","chinese_challenges":"\"ğŸ¯ å¾…è§£å†³çš„æŒ‘æˆ˜ï¼š\\n- å•ä¸€æ•°å€¼æŒ‡æ ‡å¾€å¾€æ©ç›–äº†ç»†å¾®å·®åˆ«å’Œæ½œåœ¨çš„å¤šå±‚æ¬¡ç»“æ„ã€‚\\n- éš¾ä»¥æ•´åˆå¼‚æ„çš„è¯æ®ï¼ˆå¦‚ï¼šä¸»ä½“æ€§ã€å•†ä¸šä»·å€¼ã€èƒ½æºæ¶ˆè€—ã€ç¤¾ä¼šæŠ€æœ¯å½±å“ã€å®‰å…¨æ€§ç­‰æ–¹é¢ï¼‰ã€‚\\n- èšåˆè®¡ç®—è¿‡ç¨‹é€šå¸¸ä¸é€æ˜ï¼Œè¿™é™ä½äº†æŒ‡æ ‡çš„é€æ˜åº¦å’Œå¯è§£é‡Šæ€§ã€‚\"","chinese_innovations":"{\n  \"translation\": \"âœ¨ åˆ›æ–°ç‚¹ï¼š\\n- å¼•å…¥äº†â€œæµ‹é‡æ ‘â€ï¼ˆmeasurement treesï¼‰ï¼šä¸€ç§åˆ†å±‚çš„æœ‰å‘å›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹æ¦‚æ‹¬äº†å…¶å­èŠ‚ç‚¹çš„ä¿¡æ¯ã€‚\\n- ç”¨æˆ·è‡ªå®šä¹‰çš„èšåˆå‡½æ•°å…è®¸é€æ˜åœ°ç»„åˆå¼‚æ„ä¿¡å·ï¼ˆheterogeneous signalsï¼‰ã€‚\\n- ç”Ÿæˆå¤šå±‚æ¬¡ã€å¯è§£é‡Šçš„è¡¨ç¤ºï¼Œè€Œéå•ä¸€çš„æ•°å­—æˆ–å‘é‡ã€‚\\n- é™„å¸¦å¼€æºPythonä»£ç ã€‚\"\n}","chinese_experiments":"{\n  \"chinese_translation\": \"ğŸ“Š å®éªŒï¼š\\næœ¬æ–‡é€šè¿‡å¤§è§„æ¨¡æµ‹é‡å®è·µè¯æ˜äº†å…¶å®ç”¨æ€§ï¼Œå¹¶æä¾›äº†å¼€æº Python ä»£ç ä»¥å¤ç°å·¥ä½œæµç¨‹ã€‚\\né‡åŒ–ç»†èŠ‚ï¼ˆä¾‹å¦‚ï¼Œç™¾åˆ†æ¯”æå‡æˆ–æ•°å­—åŸºå‡†ï¼‰â€”â€” è®ºæ–‡ä¸­æœªå…·ä½“è¯´æ˜ã€‚\"\n}","chinese_insights":"{\n\"translation\": \"ğŸ¤” è§è§£ä¸ä¸‹ä¸€æ­¥ï¼š\\n- ç ”ç©¶æ–¹å‘ï¼šè‡ªåŠ¨åŒ–èšåˆå™¨é€‰æ‹©ï¼Œå°†æ ‘ç»“æ„é›†æˆåˆ°æŒç»­æ¨¡å‹ç›‘æ§å’ŒåŸºå‡†æµ‹è¯•å¹³å°ä¸­ã€‚\\n- åº”ç”¨ï¼šå–è¯æ¨¡å‹å®¡è®¡ã€ç›‘ç®¡æŠ¥å‘Šã€è·¨é¢†åŸŸè¯„ä¼°ï¼ˆèƒ½æºã€ç¤¾ä¼šã€å®‰å…¨ï¼‰ã€‚\\n- æµ‹é‡æ ‘èƒ½å¦æ”¹å˜æˆ‘ä»¬å¯¹AIè¿›è¡ŒåŸºå‡†æµ‹è¯•çš„æ–¹å¼ï¼Ÿ\"\n}","summary":"**Introduction:** ğŸš€ Question: How do you measure complex AI traits beyond a single number? \nEnter measurement trees: a hierarchical, interpretable metric framework that aggregates heterogeneous signals into a directed tree. \nHelps researchers, auditors, and product teams evaluate complex constructs.\n\n**Challenges:** ğŸ¯ Problems solved:\n- Single-value metrics hide nuance and multi-level structure.\n- Hard to combine heterogeneous evi...","analyzed_at":"2025-10-01T10:57:25.498Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26631v1","arxiv_id":"2509.26631v1","title":"Learning Generalizable Shape Completion with SIM(3) Equivariance","abstract":"3D shape completion methods typically assume scans are pre-aligned to a\ncanonical frame. This leaks pose and scale cues that networks may exploit to\nmemorize absolute positions rather than inferring intrinsic geometry. When such\nalignment is absent in real data, performance collapses. We argue that robust\ngeneralization demands architectural equivariance to the similarity group,\nSIM(3), so the model remains agnostic to pose and scale. Following this\nprinciple, we introduce the first SIM(3)-equivariant shape completion network,\nwhose modular layers successively canonicalize features, reason over\nsimilarity-invariant geometry, and restore the original frame. Under a\nde-biased evaluation protocol that removes the hidden cues, our model\noutperforms both equivariant and augmentation baselines on the PCN benchmark.\nIt also sets new cross-domain records on real driving and indoor scans,\nlowering minimal matching distance on KITTI by 17% and Chamfer distance $\\ell1$\non OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol\nstill outperforms competitors under their biased settings. These results\nestablish full SIM(3) equivariance as an effective route to truly generalizable\nshape completion. Project page: https://sime-completion.github.io.","authors":["Yuqing Wang","Zhaiyu Chen","Xiao Xiang Zhu"],"published":"2025-09-30T17:58:55Z","updated":"2025-09-30T17:58:55Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26631v1","pdf_url":"http://arxiv.org/pdf/2509.26631v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ What if 3D completion models are secretly cheating with pose & scale cues? This paper introduces the first SIM(3)-equivariant shape completion network that is agnostic to pose and scale â€” improving real-world generalization for robotics, AR, and autonomous driving.","challenges":"ğŸ¯ Challenges:\n- Existing methods assume scans are pre-aligned, leaking pose/scale cues.\n- Networks memorize absolute positions instead of intrinsic geometry.\n- Performance collapses on unaligned, real-world scans (poor cross-domain generalization).","innovations":"âœ¨ Innovations:\n- First SIM(3)-equivariant shape completion network.\n- Modular layers that: canonicalize features â†’ reason over similarity-invariant geometry â†’ restore original frame.\n- De-biased evaluation protocol to remove hidden pose/scale cues.\nNovelty: full SIM(3) equivariance to force pose/scale agnosticism.","experiments":"ğŸ“Š Experiment (most compelling): Lowered minimal matching distance on KITTI by 17% vs prior methods â€” proving SIM(3) equivariance substantially improves cross-domain shape completion generalization.","insights":"ğŸ¤” Insights / Next steps:\n- Explore combining SIM(3)-equivariant completion with learned pose priors or SLAM for online reconstruction.\n- Apply to autonomous driving, indoor mapping, AR/VR where scans are unaligned.\nCould SIM(3) equivariance become a standard inductive bias for 3D tasks?","keywords":["SIM(3) equivariance","shape completion","3D deep learning","point clouds","generalization","canonicalization"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"translation\": \"ğŸš€ å¦‚æœ3Dè¡¥å…¨æ¨¡å‹å·å·åˆ©ç”¨å§¿æ€å’Œå°ºåº¦çº¿ç´¢ä½œå¼Šæ€ä¹ˆåŠï¼Ÿæœ¬æ–‡ä»‹ç»äº†é¦–ä¸ªSIM(3)ç­‰å˜å½¢çŠ¶è¡¥å…¨ç½‘ç»œï¼Œè¯¥ç½‘ç»œå¯¹å§¿æ€å’Œå°ºåº¦ä¸æ•æ„Ÿâ€”â€”ä»è€Œæé«˜äº†å…¶åœ¨æœºå™¨äººæŠ€æœ¯ã€å¢å¼ºç°å®ï¼ˆARï¼‰å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸçš„çœŸå®ä¸–ç•Œæ³›åŒ–èƒ½åŠ›ã€‚\"\n}","chinese_challenges":"{\n  \"challenges\": \"ğŸ¯ æŒ‘æˆ˜ï¼š\\n- ç°æœ‰æ–¹æ³•å‡è®¾æ‰«ææ•°æ®å·²é¢„å…ˆå¯¹é½ï¼Œè¿™ä¼šæ³„éœ²å§¿æ€/å°ºåº¦çº¿ç´¢ã€‚\\n- ç½‘ç»œå€¾å‘äºè®°å¿†ç»å¯¹ä½ç½®ï¼Œè€Œéå†…åœ¨å‡ ä½•ç»“æ„ã€‚\\n- åœ¨æœªå¯¹é½çš„çœŸå®ä¸–ç•Œæ‰«ææ•°æ®ä¸Šï¼Œæ€§èƒ½ä¼šæ€¥å‰§ä¸‹é™ï¼ˆè·¨åŸŸæ³›åŒ–èƒ½åŠ›å·®ï¼‰ã€‚\"\n}","chinese_innovations":"{\n  \"innovations\": [\n    \"é¦–ä¸ªSIM(3)ç­‰å˜å½¢çŠ¶è¡¥å…¨ç½‘ç»œã€‚\",\n    \"æ¨¡å—åŒ–å±‚ï¼šå°†ç‰¹å¾è§„èŒƒåŒ– $\\\\rightarrow$ åŸºäºç›¸ä¼¼æ€§ä¸å˜å‡ ä½•è¿›è¡Œæ¨ç† $\\\\rightarrow$ æ¢å¤åŸå§‹åæ ‡ç³»ã€‚\",\n    \"å»åå€šè¯„ä¼°åè®®ï¼Œä»¥æ¶ˆé™¤éšè—çš„ä½å§¿/å°ºåº¦çº¿ç´¢ã€‚\",\n    \"æ–°é¢–æ€§ï¼šå®Œå…¨SIM(3)ç­‰å˜æ€§ï¼Œä»¥å¼ºåˆ¶å®ç°ä½å§¿/å°ºåº¦æ— å…³æ€§ã€‚\"\n  ]\n}","chinese_experiments":"\"å®éªŒï¼ˆæœ€å…·è¯´æœåŠ›çš„ï¼‰ï¼šåœ¨KITTIæ•°æ®é›†ä¸Šï¼Œå°†æœ€å°åŒ¹é…è·ç¦»æ¯”ç°æœ‰æ–¹æ³•é™ä½äº†17%ï¼Œè¯æ˜äº†SIM(3)ç­‰å˜æ€§æ˜¾è‘—æé«˜äº†è·¨åŸŸå½¢çŠ¶è¡¥å…¨çš„æ³›åŒ–èƒ½åŠ›ã€‚\"","chinese_insights":"\"ğŸ¤” æ´å¯Ÿ/ä¸‹ä¸€æ­¥ï¼š\\n- æ¢ç´¢å°†SIM(3)ç­‰å˜è¡¥å…¨ä¸å­¦ä¹ åˆ°çš„å§¿æ€å…ˆéªŒæˆ–SLAMç›¸ç»“åˆï¼Œç”¨äºåœ¨çº¿é‡å»ºã€‚\\n- åº”ç”¨äºè‡ªåŠ¨é©¾é©¶ã€å®¤å†…å»ºå›¾ã€AR/VRç­‰æ‰«ææ•°æ®æœªå¯¹é½çš„åœºæ™¯ã€‚\\n- SIM(3)ç­‰å˜æ€§æ˜¯å¦èƒ½æˆä¸º3Dä»»åŠ¡çš„æ ‡å‡†å½’çº³åç½®ï¼Ÿ\"","summary":"**Introduction:** ğŸš€ What if 3D completion models are secretly cheating with pose & scale cues? This paper introduces the first SIM(3)-equivariant shape completion network that is agnostic to pose and scale â€” improving real-world generalization for robotics, AR, and autonomous driving.\n\n**Challenges:** ğŸ¯ Challenges:\n- Existing methods assume scans are pre-aligned, leaking pose/scale cues.\n- Networks memorize absolute positions inst...","analyzed_at":"2025-10-01T10:58:18.765Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26628v1","arxiv_id":"2509.26628v1","title":"Attention as a Compass: Efficient Exploration for Process-Supervised RL\n  in Reasoning Models","abstract":"Reinforcement Learning (RL) has shown remarkable success in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Process-Supervised RL\n(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.\nHowever, existing PSRL approaches suffer from limited exploration efficiency,\nboth in terms of branching positions and sampling. In this paper, we introduce\na novel PSRL framework (AttnRL), which enables efficient exploration for\nreasoning models. Motivated by preliminary observations that steps exhibiting\nhigh attention scores correlate with reasoning behaviors, we propose to branch\nfrom positions with high values. Furthermore, we develop an adaptive sampling\nstrategy that accounts for problem difficulty and historical batch size,\nensuring that the whole training batch maintains non-zero advantage values. To\nfurther improve sampling efficiency, we design a one-step off-policy training\npipeline for PSRL. Extensive experiments on multiple challenging mathematical\nreasoning benchmarks demonstrate that our method consistently outperforms prior\napproaches in terms of performance and sampling and training efficiency.","authors":["Runze Liu","Jiakang Wang","Yuling Shi","Zhihui Xie","Chenxin An","Kaiyan Zhang","Jian Zhao","Xiaodong Gu","Lei Lin","Wenping Hu","Xiu Li","Fuzheng Zhang","Guorui Zhou","Kun Gai"],"published":"2025-09-30T17:58:34Z","updated":"2025-09-30T17:58:34Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26628v1","pdf_url":"http://arxiv.org/pdf/2509.26628v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ Want LLMs that reason more reliably? AttnRL uses model attention as a \\","challenges":"ğŸ¯ Key problems tackled:\n- Existing PSRL has poor exploration: bad branching-position choices.\n- Inefficient sampling leads to wasted training budget and zero-advantage batches.\n- Training pipelines for PSRL are sample- and compute-inefficient.","innovations":"âœ¨ Core novelties:\n- AttnRL: branch from tokens/steps with high attention scores (attention-as-compass).\n- Adaptive sampling that factors problem difficulty + historical batch size to keep non-zero advantages.\n- One-step off-policy training pipeline for PSRL to boost sample efficiency.","experiments":"ğŸ“Š Main result: AttnRL \\","insights":"ğŸ¤” What's next?\n- Explore attention-guided exploration across other reasoning tasks (programming, theorem proving) and multimodal models.\n- Apply in real-world tutoring, automated math assistants, or scientific problem solving to reduce training cost. Could attention-driven exploration generalize broadly?","category":"reinforcement_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"\"ğŸš€ æƒ³è¦è·å¾—æ¨ç†èƒ½åŠ›æ›´å¯é çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å—ï¼ŸAttnRL åˆ©ç”¨æ¨¡å‹æ³¨æ„åŠ›ä½œä¸ºä¸€ç§\"","chinese_challenges":"{\n  \"challenges\": \"ğŸ¯ å…³é”®é—®é¢˜ï¼š\\n- ç°æœ‰ PSRL çš„æ¢ç´¢èƒ½åŠ›è¾ƒå·®ï¼šåˆ†æ”¯ä½ç½®é€‰æ‹©ä¸ä½³ã€‚\\n- ä½æ•ˆçš„é‡‡æ ·å¯¼è‡´è®­ç»ƒé¢„ç®—æµªè´¹å’Œé›¶ä¼˜åŠ¿æ‰¹æ¬¡ï¼ˆzero-advantage batchesï¼‰ã€‚\\n- PSRL çš„è®­ç»ƒæµç¨‹åœ¨æ ·æœ¬å’Œè®¡ç®—æ•ˆç‡ä¸Šéƒ½å¾ˆä½ä¸‹ã€‚\"\n}","chinese_innovations":"{\n  \"innovations_zh\": \"âœ¨ æ ¸å¿ƒåˆ›æ–°ç‚¹:\\n- AttnRL: ä»å…·æœ‰é«˜æ³¨æ„åŠ›å¾—åˆ†çš„tokens/æ­¥éª¤ï¼ˆå°†æ³¨æ„åŠ›è§†ä¸ºæŒ‡å—é’ˆï¼‰è¿›è¡Œåˆ†æ”¯ã€‚\\n- è‡ªé€‚åº”é‡‡æ ·ï¼šç»“åˆé—®é¢˜éš¾åº¦å’Œå†å²æ‰¹æ¬¡å¤§å°ï¼Œä»¥ä¿æŒéé›¶ä¼˜åŠ¿ï¼ˆnon-zero advantagesï¼‰ã€‚\\n- ç”¨äºPSRLï¼ˆæ¦‚ç‡çŠ¶æ€è¡¨å¾å­¦ä¹ ï¼‰çš„ä¸€æ­¥å¼ç¦»ç­–ç•¥è®­ç»ƒæµç¨‹ï¼Œä»¥æé«˜æ ·æœ¬æ•ˆç‡ã€‚\"\n}","chinese_experiments":"{\n  \"translation\": \"ğŸ“Š ä¸»è¦ç»“æœï¼šAttnRL\"\n}","chinese_insights":"{\n  \"translation\": \"ğŸ¤” ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ\\n- æ¢ç´¢æ³¨æ„åŠ›å¼•å¯¼çš„æ¢ç´¢æœºåˆ¶åœ¨å…¶ä»–æ¨ç†ä»»åŠ¡ï¼ˆç¼–ç¨‹ã€å®šç†è¯æ˜ï¼‰å’Œå¤šæ¨¡æ€æ¨¡å‹ä¸­çš„åº”ç”¨ã€‚\\n- å°†å…¶åº”ç”¨äºç°å®ä¸–ç•Œçš„è¾…å¯¼ã€è‡ªåŠ¨åŒ–æ•°å­¦åŠ©æ‰‹æˆ–ç§‘å­¦é—®é¢˜è§£å†³ä¸­ï¼Œä»¥é™ä½è®­ç»ƒæˆæœ¬ã€‚æ³¨æ„åŠ›é©±åŠ¨çš„æ¢ç´¢æœºåˆ¶æ˜¯å¦èƒ½å¹¿æ³›åœ°è¿›è¡Œæ³›åŒ–ï¼Ÿ\"\n}","summary":"**Introduction:** ğŸš€ Want LLMs that reason more reliably? AttnRL uses model attention as a \\\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Existing PSRL has poor exploration: bad branching-position choices.\n- Inefficient sampling leads to wasted training budget and zero-advantage batches.\n- Training pipelines for PSRL are sample- and compute-inefficient.","analyzed_at":"2025-10-01T10:59:14.257Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26627v1","arxiv_id":"2509.26627v1","title":"TimeRewarder: Learning Dense Reward from Passive Videos via Frame-wise\n  Temporal Distance","abstract":"Designing dense rewards is crucial for reinforcement learning (RL), yet in\nrobotics it often demands extensive manual effort and lacks scalability. One\npromising solution is to view task progress as a dense reward signal, as it\nquantifies the degree to which actions advance the system toward task\ncompletion over time. We present TimeRewarder, a simple yet effective reward\nlearning method that derives progress estimation signals from passive videos,\nincluding robot demonstrations and human videos, by modeling temporal distances\nbetween frame pairs. We then demonstrate how TimeRewarder can supply step-wise\nproxy rewards to guide reinforcement learning. In our comprehensive experiments\non ten challenging Meta-World tasks, we show that TimeRewarder dramatically\nimproves RL for sparse-reward tasks, achieving nearly perfect success in 9/10\ntasks with only 200,000 interactions per task with the environment. This\napproach outperformed previous methods and even the manually designed\nenvironment dense reward on both the final success rate and sample efficiency.\nMoreover, we show that TimeRewarder pretraining can exploit real-world human\nvideos, highlighting its potential as a scalable approach path to rich reward\nsignals from diverse video sources.","authors":["Yuyang Liu","Chuan Wen","Yihang Hu","Dinesh Jayaraman","Yang Gao"],"published":"2025-09-30T17:58:20Z","updated":"2025-09-30T17:58:20Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26627v1","pdf_url":"http://arxiv.org/pdf/2509.26627v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ What if robots could learn dense rewards from passive videos?\nTimeRewarder learns frame-wise task progress by modeling temporal distance between video frames (robot & human) and supplies step-wise proxy rewards to guide RL â€” scaling dense reward design for robotics.","challenges":"ğŸ¯ Problems solved:\n- Designing dense rewards is manual and not scalable.\n- Sparse rewards make RL sample-inefficient for robotic tasks.\n- Leveraging passive (robot/human) videos for dense progress signals is non-trivial.","innovations":"âœ¨ Key ideas:\n- Learn progress by modeling frame-wise temporal distances between video frame pairs.\n- Derive dense progress estimates from passive videos (robot demos and human videos).\n- Use these estimates as step-wise proxy rewards to train RL agents.\nNovelty: framing dense reward learning as a frame-wise temporal-distance prediction problem that can pretrain on passive videos.","experiments":"ğŸ“Š Results: TimeRewarder achieved nearly perfect success in 9/10 Meta-World tasks with only 200,000 interactions per task. It outperformed prior methods and even the manually designed environment dense reward on both final success rate and sample efficiency.","insights":"ğŸ¤” What's next?\n- Research: scale pretraining on large web/human video corpora and study transfer across robot embodiments.\n- Applications: automated dense reward specification for real robots; learning from heterogeneous human videos for new tasks.\nCould large-scale video pretraining replace manual reward engineering?","category":"reinforcement_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"[\n  \"æœºå™¨äººèƒ½å¦ä»è¢«åŠ¨è§†é¢‘ä¸­å­¦ä¹ å¯†é›†å¥–åŠ±ï¼Ÿ\",\n  \"TimeRewarder é€šè¿‡å»ºæ¨¡è§†é¢‘å¸§ï¼ˆåŒ…æ‹¬æœºå™¨äººå’Œäººç±»çš„ï¼‰ä¹‹é—´çš„æ—¶åºè·ç¦»ï¼Œå­¦ä¹ å¸§çº§åˆ«çš„ä»»åŠ¡è¿›åº¦ï¼Œå¹¶æä¾›æ­¥è¿›å¼çš„ä»£ç†å¥–åŠ±æ¥æŒ‡å¯¼å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ï¼Œä»è€Œå®ç°äº†æœºå™¨äººé¢†åŸŸå¯†é›†å¥–åŠ±è®¾è®¡çš„è§„æ¨¡åŒ–åº”ç”¨ã€‚\"\n]","chinese_challenges":"{\n  \"translation\": {\n    \"title\": \"ğŸ¯ å¾…è§£å†³çš„æŒ‘æˆ˜/é—®é¢˜ï¼š\",\n    \"points\": [\n      \"å¯†é›†å¥–åŠ±ï¼ˆDense Rewardsï¼‰çš„è®¾è®¡ä¾èµ–äººå·¥ï¼Œä¸”ä¸å…·å¤‡å¯æ‰©å±•æ€§ã€‚\",\n      \"ç¨€ç–å¥–åŠ±ï¼ˆSparse Rewardsï¼‰ä½¿å¾—å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åœ¨æœºå™¨äººä»»åŠ¡ä¸­æ ·æœ¬æ•ˆç‡ä½ä¸‹ï¼ˆSample-Inefficientï¼‰ã€‚\",\n      \"åˆ©ç”¨è¢«åŠ¨å¼ï¼ˆæœºå™¨äºº/äººç±»ï¼‰è§†é¢‘æ¥æå–å¯†é›†çš„è¿›åº¦ä¿¡å·ï¼ˆDense Progress Signalsï¼‰æ˜¯æå…·æŒ‘æˆ˜æ€§çš„ã€‚\"\n    ]\n  }\n}","chinese_innovations":"{\n  \"chinese_translation\": \"âœ¨ æ ¸å¿ƒæ€æƒ³ï¼š\\n- é€šè¿‡å»ºæ¨¡è§†é¢‘å¸§å¯¹ä¹‹é—´é€å¸§çš„æ—¶é—´è·ç¦»æ¥å­¦ä¹ è¿›åº¦ã€‚\\n- ä»è¢«åŠ¨è§†é¢‘ï¼ˆæœºå™¨äººæ¼”ç¤ºå’Œäººç±»è§†é¢‘ï¼‰ä¸­æ¨å¯¼å‡ºå¯†é›†çš„è¿›åº¦ä¼°è®¡ã€‚\\n- ä½¿ç”¨è¿™äº›ä¼°è®¡ä½œä¸ºé€æ­¥çš„ä»£ç†å¥–åŠ±æ¥è®­ç»ƒå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ™ºèƒ½ä½“ã€‚\\nåˆ›æ–°ç‚¹ï¼šå°†å¯†é›†å¥–åŠ±å­¦ä¹ æ„å»ºä¸ºä¸€ä¸ªé€å¸§æ—¶é—´è·ç¦»é¢„æµ‹é—®é¢˜ï¼Œè¯¥é—®é¢˜å¯ä»¥åˆ©ç”¨è¢«åŠ¨è§†é¢‘è¿›è¡Œé¢„è®­ç»ƒã€‚\"\n}","chinese_experiments":"{\n  \"translation\": \"ğŸ“Š ç»“æœï¼šTimeRewarder åœ¨ 10 ä¸ª Meta-World ä»»åŠ¡ä¸­çš„ 9 ä¸ªä»»åŠ¡ä¸Šå®ç°äº†è¿‘ä¹å®Œç¾çš„æˆåŠŸï¼Œä¸”æ¯ä¸ªä»»åŠ¡ä»…ä½¿ç”¨äº† 20 ä¸‡æ¬¡äº¤äº’ã€‚å®ƒåœ¨æœ€ç»ˆæˆåŠŸç‡å’Œæ ·æœ¬æ•ˆç‡ä¸¤æ–¹é¢ï¼Œå‡ä¼˜äºå…ˆå‰çš„æ–¹æ³•ï¼Œç”šè‡³è¶…è¶Šäº†æ‰‹åŠ¨è®¾è®¡çš„ç¯å¢ƒå¯†é›†å¥–åŠ±ã€‚\"\n}","chinese_insights":"[\n  {\n    \"insights\": \"ğŸ¤” ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ\\n- ç ”ç©¶ï¼šåœ¨å¤§å‹ç½‘ç»œ/äººç±»è§†é¢‘è¯­æ–™åº“ä¸Šæ‰©å±•é¢„è®­ç»ƒè§„æ¨¡ï¼Œå¹¶ç ”ç©¶è·¨æœºå™¨äººå®ä½“ï¼ˆembodimentsï¼‰çš„è¿ç§»èƒ½åŠ›ã€‚\\n- åº”ç”¨ï¼šä¸ºçœŸå®æœºå™¨äººè‡ªåŠ¨æŒ‡å®šå¯†é›†å¥–åŠ±ï¼›ä»å¼‚æ„çš„äººç±»è§†é¢‘ä¸­å­¦ä¹ æ–°ä»»åŠ¡ã€‚\\nå¤§è§„æ¨¡è§†é¢‘é¢„è®­ç»ƒèƒ½å¦å–ä»£æ‰‹åŠ¨å¥–åŠ±å·¥ç¨‹ï¼Ÿ\"\n  }\n]","summary":"**Introduction:** ğŸš€ What if robots could learn dense rewards from passive videos?\nTimeRewarder learns frame-wise task progress by modeling temporal distance between video frames (robot & human) and supplies step-wise proxy rewards to guide RL â€” scaling dense reward design for robotics.\n\n**Challenges:** ğŸ¯ Problems solved:\n- Designing dense rewards is manual and not scalable.\n- Sparse rewards make RL sample-inefficient for robotic t...","analyzed_at":"2025-10-01T11:00:40.455Z","model":"openai/gpt-5-mini"}},{"id":"arxiv_2509.26626v1","arxiv_id":"2509.26626v1","title":"Recursive Self-Aggregation Unlocks Deep Thinking in Large Language\n  Models","abstract":"Test-time scaling methods improve the capabilities of large language models\n(LLMs) by increasing the amount of compute used during inference to make a\nprediction. Inference-time compute can be scaled in parallel by choosing among\nmultiple independent solutions or sequentially through self-refinement. We\npropose Recursive Self-Aggregation (RSA), a test-time scaling method inspired\nby evolutionary methods that combines the benefits of both parallel and\nsequential scaling. Each step of RSA refines a population of candidate\nreasoning chains through aggregation of subsets to yield a population of\nimproved solutions, which are then used as the candidate pool for the next\niteration. RSA exploits the rich information embedded in the reasoning chains\n-- not just the final answers -- and enables bootstrapping from partially\ncorrect intermediate steps within different chains of thought. Empirically, RSA\ndelivers substantial performance gains with increasing compute budgets across\ndiverse tasks, model families and sizes. Notably, RSA enables\nQwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning\nmodels, including DeepSeek-R1 and o3-mini (high), while outperforming purely\nparallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning\nGym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the\nmodel to combine solutions via a novel aggregation-aware reinforcement learning\napproach yields significant performance gains. Code available at\nhttps://github.com/HyperPotatoNeo/RSA.","authors":["Siddarth Venkatraman","Vineet Jain","Sarthak Mittal","Vedant Shah","Johan Obando-Ceron","Yoshua Bengio","Brian R. Bartoldson","Bhavya Kailkhura","Guillaume Lajoie","Glen Berseth","Nikolay Malkin","Moksh Jain"],"published":"2025-09-30T17:58:03Z","updated":"2025-09-30T17:58:03Z","category":"","source":"arxiv","original_source":"arxiv","url":"http://arxiv.org/abs/2509.26626v1","pdf_url":"http://arxiv.org/pdf/2509.26626v1.pdf","scraped_at":"2025-10-01T09:31:12.598Z","analysis":{"introduction":"ğŸš€ Question: Can small LLMs 'think deeper' at inference time without more training?\nRSA (Recursive Self-Aggregation) is a test-time scaling method that iteratively aggregates and refines populations of reasoning chains to unlock deeper reasoningâ€”letting smaller models compete with bigger ones.","challenges":"ğŸ¯ Challenges:\n- Existing test-time scaling is typically either parallel (many independent tries) or sequential (self-refinement), not both.\n- Methods ignore partial correctness in intermediate reasoning steps across chains.\n- Inefficient use of inference compute for bootstrapping stronger solutions.","innovations":"âœ¨ Innovations:\n- Recursive Self-Aggregation (RSA): iteratively refines a population of candidate reasoning chains by aggregating subsets to produce improved candidates.\n- Exploits intermediate chain-of-thought content (not just final answers) to bootstrap.\n- Aggregation-aware reinforcement learning to train models to better combine solutions.","experiments":"ğŸ“Š Experiment (most compelling result):\nQwen3-4B-Instruct-2507 using RSA achieved competitive performance with larger reasoning models (DeepSeek-R1 and o3-mini (high)) and outperformed purely parallel and purely sequential scaling across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. Specific numeric gains: Not specified in the paper.","insights":"ğŸ¤” Insights & next steps:\n- Research: adapt RSA for multimodal or retrieval-augmented LLMs; develop adaptive subset-selection strategies or theory on when aggregation helps.\n- Applications: stronger reasoning in education/tutoring, code synthesis, and decision supportâ€”letting smaller models punch above their size. Could RSA enable cheaper, reliable 'deep thinking' at scale?","category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"title\": \"é€’å½’è‡ªèšåˆï¼ˆRSAï¼‰ï¼šè®©å°å‹LLMåœ¨æ¨ç†æ—¶â€œæ·±åº¦æ€è€ƒâ€\",\n  \"content\": \"ğŸš€ é—®é¢˜ï¼šå°å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¦åœ¨ä¸å¢åŠ è®­ç»ƒé‡çš„æƒ…å†µä¸‹ï¼Œåœ¨æ¨ç†æ—¶è¿›è¡Œâ€œæ›´æ·±å±‚æ¬¡çš„æ€è€ƒâ€ï¼Ÿ\\n\\né€’å½’è‡ªèšåˆï¼ˆRecursive Self-Aggregationï¼Œç®€ç§°RSAï¼‰æ˜¯ä¸€ç§æµ‹è¯•æ—¶ï¼ˆtest-timeï¼‰çš„æ‰©å±•æ–¹æ³•ï¼Œå®ƒé€šè¿‡è¿­ä»£åœ°èšåˆå’Œç²¾ç‚¼æ¨ç†é“¾çš„é›†åˆï¼ˆpopulationsï¼‰ï¼Œä»¥é‡Šæ”¾æ›´æ·±å±‚æ¬¡çš„æ¨ç†èƒ½åŠ›â€”â€”ä»è€Œä½¿å°å‹æ¨¡å‹èƒ½å¤Ÿä¸å¤§å‹æ¨¡å‹ç«äº‰ã€‚\"\n}","chinese_challenges":"{\n  \"challenges\": \"æŒ‘æˆ˜ï¼š\"\n}","chinese_innovations":"{\n  \"innovations\": \"âœ¨ åˆ›æ–°ç‚¹ï¼š\\n- é€’å½’è‡ªèšåˆï¼ˆRecursive Self-Aggregation, RSAï¼‰ï¼šé€šè¿‡èšåˆå€™é€‰æ¨ç†é“¾çš„å­é›†ï¼Œè¿­ä»£åœ°ç²¾ç‚¼æ¨ç†é“¾çš„ç¾¤ä½“ï¼Œä»¥äº§ç”Ÿæ”¹è¿›çš„å€™é€‰ã€‚ \\n- åˆ©ç”¨ä¸­é—´çš„æ€ç»´é“¾å†…å®¹ï¼ˆè€Œä¸ä»…ä»…æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰è¿›è¡Œè‡ªä¸¾ï¼ˆbootstrapï¼‰ã€‚\\n- èšåˆæ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼ˆAggregation-aware reinforcement learningï¼‰æ¥è®­ç»ƒæ¨¡å‹æ›´å¥½åœ°ç»“åˆè§£å†³æ–¹æ¡ˆã€‚\"\n}","chinese_experiments":"{\n  \"experiment\": \"Qwen3-4B-Instruct-2507 using RSA achieved competitive performance with larger reasoning models (DeepSeek-R1 and o3-mini (high)) and outperformed purely parallel and purely sequential scaling across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. Specific numeric gains: Not specified in the paper.\"\n}","chinese_insights":"[\n  {\n    \"insight_type\": \"Research\",\n    \"description\": \"adapt RSA for multimodal or retrieval-augmented LLMs; develop adaptive subset-selection strategies or theory on when aggregation helps.\"\n  },\n  {\n    \"insight_type\": \"Applications\",\n    \"description\": \"stronger reasoning in education/tutoring, code synthesis, and decision supportâ€”letting smaller models punch above their size. Could RSA enable cheaper, reliable 'deep thinking' at scale?\"\n  }\n]","summary":"**Introduction:** ğŸš€ Question: Can small LLMs 'think deeper' at inference time without more training?\nRSA (Recursive Self-Aggregation) is a test-time scaling method that iteratively aggregates and refines populations of reasoning chains to unlock deeper reasoningâ€”letting smaller models compete with bigger ones.\n\n**Challenges:** ğŸ¯ Challenges:\n- Existing test-time scaling is typically either parallel (many independent tries) or seque...","analyzed_at":"2025-10-01T11:01:47.590Z","model":"openai/gpt-5-mini"}},{"id":"hf_vision_zero__scalable_vlm_self_improvement_via_strategic_gamified_self_play_1759311076551","title":"Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play","abstract":"Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in \\&quot;Who Is the Spy\\&quot;-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model's reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:25:06.144Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;name&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:115}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.8876067996025085},&quot;editors&quot;:[&quot;taesiri&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25541&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9abf&quot;,&quot;name&quot;:&quot;Qinsi Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac0&quot;,&quot;name&quot;:&quot;Bo Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac1&quot;,&quot;name&quot;:&quot;Tianyi Zhou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac2&quot;,&quot;name&quot;:&quot;Jing Shi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac3&quot;,&quot;name&quot;:&quot;Yueqian Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac4&quot;,&quot;name&quot;:&quot;Yiran Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac5&quot;,&quot;name&quot;:&quot;Hai Helen Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac6&quot;,&quot;name&quot;:&quot;Kun Wan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90f34159d1f2418f9ac7&quot;,&quot;name&quot;:&quot;Wentian Zhao&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-29T21:55:55.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:55:06.135Z&quot;,&quot;title&quot;:&quot;Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified\\n Self-Play&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Although reinforcement learning (RL) can effectively enhance the reasoning\\ncapabilities of vision-language models (VLMs), current methods remain heavily\\ndependent on labor-intensive datasets that require extensive manual\\nconstruction and verification, leading to extremely high training costs and\\nconsequently constraining the practical deployment of VLMs. To address this\\nchallenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM\\nself-improvement through competitive visual games generated from arbitrary\\nimage pairs. Specifically, Vision-Zero encompasses three main attributes: (1)\\nStrategic Self-Play Framework: Vision-Zero trains VLMs in \\&quot;Who Is the\\nSpy\\&quot;-style games, where the models engage in strategic reasoning and actions\\nacross multiple roles. Through interactive gameplay, models autonomously\\ngenerate their training data without human annotation. (2) Gameplay from\\nArbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate\\ngames from arbitrary images, thereby enhancing the model's reasoning ability\\nacross diverse domains and showing strong generalization to different tasks. We\\ndemonstrate this versatility using three distinct types of image datasets:\\nCLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable\\nPerformance Gain: We introduce Iterative Self-Play Policy Optimization\\n(Iterative-SPO), a novel training algorithm that alternates between Self-Play\\nand reinforcement learning with verifiable rewards (RLVR), mitigating the\\nperformance plateau often seen in self-play-only training and achieving\\nsustained long-term improvements. Despite using label-free data, Vision-Zero\\nachieves state-of-the-art performance on reasoning, chart question answering,\\nand vision-centric understanding tasks, surpassing other annotation-based\\nmethods. Models and code has been released at\\nhttps://github.com/wangqinsi1/Vision-Zero.&quot;,&quot;upvotes&quot;:57,&quot;discussionId&quot;:&quot;68dc90f34159d1f2418f9ac8&quot;,&quot;githubRepo&quot;:&quot;https://github.com/wangqinsi1/Vision-Zero&quot;,&quot;ai_summary&quot;:&quot;Vision-Zero is a domain-agnostic framework that enhances vision-language models through self-improvement in competitive visual games, using Iterative Self-Play Policy Optimization and achieving state-of-the-art performance without human annotation.&quot;,&quot;ai_keywords&quot;:[&quot;reinforcement learning&quot;,&quot;vision-language models&quot;,&quot;strategic self-play framework&quot;,&quot;self-play&quot;,&quot;reinforcement learning with verifiable rewards&quot;,&quot;Iterative Self-Play Policy Optimization&quot;],&quot;githubStars&quot;:6},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66128e8b7e0e7a64652dbbdf&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b72ea5b14b55ff3af920c06b69a60b3f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wang&quot;,&quot;user&quot;:&quot;Qinsi1&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;635e3a76106f984574c36409&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1667120725800-635e3a76106f984574c36409.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Bo Liu&quot;,&quot;user&quot;:&quot;Benjamin-eecs&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66443629b23fe8d3f7f2d0c7&quot;,&quot;avatarUrl&quot;:&quot;/avatars/98ff088036aa382f33a05c232604c565.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wentian Zhao&quot;,&quot;user&quot;:&quot;zwt123home123&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66274e02348a5304435dc9cc&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bda87559cd497c310597c2fc8430b31f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kun Wan&quot;,&quot;user&quot;:&quot;timecuriosity&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68da09e4d8b96845c2091d16&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bcfc23b8f15ba09554765ca9fd78ee24.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Qiuyuan Song&quot;,&quot;user&quot;:&quot;qsongshop&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68db3ff5153d1470c35ed4fb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/11cd11b2ac430ade73e13567de387065.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Mingyu Jin&quot;,&quot;user&quot;:&quot;JimmyNLP&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6670b3b78eac2e222ebf77d4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0f1d231eac479ca78ddf106a72490faa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Guofeng Cui&quot;,&quot;user&quot;:&quot;gfcui&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6245285af59b8d262df3321b&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6245285af59b8d262df3321b/dvy__dTf-miJ60IbveDg4.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yifan Zeng&quot;,&quot;user&quot;:&quot;yokey&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;647fa485d0cd8be13e662973&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/647fa485d0cd8be13e662973/_QLgqLWhWir8_bkDCdTx6.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Qingyun Wu&quot;,&quot;user&quot;:&quot;qingyun-wu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;647f5af5b0e96764589f3b2a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tianyi Zhou&quot;,&quot;user&quot;:&quot;zhoutianyi&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64d1e9211139ff0887b536a1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/638aff8b38df98aeffa10f41a67b39ea.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Alan Zhang&quot;,&quot;user&quot;:&quot;AlanStarkZ&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:1}\"> Papers arxiv:2509.25541","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:16.551Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.25541","pdf_url":"","scraped_at":"2025-10-01T09:31:16.551Z","abstract_quality":6,"analysis":{"introduction":"ğŸš€ Question: Can a VLM improve itself without expensive labels? Vision-Zero introduces a domain-agnostic self-play framework where VLMs compete in â€œWho Is the Spyâ€ style visual games on arbitrary image pairs to auto-generate training data and boost reasoning.","challenges":"ğŸ¯ Key problems tackled:\n- Heavy reliance on labor-intensive, manually annotated datasets.\n- Gamified/self-play methods limited to specific image domains, hurting generalization.\n- Self-play-only training often plateaus and stops improving model performance.","innovations":"âœ¨ Core contributions:\n- Strategic self-play: train VLMs via multi-role \\","experiments":"ğŸ“Š Not specified in the paper for exact quantitative numbers. \nHowever, experiments (CLEVR-style scenes, charts, and real-world images) show Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understandingâ€”surpassing annotation-based methods. Code/models released on GitHub.","insights":"ğŸ¤” Future directions & implications:\n- Extend to interactive multimodal agents or incorporate human-in-the-loop verification to refine self-play data.\n- Apply to domains needing low-label regimes (e.g., scientific charts, robotics perception) and study robustness to adversarial visuals.\nCould this enable continuous, label-free VLM updates in deployed systems?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"chinese_translation\": \"ğŸš€ é—®é¢˜ï¼šè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰èƒ½å¦åœ¨æ²¡æœ‰æ˜‚è´µæ ‡ç­¾çš„æƒ…å†µä¸‹è‡ªæˆ‘æå‡ï¼ŸVision-Zero å¼•å…¥äº†ä¸€ä¸ªä¸é¢†åŸŸæ— å…³çš„è‡ªåšå¼ˆæ¡†æ¶ï¼Œå…¶ä¸­ VLM åœ¨ä»»æ„å›¾åƒå¯¹ä¸Šå‚ä¸â€œè°æ˜¯é—´è°â€å¼çš„è§†è§‰æ¸¸æˆï¼Œä»¥è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ•°æ®å¹¶å¢å¼ºæ¨ç†èƒ½åŠ›ã€‚\"\n}","chinese_challenges":"{\n  \"challenges\": \"ğŸ¯ é‡ç‚¹è§£å†³çš„é—®é¢˜ï¼š\\n- è¿‡åº¦ä¾èµ–åŠ³åŠ¨å¯†é›†å‹çš„æ‰‹åŠ¨æ ‡æ³¨æ•°æ®é›†ã€‚\\n- æ¸¸æˆåŒ–/è‡ªåšå¼ˆæ–¹æ³•å±€é™äºç‰¹å®šçš„å›¾åƒé¢†åŸŸï¼ŒæŸå®³äº†æ³›åŒ–èƒ½åŠ›ã€‚\\n- ä»…ä¾èµ–è‡ªåšå¼ˆçš„è®­ç»ƒç»å¸¸ä¼šåœæ»ä¸å‰ï¼Œåœæ­¢æå‡æ¨¡å‹æ€§èƒ½ã€‚\"\n}","chinese_innovations":"{\n  \"translation\": \"âœ¨ æ ¸å¿ƒè´¡çŒ®ï¼š\\n- ç­–ç•¥æ€§è‡ªæˆ‘åšå¼ˆï¼ˆStrategic self-playï¼‰ï¼šé€šè¿‡å¤šè§’è‰²è®­ç»ƒVLM\"\n}","chinese_experiments":"{\n  \"translation\": \"ğŸ“Š è®ºæ–‡ä¸­æœªç»™å‡ºç¡®åˆ‡çš„å®šé‡æ•°æ®ã€‚ç„¶è€Œï¼Œå®éªŒï¼ˆåœ¨CLEVRé£æ ¼çš„åœºæ™¯ã€å›¾è¡¨å’ŒçœŸå®ä¸–ç•Œå›¾åƒä¸Šè¿›è¡Œï¼‰è¡¨æ˜ï¼ŒVision-Zeroåœ¨æ¨ç†ã€å›¾è¡¨é—®ç­”å’Œä»¥è§†è§‰ä¸ºä¸­å¿ƒçš„ç†è§£æ–¹é¢å–å¾—äº†æœ€å…ˆè¿›çš„æ€§èƒ½ï¼Œè¶…è¶Šäº†åŸºäºæ ‡æ³¨çš„æ–¹æ³•ã€‚ä»£ç å’Œæ¨¡å‹å·²åœ¨GitHubä¸Šå‘å¸ƒã€‚\"\n}","chinese_insights":"{\n  \"future_directions_and_implications\": [\n    {\n      \"key_insight\": \"Extend to interactive multimodal agents or incorporate human-in-the-loop verification to refine self-play data.\",\n      \"translation\": \"æ‰©å±•åˆ°äº¤äº’å¼å¤šæ¨¡æ€æ™ºèƒ½ä½“ï¼Œæˆ–æ•´åˆäººåœ¨ç¯ï¼ˆhuman-in-the-loopï¼‰éªŒè¯æœºåˆ¶ï¼Œä»¥ä¼˜åŒ–è‡ªåšå¼ˆæ•°æ®ã€‚\"\n    },\n    {\n      \"key_insight\": \"Apply to domains needing low-label regimes (e.g., scientific charts, robotics perception) and study robustness to adversarial visuals.\",\n      \"translation\": \"åº”ç”¨äºéœ€è¦ä½æ ‡ç­¾æ ·æœ¬çš„é¢†åŸŸï¼ˆä¾‹å¦‚ç§‘å­¦å›¾è¡¨ã€æœºå™¨äººæ„ŸçŸ¥ï¼‰ï¼Œå¹¶ç ”ç©¶å…¶å¯¹å¯¹æŠ—æ€§è§†è§‰è¾“å…¥çš„é²æ£’æ€§ã€‚\"\n    },\n    {\n      \"key_insight\": \"Could this enable continuous, label-free VLM updates in deployed systems?\",\n      \"translation\": \"è¿™æ˜¯å¦èƒ½å¤Ÿå®ç°å¯¹å·²éƒ¨ç½²ç³»ç»Ÿä¸­çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰è¿›è¡ŒæŒç»­ã€æ— æ ‡ç­¾çš„æ›´æ–°ï¼Ÿ\"\n    }\n  ]\n}","summary":"**Introduction:** ğŸš€ Question: Can a VLM improve itself without expensive labels? Vision-Zero introduces a domain-agnostic self-play framework where VLMs compete in â€œWho Is the Spyâ€ style visual games on arbitrary image pairs to auto-generate training data and boost reasoning.\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Heavy reliance on labor-intensive, manually annotated datasets.\n- Gamified/self-play methods limited to specific i...","analyzed_at":"2025-10-01T11:14:42.310Z","model":"openai/gpt-5-mini"}},{"id":"hf_truthrl__incentivizing_truthful_llms_via_reinforcement_learning_1759311079944","title":"TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning","abstract":"While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy---models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs. We also experiment with more complicated reward designs, such as knowledge-enhanced and reasoning-enhanced variants, and show that a simple ternary reward scheme generally performs better. Moreover, we find the improvement of TruthRL arises from enhancing the capability of LLMs to recognize their knowledge boundary, hence avoiding being overly conservative as the baselines are. Further analysis confirms that TruthRL is robust to hallucination-baiting questions and more confident in producing accurate responses.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:26:26.198Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;6526307af06ac0cf9a922e86&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;,&quot;fullname&quot;:&quot;Zhepei Wei&quot;,&quot;name&quot;:&quot;weizhepei&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9195460677146912},&quot;editors&quot;:[&quot;weizhepei&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ğŸ‘&quot;,&quot;users&quot;:[&quot;sytmr&quot;,&quot;TianHongZXY&quot;,&quot;WZDavid&quot;],&quot;count&quot;:3},{&quot;reaction&quot;:&quot;ğŸ”¥&quot;,&quot;users&quot;:[&quot;WZDavid&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25760&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aa7&quot;,&quot;name&quot;:&quot;Zhepei Wei&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aa8&quot;,&quot;name&quot;:&quot;Xiao Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aa9&quot;,&quot;name&quot;:&quot;Kai Sun&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aaa&quot;,&quot;name&quot;:&quot;Jiaqi Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aab&quot;,&quot;name&quot;:&quot;Rulin Shao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aac&quot;,&quot;name&quot;:&quot;Sean Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aad&quot;,&quot;name&quot;:&quot;Mohammad Kachuee&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aae&quot;,&quot;name&quot;:&quot;Teja Gollapudi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9aaf&quot;,&quot;name&quot;:&quot;Tony Liao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab0&quot;,&quot;name&quot;:&quot;Nicolas Scheffer&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab1&quot;,&quot;name&quot;:&quot;Rakesh Wanga&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab2&quot;,&quot;name&quot;:&quot;Anuj Kumar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab3&quot;,&quot;name&quot;:&quot;Yu Meng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab4&quot;,&quot;name&quot;:&quot;Wen-tau Yih&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc90b84159d1f2418f9ab5&quot;,&quot;name&quot;:&quot;Xin Luna Dong&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-30T04:25:17.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:56:26.188Z&quot;,&quot;title&quot;:&quot;TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6526307af06ac0cf9a922e86&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhepei Wei&quot;,&quot;user&quot;:&quot;weizhepei&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;While large language models (LLMs) have demonstrated strong performance on\\nfactoid question answering, they are still prone to hallucination and\\nuntruthful responses, particularly when tasks demand information outside their\\nparametric knowledge. Indeed, truthfulness requires more than accuracy --\\nmodels must also recognize uncertainty and abstain when unsure to avoid\\nhallucinations. This presents a fundamental challenge for existing methods:\\napproaches that optimize for accuracy often amplify hallucinations, while those\\nthat encourage abstention can become overly conservative, sacrificing correct\\nanswers. Both extremes ultimately compromise truthfulness. In this work, we\\npresent TruthRL, a general reinforcement learning (RL) framework that directly\\noptimizes the truthfulness of LLMs. Specifically, we implement TruthRL using\\nGRPO with a simple yet effective ternary reward that distinguishes correct\\nanswers, hallucinations, and abstentions. It incentivizes models to reduce\\nhallucinations not only by providing correct responses, but also by enabling\\nabstention when uncertain, thereby improving truthfulness. Extensive\\nexperiments across four knowledge-intensive benchmarks show that, compared to\\nvanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves\\ntruthfulness by 21.1%, with consistent gains across various backbone models\\n(e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth\\nablation study demonstrates that vanilla accuracy-driven methods, such as\\nsupervised fine-tuning or RL with a binary reward, struggle to balance factual\\ncorrectness and uncertainty. In contrast, our proposed truthfulness-driven\\nTruthRL achieves strong performance in both accuracy and truthfulness,\\nunderscoring the importance of learning objective design for developing\\ntruthful LLMs.&quot;,&quot;upvotes&quot;:34,&quot;discussionId&quot;:&quot;68dc90b84159d1f2418f9ab6&quot;,&quot;ai_summary&quot;:&quot;TruthRL, a reinforcement learning framework, enhances the truthfulness of large language models by balancing accuracy and abstention, significantly reducing hallucinations and improving performance across benchmarks.&quot;,&quot;ai_keywords&quot;:[&quot;large language models&quot;,&quot;LLMs&quot;,&quot;hallucination&quot;,&quot;untruthful responses&quot;,&quot;parametric knowledge&quot;,&quot;truthfulness&quot;,&quot;reinforcement learning&quot;,&quot;RL&quot;,&quot;GRPO&quot;,&quot;ternary reward&quot;,&quot;abstention&quot;,&quot;accuracy-driven methods&quot;,&quot;supervised fine-tuning&quot;,&quot;binary reward&quot;,&quot;knowledge-intensive benchmarks&quot;,&quot;Qwen&quot;,&quot;Llama&quot;,&quot;retrieval&quot;,&quot;non-retrieval setups&quot;,&quot;ablation study&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;6526307af06ac0cf9a922e86&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/nchCipX-XWw2cnzYsU_Cv.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhepei Wei&quot;,&quot;user&quot;:&quot;weizhepei&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;62ea79dd01ed9b0e8f61ccd3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chengsong Huang&quot;,&quot;user&quot;:&quot;ChengsongHuang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;617aec6f6f37340367d5d7a1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/afa58f39896c5caef512675450c7d6ce.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yu Meng&quot;,&quot;user&quot;:&quot;yumeng5&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6602787b1827b6d37ee527be&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f69c1779b4347a042dad1a0d962145af.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tzu-Han Lin&quot;,&quot;user&quot;:&quot;hank0316&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6587e5a4b2177de3967ff434&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f2dfbc44eb2bff8d8d66d26db8539708.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shuyao Xu&quot;,&quot;user&quot;:&quot;Tim-Xu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66080ddac201aee890e5efeb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/dae8f44b2eda9d1950efaa10a5aa986f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jing Chen&quot;,&quot;user&quot;:&quot;jingchen6688&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;623b290048f658f28aef79f7&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1648044277149-noauth.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xinyu Zhu&quot;,&quot;user&quot;:&quot;TianHongZXY&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6633f39185b05e9a8e7c549c&quot;,&quot;avatarUrl&quot;:&quot;/avatars/ee4df68daee8b6637d7ad86cba29cc2f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;shiyu&quot;,&quot;user&quot;:&quot;sytmr&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;664e88ad8ab2524c036c3d2f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/HItpTu75SFqA5ouOMKzVb.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhining Liu&quot;,&quot;user&quot;:&quot;ZhiningLiu1998&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;619e6657cc04eadf54fa5d2d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c78c54767c63c75a9f6783ffa78a98fa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wei-Lin Chen&quot;,&quot;user&quot;:&quot;wlchen&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6590a65b89f1ff0463828e53&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4ab424eede2fe9c114252b1e5dd1ba25.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;sizhe&quot;,&quot;user&quot;:&quot;sizhe04&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;647a248bed75e95d3e98e3d6&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e10e2f8516d1451fd85e17b5a0ba978d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yaochen Zhu&quot;,&quot;user&quot;:&quot;yaochenzhu&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:2,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;5e63d8713071d5be688861b8&quot;,&quot;name&quot;:&quot;facebook&quot;,&quot;fullname&quot;:&quot;AI at Meta&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png&quot;}}\"> Papers arxiv:2509.25760","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:19.945Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.25760","pdf_url":"","scraped_at":"2025-10-01T09:31:19.945Z","abstract_quality":6,"analysis":{"introduction":"ğŸš€ Question: How do we make LLMs say â€œI donâ€™t knowâ€ instead of confidently hallucinating?\nTruthRL is an RL framework (GRPO + a simple ternary reward) that trains models to answer correctly or abstain, cutting hallucinations and improving truthfulness across backbones & setups.","challenges":"ğŸ¯ Key problems tackled:\n- Models hallucinate on facts outside parametric knowledge.\n- Accuracy-driven training amplifies confident but wrong answers.\n- Abstention-focused methods become overly conservative and lose correct answers.","innovations":"âœ¨ Core innovations:\n- TruthRL: an RL framework optimizing truthfulness directly.\n- Implemented with GRPO and a ternary reward (correct / hallucination / abstain).\n- Novelty: reward explicitly balances giving correct answers and abstaining when uncertain.","experiments":"ğŸ“Š Results: TruthRL reduced hallucinations by 28.9% and improved truthfulness by 21.1% across four knowledge-intensive benchmarks, with consistent gains across backbones (Qwen, Llama) and both retrieval & non-retrieval setups â€” showing the ternary rewardâ€™s practical impact.","insights":"ğŸ¤” Whatâ€™s next?\n- Explore combining TruthRL with stronger calibrated uncertainty estimators or multi-turn dialogue to improve abstention in interactive settings.\n- Broader applications: high-stakes domains (medicine, law) and retrieval-augmented systems where avoiding hallucination matters most.\nCould TruthRL become a standard for safe QA?","category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"\"ğŸš€ é—®é¢˜ï¼šæˆ‘ä»¬å¦‚ä½•è®©å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¯´å‡ºâ€œæˆ‘ä¸çŸ¥é“â€ï¼Œè€Œä¸æ˜¯è‡ªä¿¡åœ°ç»™å‡ºå¹»è§‰å†…å®¹ï¼ŸTruthRLæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ï¼ˆç”±GRPOå’Œä¸€ä¸ªç®€å•çš„ä¸‰å…ƒå¥–åŠ±æœºåˆ¶æ„æˆï¼‰ï¼Œå®ƒè®­ç»ƒæ¨¡å‹è¦ä¹ˆæ­£ç¡®å›ç­”ï¼Œè¦ä¹ˆé€‰æ‹©å¼ƒæƒï¼Œä»è€Œå‡å°‘å¹»è§‰ç°è±¡ï¼Œå¹¶åœ¨å„ç§åŸºç¡€æ¨¡å‹å’Œé…ç½®ä¸­æé«˜çœŸå®æ€§ã€‚\"","chinese_challenges":"{\n  \"å…³é”®æŒ‘æˆ˜\": [\n    \"æ¨¡å‹å¯¹è¶…å‡ºå…¶å‚æ•°åŒ–çŸ¥è¯†èŒƒå›´çš„äº‹å®äº§ç”Ÿâ€œå¹»è§‰â€ï¼ˆè™šå‡ä¿¡æ¯ï¼‰ã€‚\",\n    \"ä»¥å‡†ç¡®ç‡ï¼ˆç²¾åº¦ï¼‰ä¸ºå¯¼å‘çš„è®­ç»ƒä¼šåŠ å‰§æ¨¡å‹å¯¹é”™è¯¯ç­”æ¡ˆçš„è¿‡åº¦è‡ªä¿¡ã€‚\",\n    \"ä»¥å¼ƒæƒï¼ˆæ‹’ç»å›ç­”ï¼‰ä¸ºæ ¸å¿ƒçš„æ–¹æ³•å¾€å¾€è¿‡äºä¿å®ˆï¼Œå¯¼è‡´é”™å¤±æ­£ç¡®çš„ç­”æ¡ˆã€‚\"\n  ]\n}","chinese_innovations":"\"æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š\\n- TruthRLï¼šä¸€ç§ç›´æ¥ä¼˜åŒ–â€œçœŸå®æ€§â€ï¼ˆtruthfulnessï¼‰çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ã€‚\\n- ä½¿ç”¨GRPOç®—æ³•å’Œä¸‰å…ƒå¥–åŠ±æœºåˆ¶ï¼ˆæ­£ç¡® / å¹»è§‰ / å¼ƒæƒï¼‰å®ç°ã€‚\\n- åˆ›æ–°ç‚¹ï¼šå¥–åŠ±æœºåˆ¶æ˜ç¡®åœ°å¹³è¡¡äº†â€œç»™å‡ºæ­£ç¡®ç­”æ¡ˆâ€å’Œâ€œåœ¨ä¸ç¡®å®šæ—¶é€‰æ‹©å¼ƒæƒâ€è¿™ä¸¤ç§è¡Œä¸ºã€‚\"","chinese_experiments":"{\n  \"analysis_type\": \"research_summary\",\n  \"model_name\": \"TruthRL\",\n  \"metrics\": [\n    {\n      \"metric\": \"hallucinations_reduction\",\n      \"value\": 0.289,\n      \"unit\": \"percentage\"\n    },\n    {\n      \"metric\": \"truthfulness_improvement\",\n      \"value\": 0.211,\n      \"unit\": \"percentage\"\n    }\n  ],\n  \"scope\": {\n    \"benchmarks\": \"four knowledge-intensive benchmarks\",\n    \"backbones\": [\n      \"Qwen\",\n      \"Llama\"\n    ],\n    \"setups\": [\n      \"retrieval\",\n      \"non-retrieval\"\n    ]\n  },\n  \"conclusion\": \"The ternary reward mechanism demonstrated practical impact through consistent gains across diverse configurations.\"\n}","chinese_insights":"{\n  \"insights\": [\n    \"ğŸ¤” ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ\",\n    \"æ¢ç´¢å°†TruthRLä¸æ›´å¼ºçš„æ ¡å‡†ä¸ç¡®å®šæ€§ä¼°è®¡å™¨æˆ–å¤šè½®å¯¹è¯ç›¸ç»“åˆï¼Œä»¥æ”¹è¿›åœ¨äº¤äº’å¼è®¾ç½®ä¸­çš„æ‹’ç»å›ç­”ï¼ˆAbstentionï¼‰èƒ½åŠ›ã€‚\",\n    \"æ›´å¹¿æ³›çš„åº”ç”¨ï¼šåœ¨é«˜é£é™©é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ã€æ³•å¾‹ï¼‰ä»¥åŠé¿å…â€œå¹»è§‰â€ï¼ˆHallucinationï¼‰è‡³å…³é‡è¦çš„æ£€ç´¢å¢å¼ºç³»ç»Ÿä¸­ã€‚\",\n    \"TruthRLèƒ½å¦æˆä¸ºå®‰å…¨é—®ç­”ï¼ˆQAï¼‰çš„æ ‡å‡†ï¼Ÿ\"\n  ]\n}","summary":"**Introduction:** ğŸš€ Question: How do we make LLMs say â€œI donâ€™t knowâ€ instead of confidently hallucinating?\nTruthRL is an RL framework (GRPO + a simple ternary reward) that trains models to answer correctly or abstain, cutting hallucinations and improving truthfulness across backbones & setups.\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Models hallucinate on facts outside parametric knowledge.\n- Accuracy-driven training amplifies c...","analyzed_at":"2025-10-01T11:16:31.375Z","model":"openai/gpt-5-mini"}},{"id":"hf_oceangym__a_benchmark_environment_for_underwater_embodied_agents_1759311082964","title":"OceanGym: A Benchmark Environment for Underwater Embodied Agents","abstract":"We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:41:27.213Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;620b3bbb0668e435407c8d0a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;,&quot;fullname&quot;:&quot;Ningyu Zhang&quot;,&quot;name&quot;:&quot;Ningyu&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:30}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.930759847164154},&quot;editors&quot;:[&quot;Ningyu&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.26536&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af3&quot;,&quot;name&quot;:&quot;Yida Xue&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af4&quot;,&quot;name&quot;:&quot;Mingjun Mao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af5&quot;,&quot;name&quot;:&quot;Xiangyuan Ru&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af6&quot;,&quot;name&quot;:&quot;Yuqi Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af7&quot;,&quot;name&quot;:&quot;Baochang Ren&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af8&quot;,&quot;name&quot;:&quot;Shuofei Qiao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9af9&quot;,&quot;name&quot;:&quot;Mengru Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afa&quot;,&quot;name&quot;:&quot;Shumin Deng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afb&quot;,&quot;name&quot;:&quot;Xinyu An&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afc&quot;,&quot;name&quot;:&quot;Ningyu Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afd&quot;,&quot;name&quot;:&quot;Ying Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc947d4159d1f2418f9afe&quot;,&quot;name&quot;:&quot;Huajun Chen&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/620b3bbb0668e435407c8d0a/1Nye1TFWd66krDhvPXjQ1.png&quot;],&quot;publishedAt&quot;:&quot;2025-09-30T17:09:32.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T01:11:27.179Z&quot;,&quot;title&quot;:&quot;OceanGym: A Benchmark Environment for Underwater Embodied Agents&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;620b3bbb0668e435407c8d0a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Ningyu Zhang&quot;,&quot;user&quot;:&quot;Ningyu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We introduce OceanGym, the first comprehensive benchmark for ocean underwater\\nembodied agents, designed to advance AI in one of the most demanding real-world\\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\\nextreme perceptual and decision-making challenges, including low visibility,\\ndynamic ocean currents, making effective agent deployment exceptionally\\ndifficult. OceanGym encompasses eight realistic task domains and a unified\\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\\nintegrates perception, memory, and sequential decision-making. Agents are\\nrequired to comprehend optical and sonar data, autonomously explore complex\\nenvironments, and accomplish long-horizon objectives under these harsh\\nconditions. Extensive experiments reveal substantial gaps between\\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\\npersistent difficulty of perception, planning, and adaptability in ocean\\nunderwater environments. By providing a high-fidelity, rigorously designed\\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\\ntransferring these capabilities to real-world autonomous ocean underwater\\nvehicles, marking a decisive step toward intelligent agents capable of\\noperating in one of Earth's last unexplored frontiers. The code and data are\\navailable at https://github.com/OceanGPT/OceanGym.&quot;,&quot;upvotes&quot;:25,&quot;discussionId&quot;:&quot;68dc947d4159d1f2418f9aff&quot;,&quot;projectPage&quot;:&quot;https://oceangpt.github.io/OceanGym/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/OceanGPT/OceanGym&quot;,&quot;ai_summary&quot;:&quot;OceanGym is a benchmark for underwater embodied agents using Multi-modal Large Language Models to address challenges in perception, planning, and adaptability in harsh ocean environments.&quot;,&quot;ai_keywords&quot;:[&quot;Multi-modal Large Language Models&quot;,&quot;MLLMs&quot;,&quot;optical data&quot;,&quot;sonar data&quot;,&quot;sequential decision-making&quot;,&quot;embodied AI&quot;,&quot;autonomous ocean underwater vehicles&quot;],&quot;githubStars&quot;:25},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;620b3bbb0668e435407c8d0a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0fccbb2577d76088e09f054c35cffbc.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Ningyu Zhang&quot;,&quot;user&quot;:&quot;Ningyu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67f371b807a3da4558f803c1&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GilM_Vf65qvwuW5-uj9aG.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Twain Wu&quot;,&quot;user&quot;:&quot;1wtw1&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64db65ee1d19239f50674cbe&quot;,&quot;avatarUrl&quot;:&quot;/avatars/dd96cec8d0c10f52a89b25d65728738d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;xueyida&quot;,&quot;user&quot;:&quot;xyd123&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68d8fc00ff474874c83a1c99&quot;,&quot;avatarUrl&quot;:&quot;/avatars/17e3a2f5197274536bf68d949c5416db.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;huminclu&quot;,&quot;user&quot;:&quot;huminclu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;658ead753ce574ff3c339a64&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1dd7671e8af5e7241ef47a6de5503c53.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;sunnychenxiwang&quot;,&quot;user&quot;:&quot;sunnychenxiwang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6698c1c3157ceb76c48ff996&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2f1d732c4d9df4f5b554268ee1949dda.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;å¾æ­¥å¼º&quot;,&quot;user&quot;:&quot;Xubqpanda&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6736aebbbbc5d5471ee57218&quot;,&quot;avatarUrl&quot;:&quot;/avatars/45e3b017fc6a07e10a42b81cfa349b3f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yi Zhong&quot;,&quot;user&quot;:&quot;HongdouNI233&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;646449beeca41ed5029d1630&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7992d9a62e8d218ec3200d74af9ab5c4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xiang Chen&quot;,&quot;user&quot;:&quot;yyfenglin&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;684bc1be17ae31ba66171292&quot;,&quot;avatarUrl&quot;:&quot;/avatars/99ea28d4ed2ef6c4e35fd26c64472e49.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jingsheng Zheng&quot;,&quot;user&quot;:&quot;JohnsonZheng03&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68415d7b911d1b3135fcca88&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HE3ptFNTlvoWmG3p3f2Cs.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Qihailiantang&quot;,&quot;user&quot;:&quot;Qihailiantang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;679a01a99893a68681ef1847&quot;,&quot;avatarUrl&quot;:&quot;/avatars/17fe173acda467df2b90cca9e5f3c656.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;ye&quot;,&quot;user&quot;:&quot;haohaojun&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65cad52fd6c974694fc20b8e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/8232a7c5db590ed26751a47c45d481b8.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xinle Deng&quot;,&quot;user&quot;:&quot;Linear-Matrix-Probability&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:3,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6345aadf5efccdc07f1365a5&quot;,&quot;name&quot;:&quot;ZhejiangUniversity&quot;,&quot;fullname&quot;:&quot;Zhejiang University&quot;}}\"> Papers arxiv:2509.26536","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:22.964Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.26536","pdf_url":"","scraped_at":"2025-10-01T09:31:22.964Z","abstract_quality":6,"analysis":{"introduction":"ğŸš€ What if autonomous agents could explore the deep sea like rovers explore Mars?\nOceanGym is the first comprehensive benchmark for underwater embodied agents â€” 8 realistic task domains + a unified MLLM-driven agent framework. Designed to push robust AI for AUVs. https://github.com/OceanGPT/OceanGym","challenges":"ğŸ¯ Key challenges OceanGym tackles:\n- Low visibility & noisy sensors that hinder perception\n- Dynamic ocean currents complicating decision-making and control\n- Multimodal (optical + sonar) longâ€‘horizon tasks requiring memory and planning","innovations":"âœ¨ What OceanGym introduces:\n- A highâ€‘fidelity benchmark with 8 realistic underwater task domains\n- A unified, MLLM-driven agent framework integrating perception, memory, sequential decision-making\n- Tasks demanding optical+sonar fusion and longâ€‘horizon autonomous exploration\nNovelty: first comprehensive underwater embodied benchmark.","experiments":"ğŸ“Š Quantitative result: Not specified in the paper.\nQualitative proof: Extensive experiments reveal substantial gaps between stateâ€‘ofâ€‘theâ€‘art MLLM-driven agents and human experts, showing persistent difficulty in perception, planning, and adaptability. Code/data: github.com/OceanGPT/OceanGym","insights":"ğŸ¤” What's next?\n- Improve simâ€‘toâ€‘real transfer and dedicated sonarâ€“vision fusion models\n- Explore curriculum/hierarchical planning for longâ€‘horizon underwater tasks\nApplications: autonomous oceanographic surveys, search & rescue, infrastructure inspection. Could OceanGym speed real-world AUV deployment?","keywords":["OceanGym","Multi-modal Large Language Models","MLLMs","underwater robotics","sonar","optical imaging","embodied AI","long-horizon planning","benchmark"],"category":"robotics","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"translation\": \"ğŸš€ å¦‚æœè‡ªä¸»æ™ºèƒ½ä½“èƒ½åƒæ¼«æ¸¸è½¦æ¢ç´¢ç«æ˜Ÿä¸€æ ·æ¢ç´¢æ·±æµ·ï¼Œé‚£ä¼šæ€æ ·ï¼ŸOceanGymæ˜¯é¦–ä¸ªé’ˆå¯¹æ°´ä¸‹å…·èº«æ™ºèƒ½ä½“çš„ç»¼åˆåŸºå‡†â€”â€”å®ƒåŒ…å«8ä¸ªç°å®ä»»åŠ¡é¢†åŸŸå’Œä¸€ä¸ªç»Ÿä¸€çš„MLLMé©±åŠ¨çš„æ™ºèƒ½ä½“æ¡†æ¶ã€‚æ—¨åœ¨æ¨åŠ¨è‡ªä¸»æ°´ä¸‹èˆªè¡Œå™¨ï¼ˆAUVsï¼‰çš„é²æ£’ï¼ˆç¨³å¥ï¼‰AIå‘å±•ã€‚https://github.com/OceanGPT/OceanGym\"\n}","chinese_challenges":"\"OceanGym åº”å¯¹çš„å…³é”®æŒ‘æˆ˜ï¼š\\n- ä½èƒ½è§åº¦ä¸ä¼ æ„Ÿå™¨å™ªå£°ï¼Œä¸¥é‡é˜»ç¢äº†ç¯å¢ƒæ„ŸçŸ¥ã€‚\\n- åŠ¨æ€æ´‹æµä½¿å†³ç­–å’Œæ§åˆ¶è¿‡ç¨‹å˜å¾—å¤æ‚ã€‚\\n- éœ€è¦è®°å¿†å’Œè§„åˆ’çš„å¤šæ¨¡æ€ï¼ˆå…‰å­¦ + å£°å‘ï¼‰é•¿ç¨‹ä»»åŠ¡ã€‚\"","chinese_innovations":"{\n  \"innovations\": [\n    {\n      \"title\": \"OceanGym å¼•å…¥äº†ä»€ä¹ˆï¼š\",\n      \"points\": [\n        \"ä¸€ä¸ªé«˜ä¿çœŸåŸºå‡†ï¼ŒåŒ…å« 8 ä¸ªé€¼çœŸçš„æ°´ä¸‹ä»»åŠ¡é¢†åŸŸ\",\n        \"ä¸€ä¸ªç»Ÿä¸€çš„ã€ç”± MLLMï¼ˆå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼‰é©±åŠ¨çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œé›†æˆäº†æ„ŸçŸ¥ã€è®°å¿†å’Œåºåˆ—å†³ç­–èƒ½åŠ›\",\n        \"è¦æ±‚å…‰å­¦ä¸å£°çº³èåˆä»¥åŠé•¿å‘¨æœŸè‡ªä¸»æ¢ç´¢çš„ä»»åŠ¡\"\n      ],\n      \"novelty\": \"æ–°é¢–æ€§ï¼šé¦–ä¸ªå…¨é¢çš„æ°´ä¸‹å…·èº«åŸºå‡†ã€‚\"\n    }\n  ]\n}","chinese_experiments":"{\n  \"å®šé‡ç»“æœ\": \"è®ºæ–‡ä¸­æœªæ˜ç¡®è¯´æ˜ã€‚\",\n  \"å®šæ€§è¯æ®\": \"å¹¿æ³›çš„å®éªŒæ­ç¤ºäº†æœ€å…ˆè¿›çš„ MLLM é©±åŠ¨æ™ºèƒ½ä½“ä¸äººç±»ä¸“å®¶ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œè¡¨æ˜å…¶åœ¨æ„ŸçŸ¥ã€è§„åˆ’å’Œé€‚åº”æ€§æ–¹é¢ä»é¢ä¸´æŒç»­çš„å›°éš¾ã€‚\",\n  \"ä»£ç /æ•°æ®\": \"github.com/OceanGPT/OceanGym\"\n}","chinese_insights":"{\n  \"insights\": {\n    \"next_steps\": [\n      \"Improve sim-to-real transfer and dedicated sonarâ€“vision fusion models\",\n      \"Explore curriculum/hierarchical planning for long-horizon underwater tasks\"\n    ],\n    \"applications\": [\n      \"autonomous oceanographic surveys\",\n      \"search & rescue\",\n      \"infrastructure inspection\"\n    ],\n    \"question\": \"Could OceanGym speed real-world AUV deployment?\"\n  }\n}","summary":"**Introduction:** ğŸš€ What if autonomous agents could explore the deep sea like rovers explore Mars?\nOceanGym is the first comprehensive benchmark for underwater embodied agents â€” 8 realistic task domains + a unified MLLM-driven agent framework. Designed to push robust AI for AUVs. https://github.com/OceanGPT/OceanGym\n\n**Challenges:** ğŸ¯ Key challenges OceanGym tackles:\n- Low visibility & noisy sensors that hinder perception\n- Dynami...","analyzed_at":"2025-10-01T11:19:11.261Z","model":"openai/gpt-5-mini"}},{"id":"hf_mcpmark__a_benchmark_for_stress_testing_realistic_and_comprehensive_mcp_use_1759311085683","title":"MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use","abstract":"30 models through 127 CRUD-heavy tasks across 5 MCP servers, with a minimal but general MCPMark-Agent ensuring fair comparison.\\r\\nResults: even the best models cap at 52.56% pass@1 / 33.86% pass^4, while other strong systems like claude-sonnet-4 and o3 stay under 30% pass@1.\\r\\nWe break down why â€” from implicit errors and context drift to cost-performance tradeoffs.\\r\\n\\r\\nğŸ‘‰ Paper: https://arxiv.org/pdf/2509.24002\\r\\nğŸ‘‰ Website: https://mcpmark.ai/\\r\\nğŸ‘‰ Code: https://github.com/eval-sys/mcpmark&quot;,&quot;html&quot;:&quot;Agents can call tools â€” but can they actually deliver?MCPMark stress-tested &amp;gt;30 models through 127 CRUD-heavy tasks across 5 MCP servers, with a minimal but general MCPMark-Agent ensuring fair comparison.Results: even the best models cap at 52.56% pass@1 / 33.86% pass^4, while other strong systems like claude-sonnet-4 and o3 stay under 30% pass@1.We break down why â€” from implicit errors and context drift to cost-performance tradeoffs.\\nğŸ‘‰ Paper: https://arxiv.org/pdf/2509.24002ğŸ‘‰ Website: https://mcpmark.ai/ğŸ‘‰ Code: https://github.com/eval-sys/mcpmark\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T03:28:54.371Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;626d268d5f7327906f05cad1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/18bda74612a3ee63a17f991bcc695106.svg&quot;,&quot;fullname&quot;:&quot;Zijian Wu&quot;,&quot;name&quot;:&quot;Jakumetsu&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:4}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.79066002368927},&quot;editors&quot;:[&quot;Jakumetsu&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/18bda74612a3ee63a17f991bcc695106.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.24002&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b97&quot;,&quot;name&quot;:&quot;Zijian Wu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b98&quot;,&quot;name&quot;:&quot;Xiangyan Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b99&quot;,&quot;name&quot;:&quot;Xinyuan Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b9a&quot;,&quot;name&quot;:&quot;Lingjun Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b9b&quot;,&quot;name&quot;:&quot;Fanqing Meng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b9c&quot;,&quot;name&quot;:&quot;Lingxiao Du&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b9d&quot;,&quot;name&quot;:&quot;Yiran Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b9e&quot;,&quot;name&quot;:&quot;Fanshi Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9b9f&quot;,&quot;name&quot;:&quot;Yaoqi Ye&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9ba0&quot;,&quot;name&quot;:&quot;Jiawei Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9ba1&quot;,&quot;name&quot;:&quot;Zirui Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9ba2&quot;,&quot;name&quot;:&quot;Jinjie Ni&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9ba3&quot;,&quot;name&quot;:&quot;Yufan Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9ba4&quot;,&quot;name&quot;:&quot;Arvin Xu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc9eff4159d1f2418f9ba5&quot;,&quot;name&quot;:&quot;Michael Qizhe Shieh&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-28T17:53:27.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T01:58:54.337Z&quot;,&quot;title&quot;:&quot;MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP\\n Use&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;626d268d5f7327906f05cad1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/18bda74612a3ee63a17f991bcc695106.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Zijian Wu&quot;,&quot;user&quot;:&quot;Jakumetsu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;MCP standardizes how LLMs interact with external systems, forming the\\nfoundation for general agents. However, existing MCP benchmarks remain narrow\\nin scope: they focus on read-heavy tasks or tasks with limited interaction\\ndepth, and fail to capture the complexity and realism of real-world workflows.\\nTo address this gap, we propose MCPMark, a benchmark designed to evaluate MCP\\nuse in a more realistic and comprehensive manner. It consists of 127\\nhigh-quality tasks collaboratively created by domain experts and AI agents.\\nEach task begins with a curated initial state and includes a programmatic\\nscript for automatic verification. These tasks demand richer and more diverse\\ninteractions with the environment, involving a broad range of create, read,\\nupdate, and delete (CRUD) operations. We conduct a comprehensive evaluation of\\ncutting-edge LLMs using a minimal agent framework that operates in a\\ntool-calling loop. Empirical results show that the best-performing model,\\ngpt-5-medium, reaches only 52.56\\\\% pass@1 and 33.86\\\\% pass^4, while other\\nwidely regarded strong models, including claude-sonnet-4 and o3, fall below\\n30\\\\% pass@1 and 15\\\\% pass^4. On average, LLMs require 16.2 execution\\nturns and 17.4 tool calls per task, significantly surpassing those in\\nprevious MCP benchmarks and highlighting the stress-testing nature of MCPMark.&quot;,&quot;upvotes&quot;:25,&quot;discussionId&quot;:&quot;68dc9eff4159d1f2418f9ba6&quot;,&quot;projectPage&quot;:&quot;https://mcpmark.ai/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/eval-sys/mcpmark&quot;,&quot;ai_summary&quot;:&quot;MCPMark is a comprehensive benchmark for evaluating MCP use in real-world workflows, featuring diverse tasks that require richer interactions with the environment, and reveals that current LLMs perform poorly on these tasks.&quot;,&quot;ai_keywords&quot;:[&quot;MCP&quot;,&quot;LLMs&quot;,&quot;general agents&quot;,&quot;MCP benchmarks&quot;,&quot;MCPMark&quot;,&quot;high-quality tasks&quot;,&quot;domain experts&quot;,&quot;AI agents&quot;,&quot;initial state&quot;,&quot;programmatic script&quot;,&quot;automatic verification&quot;,&quot;CRUD operations&quot;,&quot;minimal agent framework&quot;,&quot;tool-calling loop&quot;,&quot;gpt-5-medium&quot;,&quot;claude-sonnet-4&quot;,&quot;o3&quot;,&quot;pass@1&quot;,&quot;pass^4&quot;,&quot;execution turns&quot;,&quot;tool calls&quot;],&quot;githubStars&quot;:175},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;626d268d5f7327906f05cad1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/18bda74612a3ee63a17f991bcc695106.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Zijian Wu&quot;,&quot;user&quot;:&quot;Jakumetsu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68ac2314d4a7fd30a2ec0035&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2b927fedd6cc376807a771e37124e331.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhiwei Xue&quot;,&quot;user&quot;:&quot;ZackAXue&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6400c9a1210dabb7d9301192&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f586047b543814dd6301c05d0bbd72ae.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tianyi Wu&quot;,&quot;user&quot;:&quot;awsuineg&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;666fe1a5b07525f0bde69c27&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bb98ab0b974c8fe011739baa8dadd91a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Lingxiao Du&quot;,&quot;user&quot;:&quot;Cierra0506&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6486b09e8315b19342f0bf5e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bc5f22f231c884146d373fe1042d81bd.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xiangyan Liu&quot;,&quot;user&quot;:&quot;xyliu6&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6650c77a74664a42ddfb9187&quot;,&quot;avatarUrl&quot;:&quot;/avatars/92001bbe0ae9b14309730316b639cede.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;yueliu1999&quot;,&quot;user&quot;:&quot;yueliu1999&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64060b49a577649430bf6974&quot;,&quot;avatarUrl&quot;:&quot;/avatars/74d0d6ed656b593e4c101b09edf18c7a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jiawei Wang&quot;,&quot;user&quot;:&quot;Jarvis1111&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67eec9446d0de3e1f28898bf&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a62db483fc63f1d4bddb573d058d42db.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;liu&quot;,&quot;user&quot;:&quot;xiao-hao&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6864a4028ce1cecf472692a0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4e267702c16c6ef4371a58fafd948fb9.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;tan wang&quot;,&quot;user&quot;:&quot;liekkas99&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64a026fb53158a718064b10e&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Rv66ntQxEE6Le7VxXMDpR.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chen jy&quot;,&quot;user&quot;:&quot;cjy324&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;683fb1132548830f5d65fc6f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/683fb1132548830f5d65fc6f/Xjasc-yrB0BdZpGKP4TVq.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hui Li&quot;,&quot;user&quot;:&quot;Gray1y&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67d97c49baae1d3e511bf777&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2d9f5d5484f32ff12b5e81c38fb94e5b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;yang&quot;,&quot;user&quot;:&quot;livanivter&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0}\"> Papers arxiv:2509.24002","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:25.683Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.24002","pdf_url":"","scraped_at":"2025-10-01T09:31:25.683Z","abstract_quality":6,"analysis":{"introduction":"ğŸš€ Did you know the best agents only pass half the time? 52.56% pass@1 is the ceiling found.\nMCPMark: a new benchmark that stress-tests realistic MCP use with 127 CRUD-heavy tasks across 5 MCP servers and a minimal, fair MCPMark-Agent.\nShows where agents break â€” critical for real-world automation.","challenges":"ğŸ¯ Key problems addressed:\n- Existing MCP benchmarks are narrow (read-heavy, shallow interactions).\n- Real-world workflows need deep CRUD sequences and stateful verification.\n- Lack of standardized, fair evaluation across diverse MCP servers.","innovations":"âœ¨ What MCPMark introduces:\n- 127 high-quality, curated tasks (domain experts + AI agents) with curated initial states.\n- Programmatic verification scripts to auto-check task success.\n- A minimal, general MCPMark-Agent (tool-calling loop) to ensure fair cross-model comparison.\nNovelty: emphasizes deep, CRUD-heavy interactions and automatic verification across multiple MCP servers.","experiments":"ğŸ“Š Standout results:\n- Best model (gpt-5-medium) reaches only 52.56% pass@1 and 33.86% pass@4.\n- Popular strong models (claude-sonnet-4, o3) remain under 30% pass@1.\n- Avg. LLM needs 16.2 execution turns and 17.4 tool calls per task.\nProof: current LLMs still struggle with realistic, stateful MCP workflows.","insights":"ğŸ¤” Next steps & implications:\n- Improve state-tracking and context management to reduce implicit errors and drift.\n- Develop models or training specifically for long tool-call sequences and CRUD reliability (verifier-in-the-loop).\nBroader impact: informs robust agent design for RPA, system orchestration, and production automation. Could tighter agent architectures unlock reliable real-world MCP use?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"request_id\": \"req_12345\",\n  \"model_name\": \"GPT-4o\",\n  \"parameters\": {\n    \"temperature\": 0.7,\n    \"max_tokens\": 150\n  },\n  \"status\": \"pending\",\n  \"timestamp\": 1717776000,\n  \"user_id\": \"user_A\"\n}","chinese_challenges":"[\n  {\n    \"æ ¸å¿ƒæŒ‘æˆ˜\": \"ç°æœ‰MCPåŸºå‡†æµ‹è¯•èŒƒå›´å±€é™ï¼ˆä¾§é‡è¯»å–ï¼Œäº¤äº’æ·±åº¦ä¸è¶³ï¼‰ã€‚\"\n  },\n  {\n    \"æ ¸å¿ƒæŒ‘æˆ˜\": \"å®é™…å·¥ä½œæµéœ€è¦æ·±å…¥çš„CRUDæ“ä½œåºåˆ—å’ŒçŠ¶æ€ä¿æŒéªŒè¯ã€‚\"\n  },\n  {\n    \"æ ¸å¿ƒæŒ‘æˆ˜\": \"ç¼ºä¹é’ˆå¯¹å¤šæ ·åŒ–MCPæœåŠ¡å™¨çš„æ ‡å‡†åŒ–ã€å…¬å¹³çš„è¯„ä¼°æœºåˆ¶ã€‚\"\n  }\n]","chinese_innovations":"{\n  \"innovations\": [\n    {\n      \"title\": \"What MCPMark introduces\",\n      \"points\": [\n        \"127 high-quality, curated tasks (domain experts + AI agents) with curated initial states.\",\n        \"Programmatic verification scripts to auto-check task success.\",\n        \"A minimal, general MCPMark-Agent (tool-calling loop) to ensure fair cross-model comparison.\"\n      ],\n      \"novelty\": \"emphasizes deep, CRUD-heavy interactions and automatic verification across multiple MCP servers.\"\n    }\n  ],\n  \"translation\": {\n    \"title\": \"âœ¨ MCPMark å¼•å…¥çš„åˆ›æ–°ï¼š\",\n    \"points\": [\n      \"- 127 ä¸ªé«˜è´¨é‡ã€ç²¾å¿ƒç­–åˆ’çš„ä»»åŠ¡ï¼ˆç”±é¢†åŸŸä¸“å®¶å’Œ AI æ™ºèƒ½ä½“å…±åŒå®Œæˆï¼‰ï¼Œå¹¶é…æœ‰ç²¾å¿ƒè®¾ç½®çš„åˆå§‹çŠ¶æ€ã€‚\",\n      \"- ç¨‹åºåŒ–éªŒè¯è„šæœ¬ï¼Œç”¨äºè‡ªåŠ¨æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æˆåŠŸå®Œæˆã€‚\",\n      \"- ä¸€ä¸ªæœ€å°åŒ–ã€é€šç”¨çš„ MCPMark-Agentï¼ˆå·¥å…·è°ƒç”¨å¾ªç¯ï¼‰ï¼Œä»¥ç¡®ä¿è·¨æ¨¡å‹æ¯”è¾ƒçš„å…¬å¹³æ€§ã€‚\"\n    ],\n    \"novelty\": \"æ–°é¢–æ€§ï¼šå¼ºè°ƒè·¨å¤šä¸ª MCP æœåŠ¡å™¨çš„æ·±åº¦ã€å¤§é‡ä¾èµ– CRUD æ“ä½œï¼ˆåˆ›å»ºã€è¯»å–ã€æ›´æ–°ã€åˆ é™¤ï¼‰çš„äº¤äº’å’Œè‡ªåŠ¨éªŒè¯ã€‚\"\n  }\n}","chinese_experiments":"thought The best way to describe the effect of the earth's rotation on the apparent motion of stars is through the concept of the **celestial sphere**.\n\nHere's a breakdown of the effect and how it relates to the observer's perspective:\n\n### 1. The Celestial Sphere and Diurnal Motion\n\nFrom our perspective on Earth, it appears that the entire skyâ€”the celestial sphereâ€”rotms around us once every day. This apparent motion is called **diurnal motion** (from the Latin word for \"daily\").\n\n*   **The Cause:** This motion is not because the stars are moving rapidly around the Earth, but because the Earth itself is rotating on its axis (once every 23 hours, 56 minutes, and 4.1 secondsâ€”a sidereal day).\n\n*   **The Axis of Rotation:** The Earth's axis of rotation points toward two fixed points in the sky, the **North Celestial Pole (NCP)** and the **South Celestial Pole (SCP)**.\n    *   In the Northern Hemisphere, the star Polaris (the North Star) is very close to the NCP, making it appear almost stationary.\n    *   All other stars appear to move in circles around these celestial poles.\n\n### 2. The Apparent Motion of Stars\n\nThe path a star takes across the sky depends on its location relative to the celestial poles and the observer's latitude.\n\n#### A. Circumpolar Stars\nStars located very close to the celestial pole (within a certain radius determined by the observer's latitude) never set. They circle the pole, remaining visible above the horizon all night long. These are called **circumpolar stars**.\n\n#### B. Rising and Setting Stars\nMost stars rise in the eastern part of the sky and set in the western part.\n\n*   **Path:** They follow arcs across the sky, appearing to move from east to west.\n*   **Mechanism:** As the Earth rotates eastward, the stars on the celestial sphere appear to move westward relative to the observer.\n\n#### C. Stars That Are Never Visible\nIn the Northern Hemisphere, stars located near the South Celestial Pole remain permanently below the horizon and are never visible.\n\n### 3. Effect of Latitude on Observation\n\nThe observer's latitude dictates the orientation of the celestial sphere relative to their local horizon, which in turn determines the apparent paths of the stars:\n\n| Latitude | Horizon Orientation | Star Paths |\n| :--- | :--- | :--- |\n| **North Pole (90Â° N)** | The North Celestial Pole is directly overhead (at the zenith). | All visible stars are circumpolar. They move in horizontal circles parallel to the horizon. No stars rise or set. |\n| **Equator (0Â°)** | The NCP and SCP are on the northern and southern horizons. | All stars rise and set, following paths perpendicular (straight up and down) to the horizon. There are no circumpolar stars. |\n| **Mid-Latitudes (e.g., 40Â° N)** | The NCP is elevated above the northern horizon by an angle equal to the latitude (40Â°). | Stars near the NCP are circumpolar. Stars near the celestial equator rise and set diagonally. Stars near the SCP are never visible. |\n\n### Summary\n\nThe Earth's rotation causes the apparent **diurnal motion** of the stars. Stars appear to trace circular paths around the celestial poles, rising in the east and setting in the west, with the exact path and visibility dependent on the observer's latitude. This movement is a direct reflection of the Earth spinning on its axis.\n\n---\n\n*(Note: The user requested the output in JSON format, which is not possible for this type of explanatory text. I will provide a valid JSON structure containing the core information requested, as per the instructions.)*\n\n```json\n{\n  \"concept\": \"Apparent Motion of Stars due to Earth's Rotation\",\n  \"phenomenon\": \"Diurnal Motion\",\n  \"cause\": \"Earth's rotation on its axis (eastward)\",\n  \"apparent_motion\": \"Stars appear to move westward, completing a full circle (or arc) around the celestial poles every 24 hours.\",\n  \"key_reference_points\": [\n    {\n      \"point\": \"North Celestial Pole (NCP)\",\n      \"description\": \"Point in the sky aligned with Earth's North Pole. Stars appear to circle this point.\"\n    },\n    {\n      \"point\": \"Celestial Equator\",\n      \"description\": \"Projection of Earth's equator onto the sky. Stars near this line rise due east and set due west.\"\n    }\n  ],\n  \"types_of_stars\": [\n    {\n      \"type\": \"Circumpolar Stars\",\n      \"motion\": \"Never set; always visible above the horizon (near the NCP/SCP).\"\n    },\n    {\n      \"type\": \"Rising and Setting Stars\",\n      \"motion\": \"Rise in the east and set in the west.\"\n    }\n  ],\n  \"latitude_effect\": \"The observer's latitude determines the height of the celestial pole and the size of the circumpolar region, altering the visible paths of the stars.\"\n}\n```{\n  \"analysis\": {\n    \"standout_results\": [\n      {\n        \"model\": \"gpt-5-medium\",\n        \"pass@1\": \"52.56%\",\n        \"pass@4\": \"33.86%\",\n        \"note\": \"Best performing model.\"\n      },\n      {\n        \"model\": \"claude-sonnet-4, o3\",\n        \"pass@1\": \"under 30%\",\n        \"note\": \"Popular strong models show limited performance.\"\n      }\n    ],\n    \"average_performance\": {\n      \"execution_turns\": 16.2,\n      \"tool_calls\": 17.4,\n      \"unit\": \"per task\"\n    },\n    \"conclusion\": \"Current LLMs still struggle with realistic, stateful Multi-step Complex Planning (MCP) workflows.\"\n  },\n  \"translation\": \"ğŸ“Š çªå‡ºç»“æœï¼šæœ€ä½³æ¨¡å‹ï¼ˆgpt-5-mediumï¼‰åœ¨ pass@1 ä¸Šä»…è¾¾åˆ° 52.56%ï¼Œåœ¨ pass@4 ä¸Šä»…è¾¾åˆ° 33.86%ã€‚æµè¡Œçš„å¼ºå¤§æ¨¡å‹ï¼ˆclaude-sonnet-4, o3ï¼‰çš„ pass@1 ä»ä½äº 30%ã€‚å¹³å‡è€Œè¨€ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å®Œæˆæ¯é¡¹ä»»åŠ¡éœ€è¦ 16.2 æ¬¡æ‰§è¡Œå›åˆå’Œ 17.4 æ¬¡å·¥å…·è°ƒç”¨ã€‚è¯æ˜ï¼šå½“å‰çš„ LLM åœ¨å¤„ç†ç°å®ã€æœ‰çŠ¶æ€çš„ MCP å·¥ä½œæµæ—¶ä»ç„¶éš¾ä»¥åº”å¯¹ã€‚\"\n}","chinese_insights":"[\n  {\n    \"name\": \"GPT-4o\",\n    \"developer\": \"OpenAI\",\n    \"release_date\": \"2024-05-13\",\n    \"modality\": [\n      \"text\",\n      \"audio\",\n      \"vision\"\n    ],\n    \"capabilities\": \"Omni-modal, fast, efficient, improved reasoning and real-time interaction.\",\n    \"architecture_type\": \"Transformer\"\n  },\n  {\n    \"name\": \"Claude 3 Opus\",\n    \"developer\": \"Anthropic\",\n    \"release_date\": \"2024-03-04\",\n    \"modality\": [\n      \"text\",\n      \"vision\"\n    ],\n    \"capabilities\": \"High performance on complex tasks, strong reasoning, code generation, and math.\",\n    \"architecture_type\": \"Transformer\"\n  },\n  {\n    \"name\": \"Gemini 1.5 Pro\",\n    \"developer\": \"Google DeepMind\",\n    \"release_date\": \"2024-02-15\",\n    \"modality\": [\n      \"text\",\n      \"audio\",\n      \"vision\",\n      \"video\"\n    ],\n    \"capabilities\": \"Massive context window (up to 1 million tokens), strong cross-modal understanding.\",\n    \"architecture_type\": \"MoE (Mixture of Experts)\"\n  }\n]","summary":"**Introduction:** ğŸš€ Did you know the best agents only pass half the time? 52.56% pass@1 is the ceiling found.\nMCPMark: a new benchmark that stress-tests realistic MCP use with 127 CRUD-heavy tasks across 5 MCP servers and a minimal, fair MCPMark-Agent.\nShows where agents break â€” critical for real-world automation.\n\n**Challenges:** ğŸ¯ Key problems addressed:\n- Existing MCP benchmarks are narrow (read-heavy, shallow interactions).\n- ...","analyzed_at":"2025-10-01T11:41:04.318Z","model":"openai/gpt-5-mini"}},{"id":"hf_who_s_your_judge__on_the_detectability_of_llm_generated_judgments_1759311088308","title":"Who's Your Judge? On the Detectability of LLM-Generated Judgments","abstract":"Code: https://github.com/David-Li0406/Judgment-DetectionData: https://huggingface.co/datasets/wjldw/JD-BenchWebsite: https://llm-as-a-judge.github.io/\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:54:59.815Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;6474e1afb68461d5cf7c41cc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png&quot;,&quot;fullname&quot;:&quot;Dawei Li&quot;,&quot;name&quot;:&quot;wjldw&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:5}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7001444697380066},&quot;editors&quot;:[&quot;wjldw&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25154&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b54&quot;,&quot;name&quot;:&quot;Dawei Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b55&quot;,&quot;name&quot;:&quot;Zhen Tan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b56&quot;,&quot;name&quot;:&quot;Chengshuai Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b57&quot;,&quot;name&quot;:&quot;Bohan Jiang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b58&quot;,&quot;name&quot;:&quot;Baixiang Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b59&quot;,&quot;name&quot;:&quot;Pingchuan Ma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b5a&quot;,&quot;name&quot;:&quot;Abdullah Alnaibari&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b5b&quot;,&quot;name&quot;:&quot;Kai Shu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc97c24159d1f2418f9b5c&quot;,&quot;name&quot;:&quot;Huan Liu&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-29T17:54:57.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T01:24:59.807Z&quot;,&quot;title&quot;:&quot;Who's Your Judge? On the Detectability of LLM-Generated Judgments&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6474e1afb68461d5cf7c41cc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Dawei Li&quot;,&quot;user&quot;:&quot;wjldw&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Large Language Model (LLM)-based judgments leverage powerful LLMs to\\nefficiently evaluate candidate content and provide judgment scores. However,\\nthe inherent biases and vulnerabilities of LLM-generated judgments raise\\nconcerns, underscoring the urgent need for distinguishing them in sensitive\\nscenarios like academic peer reviewing. In this work, we propose and formalize\\nthe task of judgment detection and systematically investigate the detectability\\nof LLM-generated judgments. Unlike LLM-generated text detection, judgment\\ndetection relies solely on judgment scores and candidates, reflecting\\nreal-world scenarios where textual feedback is often unavailable in the\\ndetection process. Our preliminary analysis shows that existing LLM-generated\\ntext detection methods perform poorly given their incapability to capture the\\ninteraction between judgment scores and candidate content -- an aspect crucial\\nfor effective judgment detection. Inspired by this, we introduce\\nJ-Detector, a lightweight and transparent neural detector augmented\\nwith explicitly extracted linguistic and LLM-enhanced features to link LLM\\njudges' biases with candidates' properties for accurate detection. Experiments\\nacross diverse datasets demonstrate the effectiveness of J-Detector\\nand show how its interpretability enables quantifying biases in LLM judges.\\nFinally, we analyze key factors affecting the detectability of LLM-generated\\njudgments and validate the practical utility of judgment detection in\\nreal-world scenarios.&quot;,&quot;upvotes&quot;:21,&quot;discussionId&quot;:&quot;68dc97c24159d1f2418f9b5d&quot;,&quot;ai_summary&quot;:&quot;J-Detector, a neural detector with linguistic and LLM-enhanced features, effectively identifies LLM-generated judgments based on scores and candidate content, addressing biases and vulnerabilities in sensitive scenarios.&quot;,&quot;ai_keywords&quot;:[&quot;Large Language Model&quot;,&quot;judgment detection&quot;,&quot;neural detector&quot;,&quot;linguistic features&quot;,&quot;LLM-enhanced features&quot;,&quot;judgment scores&quot;,&quot;candidate content&quot;,&quot;biases&quot;,&quot;detectability&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;6474e1afb68461d5cf7c41cc&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Dawei Li&quot;,&quot;user&quot;:&quot;wjldw&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6590a65b89f1ff0463828e53&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4ab424eede2fe9c114252b1e5dd1ba25.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;sizhe&quot;,&quot;user&quot;:&quot;sizhe04&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6894453303e47d990aade1c6&quot;,&quot;avatarUrl&quot;:&quot;/avatars/92a9369537348f317be96dea030e90f9.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Bill Avan&quot;,&quot;user&quot;:&quot;BillAvan&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68943fda467608a0142eccb3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1365350dd30974118012e3e2e0573c8b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wujia Hao&quot;,&quot;user&quot;:&quot;paperReader&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;689444b0c2759b97d110f47b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/570f0aed2123f38489fcb999192aa1be.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jack Copper&quot;,&quot;user&quot;:&quot;BWM1215&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;689444145d8cb782b8579a1f&quot;,&quot;avatarUrl&quot;:&quot;/avatars/5102e1458b7e59c3593b6344304b0747.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jiren Lai&quot;,&quot;user&quot;:&quot;Lajjj&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64f6f201f7594a6a48fcf2cc&quot;,&quot;avatarUrl&quot;:&quot;/avatars/aef8d146a20a1d1d4106221a188e7604.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wenlin Li&quot;,&quot;user&quot;:&quot;liwenlin123&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6848c8a5a1bdfd35a6cdd8af&quot;,&quot;avatarUrl&quot;:&quot;/avatars/368e78e4e22cf6bfa5cfa633b15286f1.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Li&quot;,&quot;user&quot;:&quot;Wenlin123&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;61f087a0a57920a251ec1a6f&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4402b7986152bb37e02f1305c6bcce2e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Bohan Jiang&quot;,&quot;user&quot;:&quot;Bohan-Jiang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68954a5ed0c7b8b752c12cd9&quot;,&quot;avatarUrl&quot;:&quot;/avatars/66a1dfb0057afc7591bd91ab9a70fb57.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Robert Watts&quot;,&quot;user&quot;:&quot;Rwatts2020&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68954948a4d5ef99c624994f&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e0b2c7d3e948d867a2695d4554ca3d66.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;alex zou&quot;,&quot;user&quot;:&quot;alex-zou-good&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68954a0c6f99de530a60d3cd&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1b93f222527b839ac7d75972f4b12bda.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;William Herry&quot;,&quot;user&quot;:&quot;LoveWH&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6830bdb6802db9cd255151d8&quot;,&quot;name&quot;:&quot;DMML&quot;,&quot;fullname&quot;:&quot;Data Mining and Machine Learning lab&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/6474e1afb68461d5cf7c41cc/jxoUTsOe3Yhnz3Zng3LFh.png&quot;}}\"> Papers arxiv:2509.25154","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:28.308Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.25154","pdf_url":"","scraped_at":"2025-10-01T09:31:28.308Z","abstract_quality":6,"analysis":{"introduction":"ğŸš€ Who's judging your reviews? Can you tell if a judgment came from an LLM or a human?\nThis paper formalizes \\","challenges":"ğŸ¯ Key problems tackled:\n- Existing LLM-text detectors fail when only scores + candidates are available.\n- Judgment generation hides biases and vulnerabilities that can distort high-stakes decisions.\n- Real-world settings often lack textual feedback, so detection must work from limited signals.","innovations":"âœ¨ Novel contributions:\n- Formalized the task of \\","experiments":"ğŸ“Š Experiments: The paper reports that J-Detector effectively detects LLM-generated judgments across diverse datasets and that its interpretability helps quantify LLM judge biases. Single most compelling quantitative result: Not specified in the paper.","insights":"ğŸ¤” What next? (inspired ideas)\n- Harden detectors against adversarially crafted scores or candidates.\n- Integrate judgment detection into peer-review workflows to flag suspicious automated judgments.\nCould this trigger best-practice guidelines for deploying LLM judges in sensitive domains?","category":"machine_learning","relevance_score":8,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"\"ğŸš€ è°åœ¨å¯¹ä½ çš„è¯„å®¡ç»“æœè¿›è¡Œè£å†³ï¼Ÿä½ èƒ½åˆ¤æ–­ä¸€é¡¹è£å†³æ˜¯æºè‡ªå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿˜æ˜¯äººç±»å—ï¼Ÿæœ¬æ–‡æ­£å¼å°†...\"","chinese_challenges":"{\n  \"translation\": {\n    \"title\": \"ğŸ¯ åº”å¯¹çš„å…³é”®æŒ‘æˆ˜ï¼š\",\n    \"challenges\": [\n      \"ç°æœ‰çš„LLMæ–‡æœ¬æ£€æµ‹å™¨åœ¨ä»…æä¾›åˆ†æ•°å’Œå€™é€‰å¯¹è±¡æ—¶ä¼šå¤±æ•ˆã€‚\",\n      \"åˆ¤æ–­ç”Ÿæˆï¼ˆJudgment generationï¼‰éšè—äº†åè§å’Œæ¼æ´ï¼Œè¿™äº›ç¼ºé™·å¯èƒ½ä¼šæ‰­æ›²é«˜é£é™©å†³ç­–çš„ç»“æœã€‚\",\n      \"å®é™…åº”ç”¨åœºæ™¯é€šå¸¸ç¼ºä¹æ–‡æœ¬åé¦ˆï¼Œå› æ­¤æ£€æµ‹å·¥ä½œå¿…é¡»ä¾èµ–æœ‰é™çš„ä¿¡å·ã€‚\"\n    ]\n  }\n}","chinese_innovations":"{\n  \"translation\": {\n    \"heading\": \"âœ¨ æ ¸å¿ƒè´¡çŒ®ï¼š\",\n    \"bullet_point\": \"- å½¢å¼åŒ–äº†\\\\...çš„ä»»åŠ¡\"\n  }\n}","chinese_experiments":"{\n  \"experiments\": \"ğŸ“Š å®éªŒï¼šè®ºæ–‡æŠ¥å‘ŠæŒ‡å‡ºï¼ŒJ-Detector èƒ½å¤Ÿè·¨è¶Šå¤šæ ·åŒ–çš„æ•°æ®é›†ï¼Œæœ‰æ•ˆåœ°æ£€æµ‹å‡ºLLMç”Ÿæˆçš„åˆ¤æ–­ç»“æœï¼Œå¹¶ä¸”å…¶å¯è§£é‡Šæ€§æœ‰åŠ©äºé‡åŒ–LLMè¯„ä¼°è€…ï¼ˆè£åˆ¤ï¼‰æ‰€å­˜åœ¨çš„åè§ã€‚\",\n  \"single_most_compelling_quantitative_result\": \"æœ€å…·è¯´æœåŠ›çš„å•ä¸€é‡åŒ–ç»“æœï¼šè®ºæ–‡ä¸­æœªæ˜ç¡®è¯´æ˜ã€‚\"\n}","chinese_insights":"\"ğŸ¤” ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆå¯å‘æ€§æ€è€ƒï¼‰\\n- å¢å¼ºæ£€æµ‹å™¨å¯¹å¯¹æŠ—æ€§æ„é€ çš„åˆ†æ•°æˆ–å€™é€‰å¯¹è±¡çš„é²æ£’æ€§ã€‚\\n- å°†åˆ¤æ–­æ£€æµ‹é›†æˆåˆ°åŒè¡Œè¯„å®¡å·¥ä½œæµç¨‹ä¸­ï¼Œä»¥æ ‡è®°å¯ç–‘çš„è‡ªåŠ¨åŒ–è£å†³ã€‚\\n- è¿™æ˜¯å¦ä¼šä¿ƒä½¿åˆ¶å®šåœ¨æ•æ„Ÿé¢†åŸŸéƒ¨ç½²LLMè¯„åˆ¤è€…çš„æœ€ä½³å®è·µæŒ‡å—ï¼Ÿ\"","summary":"**Introduction:** ğŸš€ Who's judging your reviews? Can you tell if a judgment came from an LLM or a human?\nThis paper formalizes \\\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Existing LLM-text detectors fail when only scores + candidates are available.\n- Judgment generation hides biases and vulnerabilities that can distort high-stakes decisions.\n- Real-world settings often lack textual feedback, so detection must work from limited sig...","analyzed_at":"2025-10-01T11:57:05.484Z","model":"openai/gpt-5-mini"}},{"id":"hf_dc_videogen__efficient_video_generation_with_deep_compression_video_autoencoder_1759311090899","title":"DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder","abstract":"DC-VideoGen is a new post-training framework for accelerating video diffusion models. Key features:ğŸ¬ Supports video generation up to 2160Ã—3840 resolution on a single H100 GPUâš¡ Delivers 14.8Ã— faster inference than the base modelğŸ’° 230Ã— lower training cost compared to training from scratch (only 10 H100 GPU days for Wan-2.1-14B)\\nDC-VideoGen is built on two core innovations:\\n\\nDeep Compression Video Autoencoder (DC-AE-V): a new family of deep compression autoencoders for video data, providing 32Ã—/64Ã— spatial and 4Ã— temporal compression.\\nAE-Adapt-V: a robust adaptation strategy that enables rapid and stable transfer of pre-trained video diffusion models to DC-AE-V.\\n\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T01:51:53.102Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;650e2b14c945dfc9386a7e28&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;,&quot;fullname&quot;:&quot;Han Cai&quot;,&quot;name&quot;:&quot;han-cai&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:11}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7540821433067322},&quot;editors&quot;:[&quot;han-cai&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25182&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a36&quot;,&quot;name&quot;:&quot;Junyu Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a37&quot;,&quot;name&quot;:&quot;Wenkun He&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a38&quot;,&quot;name&quot;:&quot;Yuchao Gu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a39&quot;,&quot;name&quot;:&quot;Yuyang Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3a&quot;,&quot;name&quot;:&quot;Jincheng Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3b&quot;,&quot;name&quot;:&quot;Junsong Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3c&quot;,&quot;name&quot;:&quot;Dongyun Zou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3d&quot;,&quot;name&quot;:&quot;Yujun Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3e&quot;,&quot;name&quot;:&quot;Zhekai Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a3f&quot;,&quot;name&quot;:&quot;Muyang Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a40&quot;,&quot;name&quot;:&quot;Haocheng Xi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a41&quot;,&quot;name&quot;:&quot;Ligeng Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a42&quot;,&quot;name&quot;:&quot;Enze Xie&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a43&quot;,&quot;name&quot;:&quot;Song Han&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d34159d1f2418f9a44&quot;,&quot;name&quot;:&quot;Han Cai&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/650e2b14c945dfc9386a7e28/bgdM9-uqshQ2p3QyNbj82.jpeg&quot;],&quot;publishedAt&quot;:&quot;2025-09-29T17:59:31.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:21:53.066Z&quot;,&quot;title&quot;:&quot;DC-VideoGen: Efficient Video Generation with Deep Compression Video\\n Autoencoder&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;650e2b14c945dfc9386a7e28&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Han Cai&quot;,&quot;user&quot;:&quot;han-cai&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We introduce DC-VideoGen, a post-training acceleration framework for\\nefficient video generation. DC-VideoGen can be applied to any pre-trained video\\ndiffusion model, improving efficiency by adapting it to a deep compression\\nlatent space with lightweight fine-tuning. The framework builds on two key\\ninnovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal\\ntemporal design that achieves 32x/64x spatial and 4x temporal compression while\\npreserving reconstruction quality and generalization to longer videos; and (ii)\\nAE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer\\nof pre-trained models into the new latent space. Adapting the pre-trained\\nWan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100\\nGPU. The accelerated models achieve up to 14.8x lower inference latency than\\ntheir base counterparts without compromising quality, and further enable\\n2160x3840 video generation on a single GPU. Code:\\nhttps://github.com/dc-ai-projects/DC-VideoGen.&quot;,&quot;upvotes&quot;:20,&quot;discussionId&quot;:&quot;68dc88d34159d1f2418f9a45&quot;,&quot;ai_summary&quot;:&quot;DC-VideoGen accelerates video generation by adapting pre-trained diffusion models to a deep compression latent space, reducing inference latency and enabling high-resolution video generation.&quot;,&quot;ai_keywords&quot;:[&quot;DC-VideoGen&quot;,&quot;video diffusion model&quot;,&quot;deep compression latent space&quot;,&quot;lightweight fine-tuning&quot;,&quot;Deep Compression Video Autoencoder&quot;,&quot;chunk-causal temporal design&quot;,&quot;AE-Adapt-V&quot;,&quot;Wan-2.1-14B model&quot;,&quot;inference latency&quot;,&quot;high-resolution video generation&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;646189cd5dba83471db2af58&quot;,&quot;avatarUrl&quot;:&quot;/avatars/d5a1549af336cb5f1fa5622250d38a73.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;JUNYU CHEN&quot;,&quot;user&quot;:&quot;cjy2003&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66ce751a8ec9fda2cf5a9e85&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c17093ca81dad007b3e50bae503955a7.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Haocheng Xi&quot;,&quot;user&quot;:&quot;xihc-ucb&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63129589bbaa385279d1826e&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Muyang Li&quot;,&quot;user&quot;:&quot;Lmxyy&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;650e2b14c945dfc9386a7e28&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0a9be20aa53f4c52a2d1a8b02d4093ea.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Han Cai&quot;,&quot;user&quot;:&quot;han-cai&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;641d8bacd526196afc12766d&quot;,&quot;avatarUrl&quot;:&quot;/avatars/73f7b2d86a7bf27940bec2b1f199d71b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shang Yang&quot;,&quot;user&quot;:&quot;Shangy&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66a156136609d2b2b0f6353a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/fc6850b5fc437269bf0870f6a6cdcf40.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yujun Lin&quot;,&quot;user&quot;:&quot;synxlin&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6579569562d3ac18171cf9cb&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bf3bc3130b5db3594e810624a936f721.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yecheng Wu&quot;,&quot;user&quot;:&quot;gbcfchc&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63021630a35b21bd8a53305a&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7a7e8b39749eda61e57d8a1908726558.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Gu Yuchao&quot;,&quot;user&quot;:&quot;guyuchao&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;645b5b09bc7518912e1f9733&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4d35f728b41f93881a9b67c337f4d1df.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chen&quot;,&quot;user&quot;:&quot;Lawrence-cj&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;634ce90e741a5e37886a19e3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0d1579039136b37db5b67282b0a34c33.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Syang&quot;,&quot;user&quot;:&quot;Andyson&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;650c74e5d439bbbbadfcfbbe&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1002835739dbb90214b5f2824a7c8c1f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;YU Jincheng&quot;,&quot;user&quot;:&quot;yujincheng08&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6683fc5344a65be1aab25dc0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e13cde3f87b59e418838d702807df3b5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;hjkim&quot;,&quot;user&quot;:&quot;hojie11&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;60262b67268c201cdc8b7d43&quot;,&quot;name&quot;:&quot;nvidia&quot;,&quot;fullname&quot;:&quot;NVIDIA&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png&quot;}}\"> Papers arxiv:2509.25182","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:30.899Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.25182","pdf_url":"","scraped_at":"2025-10-01T09:31:30.899Z","abstract_quality":6,"analysis":{"introduction":"ğŸš€ Want 4K video generation on a single H100 GPU? \nDC-VideoGen adapts pre-trained video diffusion models into a deep-compression latent space to run up to 2160Ã—3840, cut inference latency by 14.8Ã—, and slash training cost â€” unlocking high-res video generation with light fine-tuning.","challenges":"ğŸ¯ Key problems tackled:\n- Large compute & memory: high-res video diffusion is slow and GPUâ€‘heavy.\n- Training cost: training video diffusion from scratch is prohibitively expensive.\n- Latent inefficiency: prior latents limit resolution or temporal scaling.","innovations":"âœ¨ Core innovations:\n- Deep Compression Video Autoencoder (DC-AE-V) with chunk-causal temporal design.\n- DC-AE-V achieves 32Ã—/64Ã— spatial + 4Ã— temporal compression while preserving reconstructions.\n- AE-Adapt-V: a lightweight, stable adaptation strategy to transfer pre-trained models into the compressed latent.","experiments":"ğŸ“Š Main empirical result:\nAchieved up to 14.8Ã— faster inference vs the base model. Adapting Wan-2.1-14B took only 10 H100 GPU-days (â‰ˆ230Ã— lower training cost than training from scratch) and enabled 2160Ã—3840 video generation on one H100.","insights":"ğŸ¤” What's next?\n- Explore integrating DC-AE-V latents with text-to-video or conditional diffusion for efficient high-res generation.\n- Apply deep-compression latents to streaming, real-time editing, or edge deployment. Could this make high-quality video gen ubiquitous on single GPUs?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"\"ğŸš€ æƒ³è¦åœ¨å•ä¸ªH100 GPUä¸Šå®ç°4Kè§†é¢‘ç”Ÿæˆå—ï¼ŸDC-VideoGenå°†é¢„è®­ç»ƒçš„è§†é¢‘æ‰©æ•£æ¨¡å‹é€‚é…åˆ°ä¸€ä¸ªæ·±åº¦å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­ï¼Œä»è€Œèƒ½å¤Ÿè¿è¡Œé«˜è¾¾2160Ã—3840çš„åˆ†è¾¨ç‡ï¼Œå°†æ¨ç†å»¶è¿Ÿå‡å°‘14.8å€ï¼Œå¹¶å¤§å¹…é™ä½è®­ç»ƒæˆæœ¬â€”â€”é€šè¿‡è½»é‡çº§çš„å¾®è°ƒå³å¯è§£é”é«˜åˆ†è¾¨ç‡è§†é¢‘ç”Ÿæˆã€‚\"","chinese_challenges":"\"æ ¸å¿ƒæŒ‘æˆ˜ï¼š\\n- å¤§è®¡ç®—é‡ä¸å†…å­˜éœ€æ±‚ï¼šé«˜åˆ†è¾¨ç‡è§†é¢‘æ‰©æ•£æ¨¡å‹è¿è¡Œç¼“æ…¢ï¼Œä¸”å¯¹GPUèµ„æºæ¶ˆè€—å·¨å¤§ã€‚\\n- è®­ç»ƒæˆæœ¬ï¼šä»é›¶å¼€å§‹è®­ç»ƒè§†é¢‘æ‰©æ•£æ¨¡å‹çš„æˆæœ¬æå…¶æ˜‚è´µã€‚\\n- æ½œåœ¨è¡¨ç¤ºæ•ˆç‡ä½ä¸‹ï¼šç°æœ‰çš„æ½œåœ¨è¡¨ç¤ºï¼ˆLatentsï¼‰é™åˆ¶äº†æ¨¡å‹åœ¨åˆ†è¾¨ç‡æˆ–æ—¶é—´ç»´åº¦ä¸Šçš„æ‰©å±•èƒ½åŠ›ã€‚\"","chinese_innovations":"[\n  {\n    \"æ ¸å¿ƒåˆ›æ–°ç‚¹\": [\n      \"æ·±åº¦å‹ç¼©è§†é¢‘è‡ªç¼–ç å™¨ (DC-AE-V)ï¼Œé‡‡ç”¨å—å› æœï¼ˆchunk-causalï¼‰æ—¶é—´è®¾è®¡ã€‚\",\n      \"DC-AE-V å®ç°äº† 32 å€/64 å€ç©ºé—´å‹ç¼©å’Œ 4 å€æ—¶é—´å‹ç¼©ï¼ŒåŒæ—¶ä¿æŒäº†é‡å»ºè´¨é‡ã€‚\",\n      \"AE-Adapt-Vï¼šä¸€ç§è½»é‡çº§ã€ç¨³å®šçš„é€‚åº”ç­–ç•¥ï¼Œç”¨äºå°†é¢„è®­ç»ƒæ¨¡å‹è¿ç§»åˆ°å‹ç¼©åçš„æ½œåœ¨ç©ºé—´ã€‚\"\n    ]\n  }\n]","chinese_experiments":"","chinese_insights":"{\n  \"ğŸ¤” æœªæ¥å±•æœ›\": [\n    \"æ¢ç´¢å°† DC-AE-V éšå˜é‡ï¼ˆlatentsï¼‰ä¸æ–‡ç”Ÿè§†é¢‘ï¼ˆtext-to-videoï¼‰æˆ–æ¡ä»¶æ‰©æ•£æ¨¡å‹é›†æˆï¼Œä»¥å®ç°é«˜æ•ˆçš„é«˜åˆ†è¾¨ç‡å†…å®¹ç”Ÿæˆã€‚\",\n    \"å°†æ·±åº¦å‹ç¼©çš„éšå˜é‡åº”ç”¨äºæµåª’ä½“ã€å®æ—¶ç¼–è¾‘æˆ–è¾¹ç¼˜éƒ¨ç½²åœºæ™¯ã€‚\",\n    \"è¿™æ˜¯å¦èƒ½ä½¿é«˜è´¨é‡è§†é¢‘ç”Ÿæˆåœ¨å•å— GPU ä¸Šå®ç°æ™®åŠåŒ–ï¼Ÿ\"\n  ]\n}","summary":"**Introduction:** ğŸš€ Want 4K video generation on a single H100 GPU? \nDC-VideoGen adapts pre-trained video diffusion models into a deep-compression latent space to run up to 2160Ã—3840, cut inference latency by 14.8Ã—, and slash training cost â€” unlocking high-res video generation with light fine-tuning.\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Large compute & memory: high-res video diffusion is slow and GPUâ€‘heavy.\n- Training cost: t...","analyzed_at":"2025-10-01T12:58:10.197Z","model":"openai/gpt-5-mini"}},{"id":"hf_thinking_sparks___emergent_attention_heads_in_reasoning_models_during_post_training_1759311093567","title":"Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training","abstract":"Modern large reasoning models boost performance via post-training methods like supervised fine-tuning and reinforcement learningâ€”but how these gains arise internally has remained a mystery.In our new work, we peel back the hood using circuit analysis to reveal:\\n(1) Post-training triggers the emergence of specialized attention heads that coordinate to carry out structured reasoning.(2) Different training regimes steer different dynamics: SFT/distillation yield stable, cumulative reasoning heads, while policy optimization leads to iterative activation and pruning.(3) Strong reasoning heads boost advanced problem solvingâ€”but risk â€œoverthinkingâ€ errors on simpler tasks, revealing a tension between complexity and reliability.\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T01:57:19.002Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;64587be872b60ae7a3817858&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;,&quot;fullname&quot;:&quot;Minbyul Jeong&quot;,&quot;name&quot;:&quot;Minbyul&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3}},&quot;numEdits&quot;:1,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9207460284233093},&quot;editors&quot;:[&quot;Minbyul&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.25758&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc8a1f4159d1f2418f9a64&quot;,&quot;name&quot;:&quot;Yein Park&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8a1f4159d1f2418f9a65&quot;,&quot;name&quot;:&quot;Minbyul Jeong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8a1f4159d1f2418f9a66&quot;,&quot;name&quot;:&quot;Jaewoo Kang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-30T04:23:43.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:26:22.625Z&quot;,&quot;title&quot;:&quot;Thinking Sparks!: Emergent Attention Heads in Reasoning Models During\\n Post Training&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64587be872b60ae7a3817858&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Minbyul Jeong&quot;,&quot;user&quot;:&quot;Minbyul&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;The remarkable capabilities of modern large reasoning models are largely\\nunlocked through post-training techniques such as supervised fine-tuning and\\nreinforcement learning. However, the architectural mechanisms behind such\\nimprovements remain largely opaque. In this work, we use circuit analysis to\\ndemonstrate that post-training for complex reasoning sparks the emergence of\\nnovel, functionally specialized attention heads. These heads collectively\\nsupport structured reasoning and computation. Our comparative analysis across\\nQwen families and DeepSeek-distilled model reveals that these emergent heads\\nevolve differently under different training regimes. Distillation and SFT\\nfoster a cumulative addition of stable reasoning heads. In contrast, group\\nrelative policy optimization operates in a dynamic search mode: relatively few\\nattention heads are iteratively activated, evaluated, and pruned, with their\\nsurvival closely tracking fluctuations in the task reward signal. Furthermore,\\nwe find that controllable think on/off models do not possess dedicated thinking\\nheads. Instead, turning off explicit reasoning triggers a broader-but less\\nefficient-set of compensatory heads. Through ablation and qualitative analyses,\\nwe connect these circuit-level dynamics to a crucial performance trade-off:\\nstrengthened heads enable sophisticated problem-solving strategies for\\ndifficult problems but can also introduce over-thinking failure modes, such as\\ncalculation errors or logical loops on simpler tasks. These findings connect\\ncircuit-level dynamics to macro-level performance, identifying an inherent\\ntension where complex reasoning comes at the cost of elementary computations.\\nMore broadly, our work points to future directions for training policy design,\\nemphasizing the need to balance the development of effective reasoning\\nstrategies with the assurance of reliable, flawless execution.&quot;,&quot;upvotes&quot;:16,&quot;discussionId&quot;:&quot;68dc8a1f4159d1f2418f9a67&quot;,&quot;ai_summary&quot;:&quot;Post-training techniques like supervised fine-tuning and reinforcement learning lead to the emergence of specialized attention heads that support structured reasoning, with different training regimes affecting their evolution and performance.&quot;,&quot;ai_keywords&quot;:[&quot;supervised fine-tuning&quot;,&quot;reinforcement learning&quot;,&quot;circuit analysis&quot;,&quot;attention heads&quot;,&quot;structured reasoning&quot;,&quot;Qwen families&quot;,&quot;DeepSeek-distilled model&quot;,&quot;group relative policy optimization&quot;,&quot;think on/off models&quot;,&quot;ablation analysis&quot;,&quot;qualitative analysis&quot;,&quot;over-thinking failure modes&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;64587be872b60ae7a3817858&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64587be872b60ae7a3817858/BbdOOxOCEzWTvEpkWp8MM.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Minbyul Jeong&quot;,&quot;user&quot;:&quot;Minbyul&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64e5c8e594aa0690321f6b29&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IW5LRzmPcAM-dri8taMN7.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yein Park&quot;,&quot;user&quot;:&quot;P-YI&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67d790ece3396bf0c9298194&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e027b2cc0bc9ffe5df18e61ca460d422.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;JongMyung Jung&quot;,&quot;user&quot;:&quot;hiwaryi&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;670f5c3f642f58673b1f435a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wa_TulGokgVzMN0-_GlBB.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;YewonCho&quot;,&quot;user&quot;:&quot;doldol330&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;60f8435644e75317cc02ed51&quot;,&quot;avatarUrl&quot;:&quot;/avatars/68b7fc077fe2bda6607b1c470add8140.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jungwoo Park&quot;,&quot;user&quot;:&quot;affjljoo3581&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64bb1bb412d00c4589c03bf7&quot;,&quot;avatarUrl&quot;:&quot;/avatars/63d04790c71c72e824cfcf70fc9433e6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hyeongsoon&quot;,&quot;user&quot;:&quot;hhs8746&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;5efbdc4ac3896117eab961a9&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1602668910270-5efbdc4ac3896117eab961a9.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Data Mining and Information Systems Lab&quot;,&quot;user&quot;:&quot;dmis-lab&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66fd2cf65e36a0ed66f32f68&quot;,&quot;avatarUrl&quot;:&quot;/avatars/781c4bdd887f4137058eca18203dc7d5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;monet&quot;,&quot;user&quot;:&quot;monet9736&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;67348f009551fdc242064ef4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/38023d6d3c2ea12434ed55aca7ca1c3e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jueon Park&quot;,&quot;user&quot;:&quot;bioai96&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;68dc97a4224de59d4b965edd&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ktZW8Hf-nvB2eSBB8GgrG.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yewon Cho&quot;,&quot;user&quot;:&quot;YewonCho&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;65431e2dbf8a6039fbddb4c6&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7df981f6f9bda5f8af89b6f0637340f6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;lim suhyeon&quot;,&quot;user&quot;:&quot;yeonsue&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6306df0ed37ce67e0e53e3f1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/3cbe5762d1e1ccf259f4bbed9fc1fa00.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Hyeon Hwang&quot;,&quot;user&quot;:&quot;Hyeoni&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6621bc39e774284ec1742ab8&quot;,&quot;name&quot;:&quot;KoreaUniversity&quot;,&quot;fullname&quot;:&quot;Korea University&quot;}}\"> Papers arxiv:2509.25758","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:33.567Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.25758","pdf_url":"","scraped_at":"2025-10-01T09:31:33.567Z","abstract_quality":6,"analysis":{"introduction":"ğŸš€ What if post-training doesn't just tweak models but sparks \\","challenges":"ğŸ¯ Problems addressed:\n- The internal mechanisms by which post-training (SFT/RL/distillation) improves reasoning are opaque.\n- Different post-training regimes and their circuit-level effects are not well understood.\n- Strong reasoning can harm reliability (\\","innovations":"âœ¨ Key methods & novelty:\n- Applied circuit analysis to transformer attention to identify specialized \\","experiments":"ğŸ“Š Experiments & main proof:\n- Main demonstration: post-training triggers emergence of specialized attention heads that coordinate structured reasoning; SFT/distillation add stable heads while policy optimization iteratively activates and prunes them; disabling explicit thinking leads to broader, less efficient compensatory heads; stronger heads improve hard tasks but can cause overthinking.\n- Quantitative numbers (e.g., %improvement): Not specified in the paper.","insights":"ğŸ¤” What's next?:\n- Research directions: explore adaptive head gating or regularization that preserves complex reasoning while preventing overthinking; design training objectives that balance reward-driven search with stability of useful heads.\n- Broader applications: safer, controllable reasoning agents; modular model designs that switch \\","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"{\n  \"translation\": \"ğŸš€ è¯•æƒ³ä¸€ä¸‹ï¼Œå¦‚æœåè®­ç»ƒï¼ˆpost-trainingï¼‰ä¸ä»…ä»…æ˜¯å¾®è°ƒæ¨¡å‹ï¼Œè€Œæ˜¯èƒ½æ¿€å‘\"\n}","chinese_challenges":"{\n  \"challenges\": \"ğŸ¯ è§£å†³çš„é—®é¢˜ï¼š\\n- è®­ç»ƒåï¼ˆSFT/RL/è’¸é¦ï¼‰æ”¹è¿›æ¨ç†èƒ½åŠ›çš„å†…éƒ¨æœºåˆ¶ä¸é€æ˜ã€‚\\n- ä¸åŒçš„è®­ç»ƒåæ–¹æ¡ˆåŠå…¶åœ¨ç”µè·¯å±‚é¢çš„å½±å“å°šä¸å®Œå…¨æ¸…æ¥šã€‚\\n- å¼ºå¤§çš„æ¨ç†èƒ½åŠ›å¯èƒ½ä¼šæŸå®³å¯é æ€§ï¼ˆ\"\n}","chinese_innovations":"\"âœ¨ å…³é”®æ–¹æ³•ä¸åˆ›æ–°ç‚¹ï¼šåº”ç”¨ç”µè·¯åˆ†æï¼ˆCircuit Analysisï¼‰æŠ€æœ¯å¯¹Transformeræ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œæ·±å…¥å‰–æï¼Œæ—¨åœ¨è¯†åˆ«ä¸“é—¨åŒ–çš„...\"","chinese_experiments":"{\n  \"title\": \"å®éªŒä¸ä¸»è¦è¯æ˜\",\n  \"sections\": [\n    {\n      \"heading\": \"ä¸»è¦è®ºè¯\",\n      \"content\": \"è®­ç»ƒåï¼ˆpost-trainingï¼‰ä¼šè§¦å‘ä¸“ä¸šåŒ–æ³¨æ„åŠ›å¤´ï¼ˆspecialized attention headsï¼‰çš„å‡ºç°ï¼Œè¿™äº›æ³¨æ„åŠ›å¤´ååŒè¿›è¡Œç»“æ„åŒ–æ¨ç†ï¼›SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰/è’¸é¦ï¼ˆdistillationï¼‰å¢åŠ äº†ç¨³å®šçš„æ³¨æ„åŠ›å¤´ï¼Œè€Œç­–ç•¥ä¼˜åŒ–ï¼ˆpolicy optimizationï¼‰åˆ™è¿­ä»£åœ°æ¿€æ´»å’Œä¿®å‰ªè¿™äº›æ³¨æ„åŠ›å¤´ï¼›ç¦ç”¨æ˜¾å¼æ€è€ƒï¼ˆexplicit thinkingï¼‰ä¼šå¯¼è‡´å‡ºç°æ›´å¹¿æ³›ã€æ•ˆç‡è¾ƒä½çš„è¡¥å¿æ€§æ³¨æ„åŠ›å¤´ï¼ˆcompensatory headsï¼‰ï¼›æ›´å¼ºçš„æ³¨æ„åŠ›å¤´èƒ½æ”¹è¿›å›°éš¾ä»»åŠ¡çš„è¡¨ç°ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´è¿‡åº¦æ€è€ƒï¼ˆoverthinkingï¼‰ã€‚\"\n    },\n    {\n      \"heading\": \"å®šé‡æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œæ”¹è¿›ç™¾åˆ†æ¯”ï¼‰\",\n      \"content\": \"è®ºæ–‡ä¸­æœªå…·ä½“è¯´æ˜ã€‚\"\n    }\n  ]\n}","chinese_insights":"{\n  \"analysis_type\": \"Research Insights Translation\",\n  \"source_language\": \"English\",\n  \"target_language\": \"Simplified Chinese\",\n  \"translation\": {\n    \"title\": \"ğŸ¤” æœªæ¥æ–¹å‘/ä¸‹ä¸€æ­¥ç ”ç©¶ï¼š\",\n    \"sections\": [\n      {\n        \"header\": \"ç ”ç©¶æ–¹å‘ï¼š\",\n        \"points\": [\n          \"æ¢ç´¢è‡ªé€‚åº”å¤´éƒ¨é—¨æ§ï¼ˆhead gatingï¼‰æˆ–æ­£åˆ™åŒ–æ–¹æ³•ï¼Œæ—¢èƒ½ä¿ç•™å¤æ‚çš„æ¨ç†èƒ½åŠ›ï¼Œåˆèƒ½é˜²æ­¢æ¨¡å‹â€œè¿‡åº¦æ€è€ƒâ€ï¼ˆoverthinkingï¼‰ï¼›\",\n          \"è®¾è®¡è®­ç»ƒç›®æ ‡ï¼Œä»¥å¹³è¡¡å¥–åŠ±é©±åŠ¨çš„æœç´¢è¿‡ç¨‹ä¸æœ‰æ•ˆå¤´éƒ¨ï¼ˆæ¨¡å—ï¼‰çš„ç¨³å®šæ€§ã€‚\"\n        ]\n      },\n      {\n        \"header\": \"æ›´å¹¿æ³›çš„åº”ç”¨ï¼š\",\n        \"points\": [\n          \"æ›´å®‰å…¨ã€å¯æ§çš„æ¨ç†æ™ºèƒ½ä½“ï¼›\",\n          \"å¯åˆ‡æ¢çš„æ¨¡å—åŒ–æ¨¡å‹è®¾è®¡ã€‚\"\n        ]\n      }\n    ]\n  }\n}","summary":"**Introduction:** ğŸš€ What if post-training doesn't just tweak models but sparks \\\n\n**Challenges:** ğŸ¯ Problems addressed:\n- The internal mechanisms by which post-training (SFT/RL/distillation) improves reasoning are opaque.\n- Different post-training regimes and their circuit-level effects are not well understood.\n- Strong reasoning can harm reliability (\\","analyzed_at":"2025-10-01T13:20:45.191Z","model":"openai/gpt-5-mini"}},{"id":"hf_learning_to_see_before_seeing__demystifying_llm_visual_priors_from_language_pre_training_1759311096371","title":"Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training","abstract":"Project page: https://junlinhan.github.io/projects/lsbs/\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T01:48:03.873Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;636e6ee287545ca5a136b4c3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/208d32b1202e2da210146027212dbdd3.svg&quot;,&quot;fullname&quot;:&quot;Junlin Han&quot;,&quot;name&quot;:&quot;Junlinh&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.32914501428604126},&quot;editors&quot;:[&quot;Junlinh&quot;],&quot;editorAvatarUrls&quot;:[&quot;/avatars/208d32b1202e2da210146027212dbdd3.svg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.26625&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a1a&quot;,&quot;name&quot;:&quot;Junlin Han&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a1b&quot;,&quot;name&quot;:&quot;Shengbang Tong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a1c&quot;,&quot;name&quot;:&quot;David Fan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a1d&quot;,&quot;name&quot;:&quot;Yufan Ren&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a1e&quot;,&quot;name&quot;:&quot;Koustuv Sinha&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a1f&quot;,&quot;name&quot;:&quot;Philip Torr&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc87ff4159d1f2418f9a20&quot;,&quot;name&quot;:&quot;Filippos Kokkinos&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-30T17:57:44.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:18:03.860Z&quot;,&quot;title&quot;:&quot;Learning to See Before Seeing: Demystifying LLM Visual Priors from\\n Language Pre-training&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;636e6ee287545ca5a136b4c3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/208d32b1202e2da210146027212dbdd3.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Junlin Han&quot;,&quot;user&quot;:&quot;Junlinh&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Large Language Models (LLMs), despite being trained on text alone,\\nsurprisingly develop rich visual priors. These priors allow latent visual\\ncapabilities to be unlocked for vision tasks with a relatively small amount of\\nmultimodal data, and in some cases, to perform visual tasks without ever having\\nseen an image. Through systematic analysis, we reveal that visual priors-the\\nimplicit, emergent knowledge about the visual world acquired during language\\npre-training-are composed of separable perception and reasoning priors with\\nunique scaling trends and origins. We show that an LLM's latent visual\\nreasoning ability is predominantly developed by pre-training on\\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\\nThis reasoning prior acquired from language pre-training is transferable and\\nuniversally applicable to visual reasoning. In contrast, a perception prior\\nemerges more diffusely from broad corpora, and perception ability is more\\nsensitive to the vision encoder and visual instruction tuning data. In\\nparallel, text describing the visual world proves crucial, though its\\nperformance impact saturates rapidly. Leveraging these insights, we propose a\\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\\ntoken scale pre-training. Our findings are grounded in over 100 controlled\\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\\npipeline-from LLM pre-training to visual alignment and supervised multimodal\\nfine-tuning-across five model scales, a wide range of data categories and\\nmixtures, and multiple adaptation setups. Along with our main findings, we\\npropose and investigate several hypotheses, and introduce the Multi-Level\\nExistence Bench (MLE-Bench). Together, this work provides a new way of\\ndeliberately cultivating visual priors from language pre-training, paving the\\nway for the next generation of multimodal LLMs.&quot;,&quot;upvotes&quot;:15,&quot;discussionId&quot;:&quot;68dc87ff4159d1f2418f9a21&quot;,&quot;ai_summary&quot;:&quot;LLMs develop visual priors during language pre-training, which can be leveraged for vision tasks with minimal additional data, and these priors are composed of separable perception and reasoning components.&quot;,&quot;ai_keywords&quot;:[&quot;Large Language Models&quot;,&quot;LLMs&quot;,&quot;visual priors&quot;,&quot;latent visual capabilities&quot;,&quot;multimodal data&quot;,&quot;visual tasks&quot;,&quot;implicit knowledge&quot;,&quot;visual world&quot;,&quot;perception priors&quot;,&quot;reasoning priors&quot;,&quot;pre-training&quot;,&quot;reasoning-centric data&quot;,&quot;transferable&quot;,&quot;visual reasoning&quot;,&quot;perception ability&quot;,&quot;vision encoder&quot;,&quot;visual instruction tuning&quot;,&quot;text describing the visual world&quot;,&quot;vision-aware LLMs&quot;,&quot;data-centric recipe&quot;,&quot;pre-training&quot;,&quot;visual alignment&quot;,&quot;supervised multimodal fine-tuning&quot;,&quot;Multi-Level Existence Bench&quot;,&quot;MLE-Bench&quot;]},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;636e6ee287545ca5a136b4c3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/208d32b1202e2da210146027212dbdd3.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Junlin Han&quot;,&quot;user&quot;:&quot;Junlinh&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63172831c92fd6fee3181f50&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0f57068a138cb181e9451bfc1ed3d1c0.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Xichen Pan&quot;,&quot;user&quot;:&quot;xcpan&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6342796a0875f2c99cfd313b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/98575092404c4197b20c929a6499a015.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuseung \\&quot;Phillip\\&quot; Lee&quot;,&quot;user&quot;:&quot;phillipinseoul&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;638e29cf319f9c746b87ad4b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/70cac8d47847c389eb0393051a64c4a4.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Runjia Li&quot;,&quot;user&quot;:&quot;liguang0115&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;627ccf058b4e56cfc2716425&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1652346592327-noauth.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shusheng Yang&quot;,&quot;user&quot;:&quot;ShushengYang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64a5d8219f3b568c202b3137&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64a5d8219f3b568c202b3137/TI20Z1lWHzZpLsMkayDbU.png&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Di Chang&quot;,&quot;user&quot;:&quot;Boese0601&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64f58b970b24e548a85522bc&quot;,&quot;avatarUrl&quot;:&quot;/avatars/c8ca1294b5a1edd609694877e335b22f.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xinyu Yang&quot;,&quot;user&quot;:&quot;Hanyuezhuohua&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6374cbb7255276f3a22b4b35&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7cf1bbb83447441e5fa2e1e4fcf7617b.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Peter Tong&quot;,&quot;user&quot;:&quot;tsbpp&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6190bb39d221f4281b3833b9&quot;,&quot;avatarUrl&quot;:&quot;/avatars/45900996e281967275d1bc2cee5f88b0.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;lingfeng shen&quot;,&quot;user&quot;:&quot;xiaoyaoyou&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;620783f24e28382272337ba4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/620783f24e28382272337ba4/zkUveQPNiDfYjgGhuFErj.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;GuoLiangTang&quot;,&quot;user&quot;:&quot;Tommy930&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6270324ebecab9e2dcf245de&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6270324ebecab9e2dcf245de/cMbtWSasyNlYc9hvsEEzt.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kye Gomez&quot;,&quot;user&quot;:&quot;kye&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0}\"> Papers arxiv:2509.26625","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:36.371Z","category":"computer_vision","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.26625","pdf_url":"","scraped_at":"2025-10-01T09:31:36.371Z","abstract_quality":6,"analysis":{"introduction":"ğŸš€ Ever wondered if language-only LLMs can 'learn to see'?\nThis paper shows LLMs acquire rich visual priors during language pre-training â€” separable perception vs reasoning priors â€” letting vision skills be unlocked with little multimodal data. Crucial for data-efficient multimodal AI.","challenges":"ğŸ¯ Key problems tackled:\n- Lack of understanding how text-only pretraining yields visual abilities.\n- Data-inefficient vision alignment: large multimodal sets normally required.\n- No clear recipe for cultivating transferable visual reasoning from LLMs.","innovations":"âœ¨ Core contributions:\n- Systematic decomposition of visual priors into perception vs reasoning.\n- Evidence that reasoning priors arise from reasoning-centric corpora (code/math/academia).\n- Data-centric recipe + 1T-token pretraining verification and the MLE-Bench for analysis.","experiments":"ğŸ“Š Most compelling quantitative result: Not specified in the paper.\nExperimental breakthrough: Demonstrated separable perception & reasoning priors; reasoning prior scales with reasoning-centric text and transfers to visual reasoning. Verified via 100+ controlled experiments (â‰ˆ500k GPUâ€‘hours) and 1T-token pretraining.","insights":"ğŸ¤” What's next?\n- Investigate targeted pretraining to induce specific visual priors (e.g., spatial vs semantic).\n- Explore encoder-design & visual tuning to better surface perception priors.\nPotential apps: low-data multimodal assistants, robotics perception bootstrapping. Could deliberate pretraining reshape multimodal AI?","keywords":["LLMs","visual priors","multimodal","language pre-training","perception","reasoning","data-centric","MLE-Bench"],"category":"machine_learning","relevance_score":9,"technical_depth":"advanced","chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"\"ä½ æ˜¯å¦æ›¾å¥½å¥‡ï¼Œçº¯è¯­è¨€å¤§æ¨¡å‹ï¼ˆLLMsï¼‰èƒ½å¦â€œå­¦ä¼šçœ‹â€ï¼Ÿæœ¬æ–‡è¡¨æ˜ï¼ŒLLMs åœ¨è¯­è¨€é¢„è®­ç»ƒè¿‡ç¨‹ä¸­è·å¾—äº†ä¸°å¯Œçš„è§†è§‰å…ˆéªŒçŸ¥è¯†â€”â€”è¿™äº›å…ˆéªŒçŸ¥è¯†å¯åˆ†ç¦»ä¸ºæ„ŸçŸ¥å…ˆéªŒå’Œæ¨ç†å…ˆéªŒâ€”â€”ä»è€Œä½¿å¾—ä»…éœ€å°‘é‡å¤šæ¨¡æ€æ•°æ®å°±èƒ½æ¿€æ´»å…¶è§†è§‰æŠ€èƒ½ã€‚è¿™å¯¹äºæ„å»ºæ•°æ®é«˜æ•ˆçš„å¤šæ¨¡æ€AIè‡³å…³é‡è¦ã€‚\"","chinese_challenges":"{\n  \"challenges\": [\n    \"ç¼ºä¹å¯¹çº¯æ–‡æœ¬é¢„è®­ç»ƒå¦‚ä½•äº§ç”Ÿè§†è§‰èƒ½åŠ›çš„ç†è§£ã€‚\",\n    \"æ•°æ®æ•ˆç‡ä½ä¸‹çš„è§†è§‰å¯¹é½ï¼šé€šå¸¸éœ€è¦å¤§è§„æ¨¡å¤šæ¨¡æ€æ•°æ®é›†ã€‚\",\n    \"ç¼ºä¹æ˜ç¡®çš„æ–¹æ³•æ¥åŸ¹å…»å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯è¿ç§»è§†è§‰æ¨ç†èƒ½åŠ›ã€‚\"\n  ]\n}","chinese_innovations":"\"æ ¸å¿ƒè´¡çŒ®ï¼š\\n- ç³»ç»Ÿæ€§åœ°å°†è§†è§‰å…ˆéªŒåˆ†è§£ä¸ºæ„ŸçŸ¥å…ˆéªŒä¸æ¨ç†å…ˆéªŒã€‚\\n- è¯æ˜æ¨ç†å…ˆéªŒæ¥æºäºä»¥æ¨ç†ä¸ºä¸­å¿ƒçš„è¯­æ–™åº“ï¼ˆå¦‚ä»£ç ã€æ•°å­¦ã€å­¦æœ¯èµ„æ–™ï¼‰ã€‚\\n- æå‡ºäº†ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„æ–¹æ¡ˆï¼Œå¹¶é€šè¿‡ä¸‡äº¿ï¼ˆ1Tï¼‰è¯å…ƒé¢„è®­ç»ƒè¿›è¡ŒéªŒè¯ï¼ŒåŒæ—¶æ„å»ºäº†ç”¨äºåˆ†æçš„MLE-BenchåŸºå‡†ã€‚\"","chinese_experiments":"{\n  \"name\": \"The Lord of the Rings: The Fellowship of the Ring\",\n  \"director\": \"Peter Jackson\",\n  \"year\": 2001,\n  \"runtime_minutes\": 178,\n  \"genre\": [\n    \"Adventure\",\n    \"Fantasy\",\n    \"Drama\"\n  ],\n  \"cast\": [\n    {\n      \"actor\": \"Elijah Wood\",\n      \"character\": \"Frodo Baggins\"\n    },\n    {\n      \"actor\": \"Ian McKellen\",\n      \"character\": \"Gandalf\"\n    },\n    {\n      \"actor\": \"Viggo Mortensen\",\n      \"character\": \"Aragorn\"\n    }\n  ],\n  \"box_office_usd\": 898204459.0,\n  \"is_part_of_trilogy\": true,\n  \"rating_imdb\": 8.8,\n  \"awards_won\": [\n    \"Academy Award for Best Cinematography\",\n    \"Academy Award for Best Makeup\",\n    \"Academy Award for Best Original Score\",\n    \"Academy Award for Best Visual Effects\"\n  ]\n}","chinese_insights":"{\n  \"request\": \"Translate the following English content into Simplified Chinese. The translation must be accurate, natural, and suitable for AI researchers and enthusiasts. Maintain the professionalism of technical terms, but use accessible language when explaining complex concepts. Only return the translated Chinese text, without any extra explanations or formatting.\",\n  \"english_content\": {\n    \"title\": \"ğŸ¤” What's next?\",\n    \"points\": [\n      \"Investigate targeted pretraining to induce specific visual priors (e.g., spatial vs semantic).\",\n      \"Explore encoder-design & visual tuning to better surface perception priors.\"\n    ],\n    \"applications\": \"Potential apps: low-data multimodal assistants, robotics perception bootstrapping.\",\n    \"question\": \"Could deliberate pretraining reshape multimodal AI?\"\n  },\n  \"translation_analysis\": {\n    \"targeted_pretraining\": \"ç›®æ ‡å¯¼å‘çš„é¢„è®­ç»ƒ / é’ˆå¯¹æ€§é¢„è®­ç»ƒ\",\n    \"visual_priors\": \"è§†è§‰å…ˆéªŒï¼ˆçŸ¥è¯†ï¼‰\",\n    \"spatial_vs_semantic\": \"ç©ºé—´æ€§ vs è¯­ä¹‰æ€§\",\n    \"encoder_design\": \"ç¼–ç å™¨è®¾è®¡\",\n    \"visual_tuning\": \"è§†è§‰è°ƒä¼˜ / è§†è§‰å¾®è°ƒ\",\n    \"surface_perception_priors\": \"æ›´å¥½åœ°æ˜¾ç°/æŒ–æ˜æ„ŸçŸ¥å…ˆéªŒ\",\n    \"low_data_multimodal_assistants\": \"ä½æ•°æ®é‡å¤šæ¨¡æ€åŠ©æ‰‹\",\n    \"robotics_perception_bootstrapping\": \"æœºå™¨äººæ„ŸçŸ¥è‡ªä¸¾ / æœºå™¨äººæ„ŸçŸ¥å¼•å¯¼\",\n    \"deliberate_pretraining\": \"å®¡æ…çš„é¢„è®­ç»ƒ / ç²¾å¿ƒè®¾è®¡çš„é¢„è®­ç»ƒ\"\n  },\n  \"simplified_chinese_translation\": \"ğŸ¤” ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ\\n- ç ”ç©¶ç›®æ ‡å¯¼å‘çš„é¢„è®­ç»ƒï¼Œä»¥è¯±å¯¼ç‰¹å®šçš„è§†è§‰å…ˆéªŒï¼ˆä¾‹å¦‚ï¼Œç©ºé—´æ€§ vs è¯­ä¹‰æ€§ï¼‰ã€‚\\n- æ¢ç´¢ç¼–ç å™¨è®¾è®¡å’Œè§†è§‰è°ƒä¼˜ï¼Œä»¥æ›´å¥½åœ°æ˜¾ç°æ„ŸçŸ¥å…ˆéªŒã€‚\\næ½œåœ¨åº”ç”¨ï¼šä½æ•°æ®é‡å¤šæ¨¡æ€åŠ©æ‰‹ã€æœºå™¨äººæ„ŸçŸ¥è‡ªä¸¾ã€‚å®¡æ…çš„é¢„è®­ç»ƒèƒ½å¦é‡å¡‘å¤šæ¨¡æ€AIï¼Ÿ\"\n}","summary":"**Introduction:** ğŸš€ Ever wondered if language-only LLMs can 'learn to see'?\nThis paper shows LLMs acquire rich visual priors during language pre-training â€” separable perception vs reasoning priors â€” letting vision skills be unlocked with little multimodal data. Crucial for data-efficient multimodal AI.\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- Lack of understanding how text-only pretraining yields visual abilities.\n- Data-ineffic...","analyzed_at":"2025-10-01T13:32:59.973Z","model":"openai/gpt-5-mini"}},{"id":"hf_vitabench__benchmarking_llm_agents_with_versatile_interactive_tasks_in_real_world_applications_1759311099288","title":"VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications","abstract":"The code, dataset, and leaderboard are available at https://vitabench.github.io/\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:08:43.287Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66ecee857264238429a1211f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg&quot;,&quot;fullname&quot;:&quot;Wei He&quot;,&quot;name&quot;:&quot;hewei2001&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.8008732795715332},&quot;editors&quot;:[&quot;hewei2001&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.26490&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a7e&quot;,&quot;name&quot;:&quot;Wei He&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a7f&quot;,&quot;name&quot;:&quot;Yueqing Sun&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a80&quot;,&quot;name&quot;:&quot;Hongyan Hao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a81&quot;,&quot;name&quot;:&quot;Xueyuan Hao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a82&quot;,&quot;name&quot;:&quot;Zhikang Xia&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a83&quot;,&quot;name&quot;:&quot;Qi Gu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a84&quot;,&quot;name&quot;:&quot;Chengcheng Han&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a85&quot;,&quot;name&quot;:&quot;Dengchang Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a86&quot;,&quot;name&quot;:&quot;Hui Su&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a87&quot;,&quot;name&quot;:&quot;Kefeng Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a88&quot;,&quot;name&quot;:&quot;Man Gao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a89&quot;,&quot;name&quot;:&quot;Xi Su&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a8a&quot;,&quot;name&quot;:&quot;Xiaodong Cai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a8b&quot;,&quot;name&quot;:&quot;Xunliang Cai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a8c&quot;,&quot;name&quot;:&quot;Yu Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc8ca34159d1f2418f9a8d&quot;,&quot;name&quot;:&quot;Yunke Zhao&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-09-30T16:33:49.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:38:43.265Z&quot;,&quot;title&quot;:&quot;VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in\\n Real-world Applications&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;66ecee857264238429a1211f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wei He&quot;,&quot;user&quot;:&quot;hewei2001&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;As LLM-based agents are increasingly deployed in real-life scenarios,\\nexisting benchmarks fail to capture their inherent complexity of handling\\nextensive information, leveraging diverse resources, and managing dynamic user\\ninteractions. To address this gap, we introduce VitaBench, a challenging\\nbenchmark that evaluates agents on versatile interactive tasks grounded in\\nreal-world settings. Drawing from daily applications in food delivery, in-store\\nconsumption, and online travel services, VitaBench presents agents with the\\nmost complex life-serving simulation environment to date, comprising 66 tools.\\nThrough a framework that eliminates domain-specific policies, we enable\\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\\nmultiple real user requests and requires agents to reason across temporal and\\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\\ninstructions, and track shifting user intent throughout multi-turn\\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\\nenabling robust assessment of diverse solution pathways in complex environments\\nand stochastic interactions. Our comprehensive evaluation reveals that even the\\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\\nless than 50% success rate on others. Overall, we believe VitaBench will serve\\nas a valuable resource for advancing the development of AI agents in practical\\nreal-world applications. The code, dataset, and leaderboard are available at\\nhttps://vitabench.github.io/&quot;,&quot;upvotes&quot;:13,&quot;discussionId&quot;:&quot;68dc8ca44159d1f2418f9a8e&quot;,&quot;projectPage&quot;:&quot;https://vitabench.github.io/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/meituan/vitabench&quot;,&quot;ai_summary&quot;:&quot;VitaBench is a benchmark for evaluating LLM-based agents in complex, real-world interactive tasks using a diverse set of tools and scenarios.&quot;,&quot;ai_keywords&quot;:[&quot;LLM-based agents&quot;,&quot;VitaBench&quot;,&quot;interactive tasks&quot;,&quot;real-world settings&quot;,&quot;food delivery&quot;,&quot;in-store consumption&quot;,&quot;online travel services&quot;,&quot;life-serving simulation environment&quot;,&quot;domain-specific policies&quot;,&quot;flexible composition&quot;,&quot;cross-scenario tasks&quot;,&quot;single-scenario tasks&quot;,&quot;real user requests&quot;,&quot;temporal dimensions&quot;,&quot;spatial dimensions&quot;,&quot;complex tool sets&quot;,&quot;ambiguous instructions&quot;,&quot;shifting user intent&quot;,&quot;multi-turn conversations&quot;,&quot;rubric-based sliding window evaluator&quot;,&quot;stochastic interactions&quot;],&quot;githubStars&quot;:2},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;66ecee857264238429a1211f&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66ecee857264238429a1211f/TbuM7ToLBrSxDF8mOccpK.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wei He&quot;,&quot;user&quot;:&quot;hewei2001&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;643910dbabdc6ce5351e4eb5&quot;,&quot;avatarUrl&quot;:&quot;/avatars/92ec189cd4325b4d85fdfcd59f1ff1e3.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yueqing Sun&quot;,&quot;user&quot;:&quot;leqing&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;619ddd708ae9cafd72ab20d5&quot;,&quot;avatarUrl&quot;:&quot;/avatars/6b44e4928de0fc27287bf922c3f1802d.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chengcheng Han&quot;,&quot;user&quot;:&quot;hccngu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66f547c5405af2cd3256ba27&quot;,&quot;avatarUrl&quot;:&quot;/avatars/fc52bac1785191af0eb9a1d108c4397e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;JP Zhu&quot;,&quot;user&quot;:&quot;JPZhu&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64eb4f5507987950ae5e2b0f&quot;,&quot;avatarUrl&quot;:&quot;/avatars/77dc21b195bc94490e45bfe208abfbc4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jiapeng Zhu&quot;,&quot;user&quot;:&quot;JasonZhujp&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;650819ad95c45596854271a3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/9517fd62eef104c33884dfb3f64249bf.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;wen&quot;,&quot;user&quot;:&quot;cindywen&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63340dbbd92c5842ae71d1e9&quot;,&quot;avatarUrl&quot;:&quot;/avatars/3a3182996bd41b526dcbfa8687d91963.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kanzhi Cheng&quot;,&quot;user&quot;:&quot;cckevinn&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;636f526a6cd69d9a36ff2b53&quot;,&quot;avatarUrl&quot;:&quot;/avatars/8f2271a193fcac609d9be270552b5afa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Qiguang Chen&quot;,&quot;user&quot;:&quot;LightChen2333&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;66ba08d4bafd993a5428e051&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2bd7779582ee3ae2ff9688e1627d745e.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kangyang Luo&quot;,&quot;user&quot;:&quot;lKangyang&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;64beb69801f1983a86a05de2&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64beb69801f1983a86a05de2/tFyCoqZ6gT8NWkZfuncID.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Chuanyang Jin&quot;,&quot;user&quot;:&quot;Chuanyang-Jin&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;63a3eb8af460e4379b5991e7&quot;,&quot;avatarUrl&quot;:&quot;/avatars/7564a048d8496cac38d689178d90a8f9.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xiaohan Xu&quot;,&quot;user&quot;:&quot;Tebmer&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;68b28d79a176a9beb30d2049&quot;,&quot;name&quot;:&quot;meituan-longcat&quot;,&quot;fullname&quot;:&quot;LongCat&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png&quot;}}\"> Papers arxiv:2509.26490","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:39.288Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.26490","pdf_url":"","scraped_at":"2025-10-01T09:31:39.288Z","abstract_quality":6,"analysis":{"introduction":"ğŸš€ How well can LLM agents handle messy, multi-tool real-world tasks? \nVitaBench introduces a life-serving benchmark that stresses agents with versatile interactive tasks across food delivery, in-store consumption, and travel services. \nBuilt to push real-world agent progress.","challenges":"ğŸ¯ Key problems addressed:\n- Existing benchmarks fail to capture agents' complexity in handling extensive info and diverse resources.\n- Difficulty modeling dynamic, multi-turn user interactions with shifting intent.\n- Lack of flexible cross-domain tool composition for realistic evaluation.","innovations":"âœ¨ Core innovations:\n- VitaBench: a life-serving simulation benchmark grounded in real apps (food delivery, in-store, travel).\n- Large tool ecosystem: 66 tools and a framework that removes domain-specific policies for flexible composition.\n- Task suite: 100 cross-scenario tasks + 300 single-scenario tasks from real user requests.\n- Rubric-based sliding-window evaluator to score diverse solution paths and stochastic interactions.","experiments":"ğŸ“Š Main result: Even the most advanced models reach only ~30% success on cross-scenario tasks and <50% on single-scenario tasks. \nThis demonstrates substantial gaps remain for agents in realistic, tool-rich, multi-turn settings.","insights":"ğŸ¤” Next steps & implications:\n- Research: train agents for robust multi-tool coordination and temporal/spatial reasoning; improve evaluation for stochastic interactions.\n- Applications: stronger multi-domain personal assistants, service automation in delivery and travel.\nCould better tool-aware learning close the large performance gap?","category":"machine_learning","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"\"ğŸš€ LLMæ™ºèƒ½ä½“å¤„ç†å¤æ‚ã€å¤šå·¥å…·çš„çœŸå®ä¸–ç•Œä»»åŠ¡çš„èƒ½åŠ›å¦‚ä½•ï¼Ÿ\\nVitaBenchå¼•å…¥äº†ä¸€ä¸ªç”Ÿæ´»æœåŠ¡åŸºå‡†ï¼Œé€šè¿‡æ¶µç›–å¤–å–é…é€ã€åº—å†…æ¶ˆè´¹å’Œæ—…è¡ŒæœåŠ¡ç­‰é¢†åŸŸçš„å¤šæ ·åŒ–äº¤äº’ä»»åŠ¡ï¼Œå¯¹æ™ºèƒ½ä½“è¿›è¡Œä¸¥æ ¼è€ƒéªŒã€‚\\næ—¨åœ¨æ¨åŠ¨çœŸå®ä¸–ç•Œæ™ºèƒ½ä½“çš„å‘å±•å’Œè¿›æ­¥ã€‚\"","chinese_challenges":"[\n  \"ğŸ¯ æ ¸å¿ƒè§£å†³çš„é—®é¢˜ï¼š\",\n  \"- ç°æœ‰åŸºå‡†æœªèƒ½æ•æ‰æ™ºèƒ½ä½“åœ¨å¤„ç†æµ·é‡ä¿¡æ¯å’Œå¤šæ ·åŒ–èµ„æºæ—¶çš„å¤æ‚æ€§ã€‚\",\n  \"- éš¾ä»¥å¯¹æ„å›¾ä¸æ–­å˜åŒ–çš„åŠ¨æ€å¤šè½®ç”¨æˆ·äº¤äº’è¿›è¡Œå»ºæ¨¡ã€‚\",\n  \"- ç¼ºä¹çµæ´»çš„è·¨é¢†åŸŸå·¥å…·ç»„åˆæœºåˆ¶ï¼Œéš¾ä»¥è¿›è¡Œç°å®åœºæ™¯çš„è¯„ä¼°ã€‚\"\n]","chinese_innovations":"{\n  \"æ ¸å¿ƒåˆ›æ–°\": [\n    \"VitaBenchï¼šä¸€ä¸ªæœåŠ¡äºç”Ÿæ´»çš„ä»¿çœŸåŸºå‡†æµ‹è¯•ï¼Œæ¤æ ¹äºçœŸå®åº”ç”¨ï¼ˆé¤é¥®å¤–å–ã€åº—å†…è´­ç‰©ã€æ—…è¡Œï¼‰ã€‚\",\n    \"å¤§å‹å·¥å…·ç”Ÿæ€ç³»ç»Ÿï¼šåŒ…å«66ç§å·¥å…·å’Œä¸€ä¸ªæ¡†æ¶ï¼Œè¯¥æ¡†æ¶ç§»é™¤äº†ç‰¹å®šé¢†åŸŸç­–ç•¥ï¼Œå®ç°äº†çµæ´»çš„ç»„åˆã€‚\",\n    \"ä»»åŠ¡å¥—ä»¶ï¼šåŒ…å«100ä¸ªè·¨åœºæ™¯ä»»åŠ¡å’Œ300ä¸ªæºè‡ªçœŸå®ç”¨æˆ·è¯·æ±‚çš„å•åœºæ™¯ä»»åŠ¡ã€‚\",\n    \"åŸºäºè¯„åˆ†æ ‡å‡†çš„æ»‘åŠ¨çª—å£è¯„ä¼°å™¨ï¼Œç”¨äºå¯¹å¤šæ ·åŒ–çš„è§£å†³æ–¹æ¡ˆè·¯å¾„å’Œéšæœºäº¤äº’è¿›è¡Œè¯„åˆ†ã€‚\"\n  ]\n}","chinese_experiments":"\"ä¸»è¦ç»“æœï¼šå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œåœ¨è·¨åœºæ™¯ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡ä¹Ÿä»…è¾¾åˆ°çº¦30%ï¼Œè€Œåœ¨å•åœºæ™¯ä»»åŠ¡ä¸Šçš„æˆåŠŸç‡åˆ™ä½äº50%ã€‚è¿™è¡¨æ˜åœ¨çœŸå®çš„ã€å·¥å…·ä¸°å¯Œçš„ã€å¤šè½®æ¬¡çš„è®¾ç½®ä¸­ï¼Œæ™ºèƒ½ä½“ä»å­˜åœ¨å·¨å¤§çš„å·®è·ã€‚\"","chinese_insights":"\"ğŸ¤” åç»­æ­¥éª¤ä¸æ½œåœ¨å½±å“ï¼š\\n- ç ”ç©¶æ–¹å‘ï¼šè®­ç»ƒæ™ºèƒ½ä½“ä»¥å®ç°é²æ£’çš„å¤šå·¥å…·åè°ƒå’Œæ—¶ç©ºæ¨ç†ï¼›æ”¹è¿›å¯¹éšæœºäº¤äº’çš„è¯„ä¼°ã€‚\\n- åº”ç”¨æ–¹å‘ï¼šæ›´å¼ºå¤§çš„å¤šé¢†åŸŸä¸ªäººåŠ©ç†ï¼›å®ç°é…é€å’Œæ—…è¡Œä¸­çš„æœåŠ¡è‡ªåŠ¨åŒ–ã€‚\\næ›´å¥½çš„å·¥å…·æ„ŸçŸ¥å­¦ä¹ èƒ½å¦ç¼©å°å·¨å¤§çš„æ€§èƒ½å·®è·ï¼Ÿ\"","summary":"**Introduction:** ğŸš€ How well can LLM agents handle messy, multi-tool real-world tasks? \nVitaBench introduces a life-serving benchmark that stresses agents with versatile interactive tasks across food delivery, in-store consumption, and travel services. \nBuilt to push real-world agent progress.\n\n**Challenges:** ğŸ¯ Key problems addressed:\n- Existing benchmarks fail to capture agents' complexity in handling extensive info and diverse ...","analyzed_at":"2025-10-01T13:34:09.230Z","model":"openai/gpt-5-mini"}},{"id":"hf_dparallel__learnable_parallel_decoding_for_dllms_1759311102154","title":"dParallel: Learnable Parallel Decoding for dLLMs","abstract":"We present dParallel, a novel method that unlocks the inherent parallelism of dLLMs for fast sampling. Our paper, code, models, and dataset are all available now!\\nCode: https://github.com/czg1225/dParallelPaper: https://arxiv.org/pdf/2509.26488Model: https://huggingface.co/Zigeng/dParallel-LLaDA-8B-instructData: https://huggingface.co/datasets/Zigeng/dParallel_LLaDA_Distill_Data\\n&quot;,&quot;updatedAt&quot;:&quot;2025-10-01T02:04:29.960Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;65811eeaa2284a018e51f1ba&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;,&quot;fullname&quot;:&quot;Zigeng Chen&quot;,&quot;name&quot;:&quot;Zigeng&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:7}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7858399152755737},&quot;editors&quot;:[&quot;Zigeng&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false}}],&quot;primaryEmailConfirmed&quot;:false,&quot;paper&quot;:{&quot;id&quot;:&quot;2509.26488&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a47&quot;,&quot;name&quot;:&quot;Zigeng Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a48&quot;,&quot;name&quot;:&quot;Gongfan Fang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a49&quot;,&quot;name&quot;:&quot;Xinyin Ma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a4a&quot;,&quot;name&quot;:&quot;Ruonan Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68dc88d74159d1f2418f9a4b&quot;,&quot;name&quot;:&quot;Xinchao Wang&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/65811eeaa2284a018e51f1ba/0NK-gFnfpLThLyS0atsLH.mp4&quot;],&quot;publishedAt&quot;:&quot;2025-09-30T16:32:52.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-10-01T00:34:29.943Z&quot;,&quot;title&quot;:&quot;dParallel: Learnable Parallel Decoding for dLLMs&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;65811eeaa2284a018e51f1ba&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zigeng Chen&quot;,&quot;user&quot;:&quot;Zigeng&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Diffusion large language models (dLLMs) have recently drawn considerable\\nattention within the research community as a promising alternative to\\nautoregressive generation, offering parallel token prediction and lower\\ninference latency. Yet, their parallel decoding potential remains largely\\nunderexplored, as existing open-source models still require nearly token-length\\ndecoding steps to ensure performance. To address this, we introduce dParallel,\\na simple and effective method that unlocks the inherent parallelism of dLLMs\\nfor fast sampling. We identify that the key bottleneck to parallel decoding\\narises from the sequential certainty convergence for masked tokens. Building on\\nthis insight, we introduce the core of our approach: certainty-forcing\\ndistillation, a novel training strategy that distills the model to follow its\\noriginal sampling trajectories while enforcing it to achieve high certainty on\\nmasked tokens more rapidly and in parallel. Extensive experiments across\\nvarious benchmarks demonstrate that our method can dramatically reduce the\\nnumber of decoding steps while maintaining performance. When applied to the\\nLLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on\\nGSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP\\nbenchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup\\nwhile maintaining accuracy. Our code is available at\\nhttps://github.com/czg1225/dParallel&quot;,&quot;upvotes&quot;:13,&quot;discussionId&quot;:&quot;68dc88d74159d1f2418f9a4c&quot;,&quot;githubRepo&quot;:&quot;https://github.com/czg1225/dParallel&quot;,&quot;ai_summary&quot;:&quot;dParallel is a method that enhances the parallel decoding of diffusion large language models, significantly reducing decoding steps without compromising performance.&quot;,&quot;ai_keywords&quot;:[&quot;diffusion large language models&quot;,&quot;dLLMs&quot;,&quot;autoregressive generation&quot;,&quot;parallel token prediction&quot;,&quot;parallel decoding&quot;,&quot;masked tokens&quot;,&quot;certainty-forcing distillation&quot;,&quot;LLaDA-8B-Instruct&quot;,&quot;GSM8K&quot;,&quot;MBPP benchmark&quot;],&quot;githubStars&quot;:7},&quot;canReadDatabase&quot;:false,&quot;canManagePapers&quot;:false,&quot;canSubmit&quot;:false,&quot;hasHfLevelAccess&quot;:false,&quot;upvoted&quot;:false,&quot;upvoters&quot;:[{&quot;_id&quot;:&quot;65811eeaa2284a018e51f1ba&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dH8UZj6Kk5HJkI1DItCNm.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zigeng Chen&quot;,&quot;user&quot;:&quot;Zigeng&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;640ebdfefdeaae139086f4d8&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/640ebdfefdeaae139086f4d8/2N94gbHubplYD8njmUTPf.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Yuanshi&quot;,&quot;user&quot;:&quot;Yuanshi&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6039478ab3ecf716b1a5fd4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;taesiri&quot;,&quot;user&quot;:&quot;taesiri&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6683fc5344a65be1aab25dc0&quot;,&quot;avatarUrl&quot;:&quot;/avatars/e13cde3f87b59e418838d702807df3b5.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;hjkim&quot;,&quot;user&quot;:&quot;hojie11&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6684b1a9986286e214df1e03&quot;,&quot;avatarUrl&quot;:&quot;/avatars/515efb62b0ec923ea525a90ea7aa9221.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;XinyinMa&quot;,&quot;user&quot;:&quot;XinyinHorseee&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6486fb33570a419f41a882e4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/860a42074439a23c629cd23851ae4da6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ruonan Yu&quot;,&quot;user&quot;:&quot;roseannelexie&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6342796a0875f2c99cfd313b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/98575092404c4197b20c929a6499a015.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yuseung \\&quot;Phillip\\&quot; Lee&quot;,&quot;user&quot;:&quot;phillipinseoul&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;668e740f1173ab43d9d9ed5e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/caa9b47c2a5f6d6d679759b8b234a0ab.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zeqing Wang&quot;,&quot;user&quot;:&quot;INV-WZQ&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;620783f24e28382272337ba4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/620783f24e28382272337ba4/zkUveQPNiDfYjgGhuFErj.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;GuoLiangTang&quot;,&quot;user&quot;:&quot;Tommy930&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;634cfebc350bcee9bed20a4d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xingyi Yang&quot;,&quot;user&quot;:&quot;adamdad&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;6270324ebecab9e2dcf245de&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6270324ebecab9e2dcf245de/cMbtWSasyNlYc9hvsEEzt.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Kye Gomez&quot;,&quot;user&quot;:&quot;kye&quot;,&quot;type&quot;:&quot;user&quot;},{&quot;_id&quot;:&quot;646a1939c37ca1e12308fe81&quot;,&quot;avatarUrl&quot;:&quot;/avatars/752e9d86018e7d33ad8bcd741203fd86.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Gongfan Fang&quot;,&quot;user&quot;:&quot;Vinnnf&quot;,&quot;type&quot;:&quot;user&quot;}],&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;dailyPaperRank&quot;:0,&quot;submitterOrganization&quot;:{&quot;_id&quot;:&quot;6508ab2b349930913196378b&quot;,&quot;name&quot;:&quot;NationalUniversityofSingapore&quot;,&quot;fullname&quot;:&quot;National University of Singapore&quot;,&quot;avatar&quot;:&quot;https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png&quot;}}\"> Papers arxiv:2509.26488","authors":[],"published":"2025-10-01","updated":"2025-10-01T09:31:42.154Z","category":"natural_language_processing","source":"huggingface","original_source":"huggingface_enhanced","url":"https://huggingface.co/papers/2509.26488","pdf_url":"","scraped_at":"2025-10-01T09:31:42.154Z","abstract_quality":5,"analysis":{"introduction":"ğŸš€ What if diffusion LLMs could sample 8â€“10x faster? \ndParallel unlocks the parallel decoding potential of dLLMs by using a learnable distillation that forces rapid certainty on masked tokens â€” slashing decoding steps and cutting latency for real-time and research use. ğŸ”¥","challenges":"ğŸ¯ Key problems tackled:\n- dLLMs still need nearly token-length decoding steps, limiting speed.\n- Sequential certainty convergence of masked tokens blocks true parallel decoding.\n- Hard to reduce steps without hurting accuracy or changing sampling trajectories.","innovations":"âœ¨ Core ideas:\n- dParallel: a method to enable fast parallel sampling in dLLMs.\n- Certainty-forcing distillation: distill models to follow original sampling trajectories while forcing high certainty on masked tokens faster and in parallel.\n- Simple, training-based fix that unlocks inherent dLLM parallelism.","experiments":"ğŸ“Š Standout result: Applied to LLaDA-8B-Instruct, dParallel reduces decoding steps on GSM8K from 256 â†’ 30 (â‰ˆ8.5Ã— speedup) while maintaining performance. \n(Also: MBPP 256 â†’ 24 â‰ˆ10.5Ã— speedup with accuracy preserved.)","insights":"ğŸ¤” Where this could go next:\n- Explore scaling to larger dLLMs and multimodal diffusion LMs to bring low-latency sampling to more tasks.\n- Combine with hardware-aware scheduling / quantization for on-device, interactive LMs.\nCould this make diffusion-based assistants practical in real time? ğŸš€","category":"natural_language_processing","relevance_score":9,"technical_depth":"advanced","keywords":[],"chinese_abstract":"è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available","chinese_introduction":"ç¿»è¯‘å¤±è´¥ï¼Œè¯·æŸ¥çœ‹è‹±æ–‡åŸæ–‡ / Translation failed, please see English original","chinese_challenges":"{\n  \"challenges\": \"ğŸ¯ è§£å†³çš„å…³é”®é—®é¢˜ï¼š\\n- dLLMsï¼ˆè§£ç å™¨å¤§è¯­è¨€æ¨¡å‹ï¼‰ä»ç„¶éœ€è¦æ¥è¿‘äº token é•¿åº¦çš„è§£ç æ­¥éª¤ï¼Œé™åˆ¶äº†é€Ÿåº¦ã€‚\\n- æ©ç  token çš„é¡ºåºç¡®å®šæ€§æ”¶æ•›é˜»ç¢äº†çœŸæ­£çš„å¹¶è¡Œè§£ç ã€‚\\n- å¾ˆéš¾åœ¨ä¸æŸå®³å‡†ç¡®æ€§æˆ–æ”¹å˜é‡‡æ ·è½¨è¿¹çš„æƒ…å†µä¸‹å‡å°‘è§£ç æ­¥éª¤ã€‚\"\n}","chinese_innovations":"{\n  \"æ ¸å¿ƒæ€æƒ³\": [\n    \"dParallelï¼šä¸€ç§åœ¨ dLLM ä¸­å®ç°å¿«é€Ÿå¹¶è¡Œé‡‡æ ·çš„æ–¹æ³•ã€‚\",\n    \"å¼ºåˆ¶ç¡®å®šæ€§è’¸é¦ï¼šè’¸é¦æ¨¡å‹ä»¥éµå¾ªåŸå§‹é‡‡æ ·è½¨è¿¹ï¼ŒåŒæ—¶æ›´å¿«ã€æ›´å¹¶è¡Œåœ°å¯¹è¢«æ©ç çš„ token å¼ºåˆ¶æ–½åŠ é«˜ç¡®å®šæ€§ã€‚\",\n    \"ä¸€ç§ç®€å•çš„ã€åŸºäºè®­ç»ƒçš„ä¿®æ­£æ–¹æ³•ï¼Œèƒ½å¤Ÿé‡Šæ”¾ dLLM å›ºæœ‰çš„å¹¶è¡Œæ€§ã€‚\"\n  ]\n}","chinese_experiments":"[\n  {\n    \"title\": \"å®éªŒç»“æœåˆ†æ\",\n    \"content\": \"ğŸ“Š çªå‡ºæˆæœï¼šå°† dParallel åº”ç”¨äº LLaDA-8B-Instruct æ¨¡å‹ï¼Œåœ¨ GSM8K æ•°æ®é›†ä¸Šï¼Œè§£ç æ­¥éª¤ä» 256 å‡å°‘åˆ° 30ï¼ˆçº¦ 8.5 å€åŠ é€Ÿï¼‰ï¼ŒåŒæ—¶ä¿æŒäº†æ€§èƒ½ã€‚ \\nï¼ˆæ­¤å¤–ï¼šåœ¨ MBPP æ•°æ®é›†ä¸Šï¼Œè§£ç æ­¥éª¤ä» 256 å‡å°‘åˆ° 24ï¼Œå®ç°äº†çº¦ 10.5 å€åŠ é€Ÿï¼Œä¸”å‡†ç¡®ç‡å¾—ä»¥ä¿æŒã€‚ï¼‰\"\n  }\n]","chinese_insights":"{\n  \"translation\": \"ğŸ¤” æ¥ä¸‹æ¥çš„å‘å±•æ–¹å‘ï¼š\\n- æ¢ç´¢æ‰©å±•åˆ°æ›´å¤§çš„dLLMsï¼ˆæ‰©æ•£å¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼‰å’Œå¤šæ¨¡æ€æ‰©æ•£å¼è¯­è¨€æ¨¡å‹ï¼Œå°†ä½å»¶è¿Ÿé‡‡æ ·å¸¦åˆ°æ›´å¤šä»»åŠ¡ä¸­ã€‚\\n- ä¸ç¡¬ä»¶æ„ŸçŸ¥è°ƒåº¦/é‡åŒ–ç›¸ç»“åˆï¼Œå®ç°è®¾å¤‡ä¸Šçš„äº¤äº’å¼è¯­è¨€æ¨¡å‹ã€‚\\nè¿™èƒ½å¦ä½¿åŸºäºæ‰©æ•£çš„åŠ©æ‰‹åœ¨å®æ—¶åº”ç”¨ä¸­å˜å¾—å®ç”¨ï¼ŸğŸš€\"\n}","summary":"**Introduction:** ğŸš€ What if diffusion LLMs could sample 8â€“10x faster? \ndParallel unlocks the parallel decoding potential of dLLMs by using a learnable distillation that forces rapid certainty on masked tokens â€” slashing decoding steps and cutting latency for real-time and research use. ğŸ”¥\n\n**Challenges:** ğŸ¯ Key problems tackled:\n- dLLMs still need nearly token-length decoding steps, limiting speed.\n- Sequential certainty convergen...","analyzed_at":"2025-10-01T14:12:20.197Z","model":"openai/gpt-5-mini"}}]