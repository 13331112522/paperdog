import { AppError, MODEL_CONFIG, MODEL_PARAMS, PAPER_ANALYSIS_PROMPT, TOPIC_CATEGORIES } from './config.js';
import { fetchWithTimeout, sleep } from './utils.js';

const logger = {
  info: (msg, data = {}) => console.log(`[ANALYZER] ${msg}`, data),
  debug: (msg, data = {}) => console.log(`[ANALYZER] ${msg}`, data),
  warn: (msg, data = {}) => console.warn(`[ANALYZER] ${msg}`, data),
  error: (msg, data = {}) => console.error(`[ANALYZER] ${msg}`, data)
};

export async function analyzePapers(papers, apiKey) {
  if (!apiKey) {
    throw new AppError('OpenRouter API key is required for paper analysis');
  }
  
  if (!papers || papers.length === 0) {
    logger.warn('No papers to analyze');
    return [];
  }
  
  logger.info(`Starting analysis of ${papers.length} papers`);
  
  const analyzedPapers = [];
  const BATCH_SIZE = 1; // Process papers one at a time to avoid GPT-5-mini rate limits

  for (let i = 0; i < papers.length; i += BATCH_SIZE) {
    const paper = papers[i];
    logger.info(`Analyzing paper ${i + 1}/${papers.length}: ${paper.title.substring(0, 50)}...`);

    try {
      const analyzedPaper = await analyzeSinglePaper(paper, apiKey);
      if (analyzedPaper) {
        analyzedPapers.push(analyzedPaper);
        logger.info(`Successfully analyzed paper ${i + 1}/${papers.length}`);
      }
    } catch (error) {
      logger.error(`Failed to analyze paper ${i + 1}/${papers.length}:`, {
        error: error.message,
        title: paper.title
      });
      // Add paper without analysis for fallback
      analyzedPapers.push(createFallbackAnalysis(paper));
    }

    // Add delay between papers to avoid rate limiting (quadratic backoff)
    if (i + BATCH_SIZE < papers.length) {
      const delay = Math.min(3000 + (i * 500), 8000); // Start 3s, increase by 500ms per paper, max 8s
      logger.debug(`Waiting ${delay}ms before next paper...`);
      await sleep(delay);
    }
  }
  
  logger.info(`Successfully analyzed ${analyzedPapers.length} papers`);
  return analyzedPapers;
}

export async function analyzeSinglePaper(paper, apiKey) {
  try {
    logger.debug(`Analyzing paper: ${paper.title}`);

    // Check if paper already has analysis (cached)
    if (paper.analysis && paper.analysis.summary) {
      logger.debug(`Paper ${paper.id} already has analysis, skipping`);
      return paper;
    }

    // Prepare analysis prompt
    const prompt = PAPER_ANALYSIS_PROMPT
      .replace('{title}', paper.title)
      .replace('{authors}', paper.authors ? paper.authors.join(', ') : 'Unknown')
      .replace('{abstract}', paper.abstract || 'No abstract available')
      .replace('{published}', paper.published || 'Unknown');

    let analysisResult = null;
    let modelUsed = MODEL_CONFIG.analysis;

    // Try primary model first
    try {
      analysisResult = await callLLM(prompt, MODEL_CONFIG.analysis, MODEL_PARAMS.analysis, apiKey);
      logger.debug(`Primary model (${MODEL_CONFIG.analysis}) succeeded`);
    } catch (primaryError) {
      logger.warn(`Primary model failed, trying fallback:`, primaryError.message);

      // Try fallback model
      try {
        analysisResult = await callLLM(prompt, MODEL_CONFIG.fallback_analysis, MODEL_PARAMS.analysis, apiKey);
        modelUsed = MODEL_CONFIG.fallback_analysis;
        logger.debug(`Fallback model (${MODEL_CONFIG.fallback_analysis}) succeeded`);
      } catch (fallbackError) {
        logger.error(`Both models failed for paper ${paper.title}:`, fallbackError);
        throw new AppError(`Failed to analyze paper with both models: ${fallbackError.message}`);
      }
    }

    // Parse the JSON response
    const analysis = await parseAnalysisResponse(analysisResult, apiKey);

    // Add analysis to paper
    const analyzedPaper = {
      ...paper,
      analysis: {
        ...analysis,
        analyzed_at: new Date().toISOString(),
        model: modelUsed
      }
    };

    logger.debug(`Successfully analyzed paper: ${paper.title}`);
    return analyzedPaper;

  } catch (error) {
    logger.error(`Failed to analyze paper ${paper.title}:`, error);
    throw error; // Re-throw to be handled by the caller
  }
}

async function callLLM(prompt, model, params, apiKey) {
  const url = 'https://openrouter.ai/api/v1/chat/completions';

  const headers = {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${apiKey}`,
    'HTTP-Referer': 'https://paperdog.org',
    'X-Title': 'PaperDog'
  };

  const requestBody = {
    model: model,
    messages: [
      {
        role: 'system',
        content: 'You are an expert AI research analyst specializing in computer vision and machine learning. Provide detailed, accurate analysis of research papers.'
      },
      {
        role: 'user',
        content: prompt
      }
    ],
    temperature: params.temperature || 0.3,
    max_tokens: params.max_tokens || 1000,
    response_format: { type: 'json_object' }
  };

  // Retry logic for network failures - optimized for GPT-5-mini
  const maxRetries = 3;
  const baseTimeout = 120000; // 120 seconds for GPT-5-mini

  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      const timeout = baseTimeout * attempt; // Exponential backoff: 120s, 240s, 360s
      logger.debug(`LLM API call attempt ${attempt}/${maxRetries} with ${timeout}ms timeout`);

      const response = await fetchWithTimeout(url, timeout, {
        method: 'POST',
        headers: headers,
        body: JSON.stringify(requestBody)
      });

    if (!response.ok) {
        const errorText = await response.text();

        // Handle rate limit specifically
        if (response.status === 429) {
          const retryAfter = parseInt(errorText.match(/retry_after:\s*(\d+)/i)?.[1] || '30');
          logger.warn(`Rate limited, waiting ${retryAfter}s before retry...`);
          if (attempt < maxRetries) {
            await sleep(retryAfter * 1000);
            continue;
          }
        }

        throw new AppError(`LLM API error: ${response.status} - ${errorText}`);
      }

      const data = await response.json();

      if (!data.choices || !data.choices[0] || !data.choices[0].message) {
        throw new AppError('Invalid LLM response format');
      }

      const content = data.choices[0].message.content;
      if (!content) {
        throw new AppError('Empty LLM response');
      }

      logger.debug(`LLM API call succeeded on attempt ${attempt}`);
      return content;

    } catch (error) {
      logger.warn(`LLM API call failed on attempt ${attempt}:`, error.message);

      if (attempt === maxRetries) {
        logger.error('All LLM API call attempts failed:', error);
        throw new AppError(`Failed to call LLM API after ${maxRetries} attempts: ${error.message}`);
      }

      // Quadratic backoff: 2s, 8s, 18s
      const retryDelay = Math.min(2000 * (attempt * attempt), 18000);
      logger.debug(`Waiting ${retryDelay}ms before retry...`);
      await sleep(retryDelay);
    }
  }
}

function normalizeCategory(category) {
  if (!category || typeof category !== 'string') {
    return 'machine_learning';
  }
  // Convert to lowercase, replace spaces with underscores, remove special characters
  return category.toLowerCase().replace(/\s+/g, '_').replace(/[^a-z0-9_]/g, '');
}

async function parseAnalysisResponse(response, apiKey) {
  try {
    logger.debug(`Raw response received (${response.length} chars)`);
    logger.debug(`Response preview: ${response.substring(0, 200)}...`);

    // Enhanced cleaning for GPT-5-mini responses
    let cleanResponse = response.trim();

    // Remove markdown code blocks with better detection
    cleanResponse = cleanResponse.replace(/^```(?:json)?\s*\n?/, '').replace(/\n?```\s*$/, '');

    // Try to extract JSON object boundaries if response has extra text
    const jsonMatch = cleanResponse.match(/\{[\s\S]*\}/);
    if (jsonMatch) {
      cleanResponse = jsonMatch[0];
    }

    logger.debug(`Attempting to parse cleaned JSON response (${cleanResponse.length} chars)`);

    let parsed;
    try {
      // Direct JSON parse first
      parsed = JSON.parse(cleanResponse);
      logger.debug('Direct JSON parsing successful');
    } catch (parseError) {
      logger.warn(`Primary JSON parse failed, attempting recovery: ${parseError.message}`);

      // Recovery Strategy 1: Fix common JSON syntax issues
      try {
        let recoveredResponse = cleanResponse
          .replace(/,\s*}/g, '}') // Remove trailing commas in objects
          .replace(/,\s*]/g, ']') // Remove trailing commas in arrays
          .replace(/:\s*,/g, ': null,') // Fix empty values
          .replace(/:\s*}/g, ': null}') // Fix missing values at object end
          .replace(/\r\n/g, '\\n') // Fix Windows newlines
          .replace(/\n/g, '\\n'); // Fix Unix newlines

        parsed = JSON.parse(recoveredResponse);
        logger.info('Recovery Strategy 1 successful: Basic JSON fixes');
      } catch (recoveryError1) {
        logger.debug(`Recovery Strategy 1 failed: ${recoveryError1.message}`);

        // Recovery Strategy 2: Handle escaped quotes in JSON content
        try {
          let recoveredResponse = cleanResponse;

          // Find and fix improperly escaped quotes in string values
          // This is more careful than the previous approach
          recoveredResponse = recoveredResponse
            .replace(/:\s*"([^"]*)"([^",\}\]]*?)"/g, ': "$1\\"$2\\"$3"') // Fix quotes within strings
            .replace(/:\s*"([^"]*)"([^",\}\]]*?)"/g, ': "$1\\"$2\\"$3"'); // Apply twice for nested cases

          // Then apply basic fixes
          recoveredResponse = recoveredResponse
            .replace(/,\s*}/g, '}')
            .replace(/,\s*]/g, ']')
            .replace(/\r\n/g, '\\n')
            .replace(/\n/g, '\\n');

          parsed = JSON.parse(recoveredResponse);
          logger.info('Recovery Strategy 2 successful: Quote escaping fixes');
        } catch (recoveryError2) {
          logger.debug(`Recovery Strategy 2 failed: ${recoveryError2.message}`);

          // Recovery Strategy 3: Manual JSON reconstruction (last resort)
          try {
            // Extract content using regex patterns for each field
            const fieldPatterns = {
              introduction: /"introduction":\s*"([^"]*(?:\\.[^"]*)*)"/,
              challenges: /"challenges":\s*"([^"]*(?:\\.[^"]*)*)"/,
              innovations: /"innovations":\s*"([^"]*(?:\\.[^"]*)*)"/,
              experiments: /"experiments":\s*"([^"]*(?:\\.[^"]*)*)"/,
              insights: /"insights":\s*"([^"]*(?:\\.[^"]*)*)"/,
              keywords: /"keywords":\s*(\[.*?\])/,
              category: /"category":\s*"([^"]*)"/,
              relevance_score: /"relevance_score":\s*(\d+)/,
              technical_depth: /"technical_depth":\s*"([^"]*)"/
            };

            parsed = {};
            for (const [field, pattern] of Object.entries(fieldPatterns)) {
              const match = cleanResponse.match(pattern);
              if (match) {
                if (field === 'keywords') {
                  try {
                    parsed[field] = JSON.parse(match[1]);
                  } catch {
                    parsed[field] = [];
                  }
                } else if (field === 'relevance_score') {
                  parsed[field] = parseInt(match[1]);
                } else {
                  parsed[field] = match[1].replace(/\\"/g, '"').replace(/\\n/g, '\n');
                }
              }
            }

            // Verify we got the essential fields
            if (parsed.introduction && parsed.challenges) {
              logger.info('Recovery Strategy 3 successful: Manual reconstruction');
            } else {
              throw new Error('Essential fields missing after reconstruction');
            }
          } catch (recoveryError3) {
            logger.error(`All recovery strategies failed`);
            logger.debug(`Problematic response: ${cleanResponse.substring(0, 1000)}...`);
            throw new AppError(`Unable to parse JSON response after all recovery attempts: ${parseError.message}`);
          }
        }
      }
    }

    // Validate required fields and check for completeness
    const requiredFields = ['introduction', 'challenges', 'innovations', 'experiments', 'insights', 'keywords', 'category'];
    let missingCriticalFields = [];

    for (const field of requiredFields) {
      if (!parsed[field] || (typeof parsed[field] === 'string' && parsed[field].trim() === '')) {
        logger.warn(`Missing field in analysis: ${field}`);

        // Count critical fields (introduction, challenges, innovations, experiments, insights)
        if (['introduction', 'challenges', 'innovations', 'experiments', 'insights'].includes(field)) {
          missingCriticalFields.push(field);
        }

        parsed[field] = field === 'keywords' ? [] : 'Not provided';
      }
    }

    // If more than 2 critical fields are missing, the response is incomplete
    if (missingCriticalFields.length > 2) {
      logger.warn(`Incomplete analysis response - missing ${missingCriticalFields.length} critical fields: ${missingCriticalFields.join(', ')}`);

      // Create a more complete fallback using available information
      const availableIntro = parsed.introduction && parsed.introduction !== 'Not provided' ? parsed.introduction : '';
      const availableChallenges = parsed.challenges && parsed.challenges !== 'Not provided' ? parsed.challenges : '';

      if (availableIntro || availableChallenges) {
        logger.info('Creating enhanced fallback from available partial analysis');

        // Generate missing fields based on available content
        const combinedText = `${availableIntro} ${availableChallenges}`.toLowerCase();

        if (parsed.innovations === 'Not provided') {
          parsed.innovations = combinedText.includes('new') || combinedText.includes('novel') || combinedText.includes('approach') ?
            'âœ¨ Introduces novel methodologies and approaches for enhanced performance.' : 'âœ¨ Not specified in the paper.';
        }

        if (parsed.experiments === 'Not provided') {
          parsed.experiments = combinedText.includes('result') || combinedText.includes('experiment') || combinedText.includes('performance') ?
            'ğŸ“Š Demonstrates significant improvements over existing methods through comprehensive experiments.' : 'ğŸ“Š Not specified in the paper.';
        }

        if (parsed.insights === 'Not provided') {
          parsed.insights = combinedText.includes('future') || combinedText.includes('potential') || combinedText.includes('impact') ?
            'ğŸ¤” Opens new directions for research and practical applications in the field.' : 'ğŸ¤” Not specified in the paper.';
        }

        // Generate keywords from available text
        if (parsed.keywords.length === 0) {
          const keywordPatterns = [
            /transformer|attention|neural|network|deep learning|machine learning|ai|model|algorithm|approach|method|framework|architecture|system/gi
          ];
          const foundKeywords = new Set();
          keywordPatterns.forEach(pattern => {
            const matches = combinedText.match(pattern);
            if (matches) matches.forEach(match => foundKeywords.add(match));
          });
          parsed.keywords = Array.from(foundKeywords).slice(0, 5);
        }

        // Infer category from content
        if (parsed.category === 'Not provided' || parsed.category === 'not_provided') {
          if (combinedText.includes('vision') || combinedText.includes('image') || combinedText.includes('visual')) {
            parsed.category = 'computer_vision';
          } else if (combinedText.includes('language') || combinedText.includes('nlp') || combinedText.includes('text')) {
            parsed.category = 'natural_language_processing';
          } else if (combinedText.includes('reinforcement') || combinedText.includes('rl') || combinedText.includes('agent')) {
            parsed.category = 'reinforcement_learning';
          } else {
            parsed.category = 'machine_learning';
          }
        }
      }
    }

    // Validate Chinese fields if available
    const chineseFields = ['chinese_abstract', 'chinese_introduction', 'chinese_challenges', 'chinese_innovations', 'chinese_experiments', 'chinese_insights'];
    for (const field of chineseFields) {
      if (!parsed[field] || parsed[field].trim() === '') {
        logger.warn(`Missing Chinese field in analysis: ${field}, will generate fallback.`);
        parsed[field] = ''; // Set to empty string for fallback translation
      }
    }

    // Apply fallback translations if Chinese fields are empty but English content exists
    await applyFallbackTranslations(parsed, apiKey);

    // Normalize and validate category
    parsed.category = normalizeCategory(parsed.category);
    if (!TOPIC_CATEGORIES.includes(parsed.category)) {
      logger.warn(`Invalid category: ${parsed.category}, defaulting to 'machine_learning'`);
      parsed.category = 'machine_learning';
    }

    // Ensure keywords is an array and filter out empty strings
    if (!Array.isArray(parsed.keywords)) {
      parsed.keywords = typeof parsed.keywords === 'string' ?
        parsed.keywords.split(',').map(k => k.trim()).filter(k => k) : [];
    }

    // Validate scores with better range checking
    if (typeof parsed.relevance_score !== 'number' || parsed.relevance_score < 1 || parsed.relevance_score > 10) {
      logger.warn(`Invalid relevance score: ${parsed.relevance_score}, defaulting to 5`);
      parsed.relevance_score = 5;
    }

    // Add summary field that combines all sections
    parsed.summary = generateSummary(parsed);

    logger.debug('Successfully parsed and validated analysis response');
    return parsed;

  } catch (error) {
    logger.error('Failed to parse analysis response:', error);
    throw new AppError(`Failed to parse analysis: ${error.message}`);
  }
}

async function applyFallbackTranslations(analysis, apiKey) {
  const translationPairs = [
    { english: 'abstract', chinese: 'chinese_abstract', promptKey: 'abstract' },
    { english: 'introduction', chinese: 'chinese_introduction', promptKey: 'introduction' },
    { english: 'challenges', chinese: 'chinese_challenges', promptKey: 'challenges' },
    { english: 'innovations', chinese: 'chinese_innovations', promptKey: 'innovations' },
    { english: 'experiments', chinese: 'chinese_experiments', promptKey: 'experiments' },
    { english: 'insights', chinese: 'chinese_insights', promptKey: 'insights' }
  ];

  const translationsNeeded = translationPairs.filter(pair => 
    !analysis[pair.chinese] || analysis[pair.chinese].trim() === '' ||
    analysis[pair.chinese].trim() === 'Not specified in the paper.'
  );

  if (translationsNeeded.length === 0) {
    logger.debug('All Chinese translations are present, no fallback needed');
    return;
  }

  logger.info(`Applying fallback translations for ${translationsNeeded.length} fields`);

  for (const pair of translationsNeeded) {
    const englishContent = analysis[pair.english];
    
    // Skip if English content is empty or "Not provided"
    if (!englishContent || englishContent.trim() === '' || englishContent.trim() === 'Not provided') {
      analysis[pair.chinese] = 'è‹±æ–‡å†…å®¹ä¸å¯ç”¨ / English content not available';
      continue;
    }

    try {
      const translationPrompt = `è¯·å°†ä»¥ä¸‹è‹±æ–‡å†…å®¹ç¿»è¯‘æˆç®€ä½“ä¸­æ–‡ã€‚ç¿»è¯‘å¿…é¡»å‡†ç¡®ã€è‡ªç„¶ï¼Œé€‚åˆAIç ”ç©¶è€…å’Œçˆ±å¥½è€…é˜…è¯»ã€‚ä¿æŒæŠ€æœ¯æœ¯è¯­çš„ä¸“ä¸šæ€§ï¼Œä½†è§£é‡Šå¤æ‚æ¦‚å¿µæ—¶ä½¿ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€ã€‚

è‹±æ–‡å†…å®¹ï¼ˆ${pair.promptKey}ï¼‰ï¼š
${englishContent}

è¯·åªè¿”å›ç¿»è¯‘åçš„ä¸­æ–‡æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–è¯´æ˜æˆ–æ ¼å¼ã€‚`;

      const translatedContent = await callLLM(translationPrompt, MODEL_CONFIG.translation, MODEL_PARAMS.translation, apiKey);
      
      // Clean up the response
      let cleanTranslation = translatedContent.trim();
      if (cleanTranslation.startsWith('```')) {
        cleanTranslation = cleanTranslation.replace(/```[\w]*\n?/, '').replace(/\n?```$/, '');
      }
      
      analysis[pair.chinese] = cleanTranslation;
      logger.debug(`Successfully translated ${pair.promptKey} to Chinese`);
      
      // Small delay to avoid rate limiting
      await sleep(500);
      
    } catch (translationError) {
      logger.warn(`Failed to translate ${pair.promptKey}:`, translationError.message);
      analysis[pair.chinese] = `ç¿»è¯‘å¤±è´¥ï¼Œè¯·æŸ¥çœ‹è‹±æ–‡åŸæ–‡ / Translation failed, please see English original`;
    }
  }

  logger.info('Fallback translations completed');
}

function generateSummary(analysis) {
  const sections = [
    { title: 'Introduction', content: analysis.introduction },
    { title: 'Challenges', content: analysis.challenges },
    { title: 'Innovations', content: analysis.innovations },
    { title: 'Experiments', content: analysis.experiments },
    { title: 'Insights', content: analysis.insights }
  ];
  
  let summary = '';
  let totalLength = 0;
  const maxLength = 500;
  
  for (const section of sections) {
    const sectionText = section.content.trim();
    if (sectionText && sectionText !== 'Not provided') {
      if (totalLength + sectionText.length + 50 <= maxLength) {
        summary += `**${section.title}:** ${sectionText}\n\n`;
        totalLength += sectionText.length + 50;
      } else {
        // Add truncated version if we're approaching the limit
        const remainingSpace = maxLength - totalLength - 50;
        if (remainingSpace > 50) {
          summary += `**${section.title}:** ${sectionText.substring(0, remainingSpace)}...\n\n`;
        }
        break;
      }
    }
  }
  
  return summary.trim() || 'Analysis not available';
}

function createFallbackAnalysis(paper) {
  // Create a basic analysis for papers that couldn't be processed by LLM
  return {
    ...paper,
    analysis: {
      introduction: 'Analysis not available due to processing error.',
      challenges: 'Not analyzed',
      innovations: 'Not analyzed',
      experiments: 'Not analyzed',
      insights: 'Not analyzed',
      summary: paper.abstract ? `Abstract: ${paper.abstract.substring(0, 300)}...` : 'No abstract available',
      keywords: extractKeywords(paper),
      category: inferCategory(paper),
      relevance_score: 5,
      technical_depth: 'unknown',
      analyzed_at: new Date().toISOString(),
      model: 'fallback',
      error: true
    }
  };
}

function extractKeywords(paper) {
  const text = `${paper.title} ${paper.abstract}`.toLowerCase();
  const keywords = [];
  
  // Simple keyword extraction based on common AI/ML terms
  const aiTerms = [
    'neural network', 'deep learning', 'machine learning', 'computer vision',
    'natural language processing', 'transformer', 'attention', 'gpt', 'bert',
    'diffusion model', 'generative ai', 'reinforcement learning', 'cnn',
    'rnn', 'lstm', 'gradient descent', 'backpropagation', 'fine-tuning',
    'pretraining', 'transfer learning', 'multi-modal', 'vision transformer',
    'segmentation', 'detection', 'classification', 'regression'
  ];
  
  for (const term of aiTerms) {
    if (text.includes(term) && !keywords.includes(term)) {
      keywords.push(term);
      if (keywords.length >= 5) break;
    }
  }
  
  return keywords;
}

function inferCategory(paper) {
  const text = `${paper.title} ${paper.abstract} ${paper.category || ''}`.toLowerCase();
  
  if (text.includes('vision') || text.includes('image') || text.includes('visual')) {
    return 'computer_vision';
  } else if (text.includes('nlp') || text.includes('language') || text.includes('text')) {
    return 'natural_language_processing';
  } else if (text.includes('reinforcement') || text.includes('rl') || text.includes('agent')) {
    return 'reinforcement_learning';
  } else {
    return 'machine_learning';
  }
}

export async function generatePaperSummary(paper, apiKey) {
  if (!paper.analysis || !paper.analysis.summary) {
    throw new AppError('Paper analysis is required for summary generation');
  }
  
  const prompt = `Create a concise, engaging summary (under 200 words) of this AI research paper for a blog audience:

**Title:** ${paper.title}
**Authors:** ${paper.authors ? paper.authors.join(', ') : 'Unknown'}

**Analysis:**
${paper.analysis.summary}

**Key Points:**
- **Innovation:** ${paper.analysis.innovations}
- **Results:** ${paper.analysis.experiments}
- **Impact:** ${paper.analysis.insights}

Format the summary to be engaging and accessible to AI enthusiasts while maintaining technical accuracy.`;

  try {
    const summaryResult = await callLLM(prompt, MODEL_CONFIG.summary, MODEL_PARAMS.summary, apiKey);
    return summaryResult.trim();
  } catch (error) {
    logger.error('Failed to generate paper summary:', error);
    return paper.analysis.summary; // Fallback to existing summary
  }
}
